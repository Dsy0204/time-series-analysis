作者：尹爱华        18数科

[TOC]

## State-space模型对USB的收盘价Close进行模型的拟合和预测

######模型的拟合：前3401个数据

######模型的预测：最后5个数据

```python
%matplotlib inline
 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import datetime
from dateutil.relativedelta import relativedelta
import seaborn as sns
import statsmodels.api as sm  
from statsmodels.tsa.stattools import acf  
from statsmodels.tsa.stattools import pacf
from statsmodels.tsa.seasonal import seasonal_decompose
```


```python
data = pd.read_csv('/home/yinaihua/Desktop/时间序列/期末作业/chapter-07/USB.csv')
```


```python
Open=data['USB.Open']
Close=data['USB.Close']
```

```python
shoupanjia=Close[::-1]
```

```python
print(shoupanjia[3401:])########预测的
```

    4    35.630001
    3    35.650002
    2    35.770000
    1    36.200001
    0    36.169998
    Name: USB.Close, dtype: float64

```python
shoupanjia_train=shoupanjia[0:3401]
print(len(shoupanjia_train))
```

    3401

```python
#print(shoupanjia_train)
```


```python
shoupanjia.plot(figsize=(12,8), title= 'shoupanjia_train', fontsize=14)
```


    <matplotlib.axes._subplots.AxesSubplot at 0x7fcfe8698780>

```python
#########然后我们分解数据
decomposition = seasonal_decompose(shoupanjia, freq=52)
fig = plt.figure()  
fig = decomposition.plot()  
fig.set_size_inches(12, 6)
```


    <Figure size 432x288 with 0 Axes>

```python
########编写函数测试一下时间序列是否稳定（这个在前面ARIMA建模中已经写过，稍有不同）
from statsmodels.tsa.stattools import adfuller
def test_stationarity(timeseries):
    
    #Determing rolling statistics
    rolmean = timeseries.rolling(window=52).mean()
    rolstd = timeseries.rolling(window=52).std()
 
    #Plot rolling statistics:
    fig = plt.figure(figsize=(12, 8))
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show()
    
    #Perform Dickey-Fuller test:
    print('Results of Dickey-Fuller Test:')
    dftest = adfuller(timeseries, autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in dftest[4].items():
        dfoutput['Critical Value (%s)'%key] = value
    print(dfoutput)
```


```python
########我们测试一下稳定性   #若统计量显著小于三个置信度且p值接近0，为平稳序列
test_stationarity(shoupanjia_train)
########从p值大于0.5和t统计量大于三个Critical Value可以得出，整体的序列并没有到达稳定性要求
```


    Results of Dickey-Fuller Test:
    Test Statistic                   -1.460379
    p-value                           0.553012
    #Lags Used                       27.000000
    Number of Observations Used    3372.000000
    Critical Value (1%)              -3.432291
    Critical Value (5%)              -2.862398
    Critical Value (10%)             -2.567226
    dtype: float64

```python
#########我们先进行一阶差分
first_difference = shoupanjia_train.diff(1)  
test_stationarity(first_difference.dropna(inplace=False))
#########一节差分后稳定
```


    Results of Dickey-Fuller Test:
    Test Statistic                -1.181560e+01
    p-value                        8.653151e-22
    #Lags Used                     2.600000e+01
    Number of Observations Used    3.372000e+03
    Critical Value (1%)           -3.432291e+00
    Critical Value (5%)           -2.862398e+00
    Critical Value (10%)          -2.567226e+00
    dtype: float64

```python
##确定pd的值
def queding_pq(D_data):
    ##############确定p,q值
    
    from statsmodels.tsa.arima_model import ARIMA
    #定阶
    
    #一般阶数不超过length/10
    
    #pmax = int(len(D_data)/10) 
    pmax = 6
    #一般阶数不超过length/10
    
    #qmax = int(len(D_data)/10) 
    qmax =6
    
    #bic矩阵
    
    bic_matrix = [] 
    for p in range(pmax+1):
      tmp = []
      for q in range(qmax+1):
    #存在部分报错，所以用try来跳过报错。
        try: 
          print(ARIMA(D_data, (p,1,q)).fit().bic)
          tmp.append(ARIMA(D_data, (p,1,q)).fit().bic)
        except:
          tmp.append(100000)
      bic_matrix.append(tmp)
    
    #从中可以找出最小值
    
    bic_matrix = pd.DataFrame(bic_matrix) 
    
    #先用stack展平，然后用idxmin找出最小值位置。
    
    p,q = bic_matrix.stack().idxmin() 
    
    print(u'BIC最小的p值和q值为：%s、%s' %(p,q))
    
    # 取BIC信息量达到最小的模型阶数。
```


```python
queding_pq(shoupanjia_train)#######最佳参数是2和2
```

    /home/yinaihua/ENTER/lib/python3.7/site-packages/scipy/signal/signaltools.py:1341: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.
      out_full[ind] += zi
    /home/yinaihua/ENTER/lib/python3.7/site-packages/scipy/signal/signaltools.py:1344: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.
      out = out_full[ind]
    /home/yinaihua/ENTER/lib/python3.7/site-packages/scipy/signal/signaltools.py:1350: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.
      zf = out_full[ind]


    6833.832461606839
    6814.937070640504
    6819.606486805211
    6826.298273216813
    6823.693045944312
    6831.821550760242
    6837.8772889721795
    6813.442989975905
    6819.884839432217
    6824.895906353686
    6819.511049354809
    6827.629248165969
    6812.060477123838
    6833.173887199094
    6827.563986598095
    6832.331824119184
    6824.092302602131
    6826.08949167178
    6833.787113266529
    6822.6644185576715
    6830.432728548182
    6830.5448248587645
    6831.707768923442
    6815.953390188296
    6830.670759917198
    6832.1646118482595
    6831.794794484251
    6823.788831580631
    6826.581002835731
    6857.432564635776
    6833.277565849931
    6828.924773155949
    6830.712957671376
    6838.066406273758
    6844.661366813955
    6836.199300887186
    BIC最小的p值和q值为：2、2

```python
#通过网格搜索对seasonal_order进行定阶/绘制ACF图进行判断
def get_ARIMA_params(data, pdq, m=52):#########0,0,2,52
    import itertools       
    import warnings

    p = d = q = range(0, 3)
    seasonal_pdq = [(x[0], x[1], x[2], m) for x in list(itertools.product(p, d, q))]
    score_aic = 1000000.0
    warnings.filterwarnings("ignore") # specify to ignore warning messages
    for param_seasonal in seasonal_pdq:
        mod = sm.tsa.statespace.SARIMAX(data,
                                        order=pdq,
                                        seasonal_order=param_seasonal,
                                        enforce_stationarity=False,
                                        enforce_invertibility=False)
        results = mod.fit()
        print('x{}12 - AIC:{}'.format(param_seasonal, results.aic))
        if results.aic < score_aic:
            score_aic = results.aic
            params = param_seasonal, results.aic
    param_seasonal, results.aic = params
    print('x{}12 - AIC:{}'.format(param_seasonal, results.aic))
pdq = [2, 1, 2]
get_ARIMA_params(shoupanjia_train, pdq, m=52)

```

    x(0, 0, 0, 52)12 - AIC:6790.239344141735
    x(0, 0, 1, 52)12 - AIC:6458.559926642996
    x(0, 0, 2, 52)12 - AIC:5816.153942812474
    x(0, 1, 0, 52)12 - AIC:8989.52985613661
    x(0, 1, 1, 52)12 - AIC:6313.606120507984



```python

mod = sm.tsa.statespace.SARIMAX(shoupanjia_train,
                                order=(2, 1, 2),
                                seasonal_order=(0, 0, 2, 52),
                                enforce_stationarity=False,
                                enforce_invertibility=False).fit()


```


```python
print(mod.summary())
```

                                     Statespace Model Results                                 
    ==========================================================================================
    Dep. Variable:                          USB.Close   No. Observations:                 3401
    Model:             SARIMAX(2, 1, 2)x(0, 0, 2, 52)   Log Likelihood               -2902.866
    Date:                            Mon, 10 Aug 2020   AIC                           5819.733
    Time:                                    10:20:34   BIC                           5862.430
    Sample:                                         0   HQIC                          5835.018
                                               - 3401                                         
    Covariance Type:                              opg                                         
    ==============================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
    ------------------------------------------------------------------------------
    ar.L1          0.6087      0.372      1.638      0.101      -0.120       1.337
    ar.L2          0.1301      0.180      0.724      0.469      -0.222       0.482
    ma.L1         -0.6940      0.372     -1.864      0.062      -1.424       0.036
    ma.L2         -0.0866      0.204     -0.425      0.671      -0.486       0.313
    ma.S.L52      -0.0070      0.011     -0.619      0.536      -0.029       0.015
    ma.S.L104     -0.0106      0.013     -0.796      0.426      -0.037       0.016
    sigma2         0.3413      0.005     74.022      0.000       0.332       0.350
    ===================================================================================
    Ljung-Box (Q):                      103.98   Jarque-Bera (JB):              3545.48
    Prob(Q):                              0.00   Prob(JB):                         0.00
    Heteroskedasticity (H):               1.80   Skew:                             0.06
    Prob(H) (two-sided):                  0.00   Kurtosis:                         8.08
    ===================================================================================
    
    Warnings:
    [1] Covariance matrix calculated using the outer product of gradients (complex-step).



```python
####评估模型预测结果
test=shoupanjia[3401:]########预测的
predictions = mod.predict(start=len(shoupanjia_train), end=len(shoupanjia_train)+len(test)-1, dynamic=False,tpy='levels')######有差分，要加typ='levels'
predictions=np.matrix(predictions)

```


```python
test=np.matrix(test)
predictions=np.matrix(predictions)


for i in range(5):
    #print(test[i])
    print('predicted=%f, expected=%f' ,predictions[0,i], test[0,i])

from sklearn.metrics import mean_squared_error
error = mean_squared_error(test, predictions)
print('Test MSE: %.3f' % error)
```

    predicted=%f, expected=%f 35.600110300027694 35.630001
    predicted=%f, expected=%f 35.60297737643107 35.650002
    predicted=%f, expected=%f 35.59840494300382 35.77
    predicted=%f, expected=%f 35.595434690605536 36.200001
    predicted=%f, expected=%f 35.59828989835756 36.169998
    Test MSE: 0.145
    #######可以看出，模型的预测数据与真实数据比较接近


```python
######结论：使用SARIMAX模型预测最后5个收盘价，均方误差为0.145，达到了一个较好的水平
```

## 机器学习算法对USB的收盘价Close进行模型的拟合和预测

### 已知：

股市的开盘价(Open)、最高价(High)、最低价(Low)、成交量(Volume)、调整(Adjusted)，

### 预测：收盘价(Close)

### 选择模型：回归模型

### 过程：

#### 1.首先，我们可视化一下这些变量之间的关系

```python
import pandas as pd
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import mean_squared_error

data = pd.read_csv("USB.csv")
data = data[::-1]
open = data['USB.Open']
high = data['USB.High']
low = data['USB.Low']
volume = data['USB.Volume']
adjusted = data['USB.Adjusted']
close = data['USB.Close']

plt.subplot(3,2,1)
plt.plot(open,close)
plt.subplot(3,2,2)
plt.plot(high,close)
plt.subplot(3,2,3)
plt.plot(low,close)
plt.subplot(3,2,4)
plt.plot(volume,close)
plt.subplot(3,2,5)
plt.plot(adjusted,close)
plt.show()
```

<img src="https://s1.ax1x.com/2020/08/13/dS6FAK.png" style="zoom:80%;" />

上图是开盘价、最高价、最低价、成交量和调整与收盘价之间关系的图像

可以看出：（1）开盘价、最高价、最低价与收盘价呈线性关系且符合程度高

​					（2）成交量与收盘价之间类似呈反比关系

​					（3）调整与收盘价之间呈线性关系但符合程度不高。

#### 2.下面我们先用开盘价、最高价、收盘价进行拟合。

```python
import pandas as pd
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import mean_squared_error

close = pd.read_csv("USB.csv",usecols=[4])#收盘价
x = pd.read_csv("USB.csv",usecols=[1,2,3])#开盘价、最高价、最低价
x = x[::-1]
close = close[::-1]#按时间先后顺序
x_train = x[0:3401]#把最后5个数值用于预测
x_test = x[3401:]
y_train = close[0:3401]
y_test = close[3401:]
model = LinearRegression()#多元线性回归模型
model.fit(x_train,y_train)
a = model.intercept_  # 截距
b = model.coef_  # 回归系数
print("最佳拟合线:截距", a, ",回归系数：", b[0])
```

结果为

```python
最佳拟合线:截距 [0.03089461] ,回归系数： [-0.58333996  0.81910049  0.76312957]
```

即
$$
Close = 0.03089461-0.58333996Open+0.81910049High+0.76312957Low
$$
下面对$x_{test}$进行预测并与实际值进行对比

```python
y_predict = []
x_test = np.array(x_test)
for i in range(5):
    y_predict.append((np.dot(b,x_test[i])+a)[0])#预测值

y_test = np.array(y_test)
error = mean_squared_error(y_test, y_predict)#MSE
for i in range(5):
    print("predicted:%f,expected:%f"%(y_predict[i],y_test[i]))
print('Test MSE: %.3f' % error)

predicted=[]#所有x值的预测值
x = np.array(x)
for i in range(len(x)):
    predicted.append((np.dot(b,x[i])+a)[0])
plt.plot(np.array(close),label="expected")
plt.plot(predicted,label="predicted")
plt.legend()
plt.show()
```

结果为

```python
predicted:35.561747,expected:35.630001
predicted:35.932373,expected:35.650002
predicted:35.806794,expected:35.770000
predicted:36.027865,expected:36.200001
predicted:36.158361,expected:36.169998
Test MSE: 0.023
```

均方误差为0.023较小。

#### 3.用这五个变量进行拟合：

$$
Close = a*Open + b*High + c*Low + d/Volume + e*Adjuested + f
$$
对于Volume，我们先在csv文件中求得倒数后用多元线性回归拟合。

```python
import pandas as pd
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import mean_squared_error

close = pd.read_csv("USB.csv",usecols=[4])
x = pd.read_csv("USB.csv",usecols=[1,2,3,5,6])
x = x[::-1]
close = close[::-1]
x_train = x[0:3401]
x_test = x[3401:]
y_train = close[0:3401]
y_test = close[3401:]
model = LinearRegression()
model.fit(x_train,y_train)
a = model.intercept_  # 截距
b = model.coef_  # 回归系数
print("最佳拟合线:截距", a, ",回归系数：", b[0])

y_predict = []
x_test = np.array(x_test)
for i in range(5):
    y_predict.append((np.dot(b,x_test[i])+a)[0])

y_test = np.array(y_test)
error = mean_squared_error(y_test, y_predict)
for i in range(5):
    print("predicted:%f,expected:%f"%(y_predict[i],y_test[i]))
print('Test MSE: %.3f' % error)

predicted=[]
x = np.array(x)
for i in range(len(x)):
    predicted.append((np.dot(b,x[i])+a)[0])
plt.plot(np.array(close),label="expected")
plt.plot(predicted,label="predicted")
plt.legend()
plt.show()
```

```python
最佳拟合线:截距 [-0.0706178] ,回归系数： [-5.84296632e-01  8.01253717e-01  7.82481508e-01  4.49771542e-09
  3.64364866e-02]
predicted:35.555195,expected:35.630001
predicted:35.926904,expected:35.650002
predicted:35.804902,expected:35.770000
predicted:36.031468,expected:36.200001
predicted:36.146233,expected:36.169998
Test MSE: 0.022
```

即模型为
$$
Close = -0.0706178 - 0.584296632Open + 0.801253717High + 0.782481508Low + 4.49771542e-09/Volume+0.0364364866Adjuested
$$
均方误差为0.022，且画出图像后预测值与真实值的重合度很高。

<img src="https://s1.ax1x.com/2020/08/13/az6pMn.png" style="zoom: 80%;" />

## 结论：

| 模型                                           | 均方误差 |
| ---------------------------------------------- | -------- |
| state-space model                              | 0.145    |
| 用开盘价、最高价、收盘价进行拟合(机器学习模型) | 0.022    |
| 用五个变量进行拟合（机器学习模型）             | 0.023    |

对比了5个预测值的均方误差可以发现，state-space模型的均方误差为0.145，而两种多元线性模型的均方误差分别为0.022,0.023，所以，对于此数据来说，多元线性模型（机器学习）预测结果更好。