这是applied time series analysis 的笔记和习题代码

笔记按照章节整理，习题代码在每一章或者节之后

数据和图片见文件夹

作者：尹爱华  18数科

[TOC]

# 第1章：时间序列及其特征

## 1. 自回归

$$
r_k = \frac{\sum_{t=k+1}^T(x_t-\overline{x})(x_{t-k}-\overline{x})}{Ts^2}
$$


$$
\overline{x}为样本均值，s^2为样本方差
$$

$$
\overline{x} = T^{-1}\sum_{t=1}^Tx_t
$$
$$
s^2 = T^{-1}\sum_{t=1}^T(x_t-\overline{x})^2
$$

两个随机变量X和Y的相关系数定义为
$$
\rho(X,Y) = \rho_{xy} = \frac{\text{Cov}(X,Y)}{\sqrt{\text{Var}(X) \text{Var}(Y)}} = \frac{E[(X-\mu_x)(Y-\mu_y)]}{\sqrt{E(X-\mu_x)^2 E(Y-\mu_y)^2}}
$$


如果有(X,Y)的独立同分布样本
$$
(x_t, y_t)， t=1,2,\dots,T
$$
， 可估计相关系数为
$$
\hat\rho_{xy} = \frac{\sum_{t=1}^T (x_t - \bar x)(y_t - \bar y)} {\sqrt{\sum_{t=1}^T (x_t - \bar x)^2 \sum_{t=1}^T (y_t - \bar y)^2}}
$$


对于不独立的样本， 比如时间序列样本， 也可以计算相关系数， 其估计合理性需要一些模型假设。

对于联合分布非正态的情况， 有时相关系数不能很好地反映X和Y的正向或者负向的相关。 斯皮尔曼（Spearman）相关系数是计算X的样本的秩（名次）与Y的样本的秩之间的相关系数， 也称为Spearman rank correlation。

另一种常用的非参数相关系数是肯德尔tau(Kendall’s \tau)系数， 反映了一致数对和非一致数对之间的差别。 对随机向量(X, Y)， 设(X_1, Y_1), (X_2, Y_2)相互独立且联合分布与(X,Y)联合分布相同， 定义X和Y的肯德尔tau系数为
$$
\tau = P\left[ (X_1 - X_2)(Y_1 - Y_2) > 0 \right] - P\left[ (X_1 - X_2)(Y_1 - Y_2) < 0 \right]
$$
即两个观测的分量次序一致的概率减去分量次序相反的概率。 一致的概率越大，说明两个的正向相关性越强。

## 2. 季节性

经济和金融中的月度、季度数据一般有明显的周期， 日数据也会有按照周、月、年周期的变化。 这样的性质称为季节性， 含有周期变化的时间序列称为季节时间序列。

## 3.平稳性和非平稳性
**平稳性**：恒定的平均水平是一个符号，但不是唯一的。
**非平稳性**：如果平均水平不能视为常数。

**弱平稳序列**(宽平稳序列，weakly stationary time series): 如果时间序列\{ X_t \}存在有限的二阶矩且满足：

 (1)
$$
EX_t = \mu
$$
与t无关；

 (2) 
$$
\text{Var}(X_t) = \gamma_0
$$
与t无关;

 (3) 
$$
\gamma_k = \text{Cov}(X_{t-k}, X_t), k=1,2,\dots
$$
与t无关，

则称\{ X_t \}为弱平稳序列。

适当条件下可以用时间序列的样本估计自协方差函数， 这是用一条轨道的信息推断所有实验结果\Omega， 估计公式为
$$
\hat\gamma_k = \frac{1}{T} \sum_{t=k+1}^T (x_{t-k} - \bar x)(x_t - \bar x), k=0,1,\dots, T-1
$$
称\hat\gamma_k为样本自协方差。 注意这里用了1/T而不是1/(T-k)， 用1/(T-k)在获得无偏性的同时会造成一些理论上的困难。

（4）对时间序列
$$
\{ X_t, t \in \mathbb Z \}
$$
， 
$$
如果对任意的t \in \mathbb Z和正整数n, k， (X_t, \dots, X_{t+n-1})总是与 (X_{t+k}, \dots, X_{t+n-1+k})同分布， 则称\{ X_t \}为**严平稳**时间序列。
$$
如果严平稳时间序列\{X_t \}有二阶矩， 则它也是宽平稳的。

如果宽平稳时间序列\{X_t \}的任意有限维分布都服从广义正态分布， 则\{X_t \}也是严平稳的。

如果\{ \varepsilon_t \}为独立同分布零均值白噪声， 方差为\sigma^2， \{ \psi_j \}绝对可和或者平方可和， 则线性时间序列
$$
X_t = \mu + \sum_{j=0}^\infty \psi_j \varepsilon_{t-j},  t \in \mathbb Z
$$
同时宽平稳和严平稳的时间序列。

## 4.趋势
在许多时间序列中都可以找到趋势。
许多趋势没有恒定的斜率（非线性）。

## 5.波动

整个变化序列的特征是相对的平静时期，其间散布着波动，因此该序列的方差连续变化。

## 6.共同特征
```python
###########例题1.1：某地市近60天的天气情况的可视化
import re
import requests
from matplotlib import pyplot as plt
import numpy as np
# 获取威海近60天的最低温和最高温
html = requests.get('https://www.yangshitianqi.com/weihai/60tian.html').text
#使用正则提取数据
pattern_temperature = r'<div class="fl i3 nz">(\d+~\d+)℃</div>'
pattern_date = r'<div class="t2 nz">(\d\d\.\d\d)</div>'
temperature = re.findall(pattern_temperature, html)
date = re.findall(pattern_date, html)
# 整理数据
max_d = [int(i.split('~')[1]) for i in temperature]
print(max_d)
min_d = [int(i.split('~')[0]) for i in temperature]
print(min_d)
# 定义图像质量
plt.figure(figsize=(9, 4.5), dpi=180)
# 解决中文乱码
plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号
# 绘制图像
plt.plot(max_d, 'r-')
plt.plot(min_d, 'b-')
# xy轴标识
plt.xlabel('date', size=24)
plt.ylabel('tem/℃', size=24)
plt.title('the latest 60 days in Weihai', size=24)
# 显示网格
plt.grid(axis='y')
# 显示图像
plt.show()

x = np.arange(1,60)
max_d = np.array(max_d)
min_d = np.array(min_d)
print(max_d)
print(min_d)
bar_width = 0.3
plt.bar(x, min_d, bar_width)
plt.bar(x+bar_width, max_d, bar_width, align="center")
plt.xlabel('date', size=24)
plt.ylabel('tem/℃', size=24)
plt.title('the latest 60 days in Weihai', size=24)
# 展示图片
plt.show()
```

<img src="AppliedTimeSeries_yinaihua_files\1-1-2.png" alt="1-1-2" style="zoom: 80%;" />

# 第2章 转换时间序列

通常，对数据进行转换是合适的，该系列的初始图经常提供有关对ue进行哪种转换的线索。

时间序列的转换一般分为三种：Distributional，Stationarity inducing，Decompositional

## 1. 分布变换
+ 算法 : 
    + 减少水平中的极右偏度
    + 稳定方差
    + 不能引起正常和平稳 
    
+ 动力转换 :
  $$
  f^{BC}(x_t,\lambda)=\begin{cases}
  (x_{t}^{\lambda}-1)/{\lambda}, \lambda \neq0 \\
  logx_t, \lambda =0 \end{cases}
  $$
  
+ 转换功效:

$$
f^SP(x_t, \lambda) = (sgn(x_t)|x_{t}^\lambda|-1)\ \lambda,\lambda > 0
$$

+ 广义功率(GP)：

$$
f^{GP}(x_t,\lambda)=\begin{cases}
((x_t + 1)^{\lambda}-1)/ \lambda,x_t \geq0,\lambda \neq0 \\
log(x_t +1),          x_t \geq0,\lambda =0 \\
-((-x_t + 1)^{2-\lambda}-1)/(2-\lambda),x_t <0,\lambda \neq2 \\
-log(-x_t+1),x_t<0,\lambda \neq 2
\end {cases}
$$

+ 反双曲正弦（IHS）：
  $$
  f^{IHS}(x_t,\lambda) = \frac{sinh^{-1}(\lambda x_t)}{\lambda}=log \frac{\lambda x_t + (\lambda^2x_{t}^2+1)^{1/2}}{\lambda},\lambda > 0
  $$

```python
#########例题2.1：接口股票数据加载，数据是国金证券
import datetime
import pandas as pd
import tushare as ts  # 该模块是一个免费提供股票交易数据的API<br><br>
import matplotlib.pyplot as plt

# 我们将看看从2016年1月1日开始过去一年的股票价格
start = datetime.date(2016,1,1)
end = datetime.date.today()

# 得到国金证券公司的股票数据；股票代码是600109
# 第一个参数是获取股票数据的股票代码串，第二个参数是开始日期，第三个参数是结束日期
guojin = ts.get_k_data('600109',start='2020-07-06', end='2020-08-05',autype='qfq')
print (guojin.head())

plt.figure(figsize=(25, 10))
plt.plot(guojin['date'],guojin['close'])
plt.grid()
plt.title("GuoJin close in the last one month")
plt.show()
```

Running Result:

```python
           date   open  close   high    low     volume    code
120  2020-07-06  14.99  15.35  15.35  14.68  1761690.0  600109
121  2020-07-07  16.00  15.95  16.89  15.15  4353375.0  600109
122  2020-07-08  15.81  17.01  17.47  15.80  3407363.0  600109
123  2020-07-09  16.40  16.48  16.94  15.95  3252643.0  600109
124  2020-07-10  16.20  15.66  16.30  15.50  2173360.0  600109
125  2020-07-13  15.56  15.93  16.29  15.43  2057902.0  600109
```

![](AppliedTimeSeries_yinaihua_files\1-2.png)

## 2. 平稳诱导转变

+ 一阶差分 : 
  $$
  \nabla{x_t}=x_t-x_{t-1}
  $$

    + 滞后算子B：
  $$
    \nabla{x_t}=x_t-x_{t-1}=x_t-Bx_t=(1-B)x_t
  $$

+ 二阶差分 : 
  $$
  \nabla^2x_t=(1-B)^2x_t=(1-2B+B^2)x_t=x_t-2x_{t-1}+x_{t-2}
  $$

+ 两期差异 : 
  $$
  x_t-x_{t-2}=\nabla_2x_t
  $$

+ 比例/百分比变化：
  $$
  \frac{\nabla{x}_t}{x_{t-1}}
  $$

+ 
  $$
  如果y_t = (x_t - x_{t-1})/x_{t-1}较小
  $$
  
  $$
  \frac{x_t-x_{t-1}}{x_{t-1}} \approx logx_t - logx_{t-1} = \nabla{logx_t}
  $$

```python
###############例题2.2： 对葡萄酒数据进行差分可视化
import urllib.request
import numpy as np
import matplotlib.pyplot as plt

#加载数据集
url = "http://archive.ics.uci.edu//ml//machine-learning-databases//wine//wine.data"
raw_data = urllib.request.urlopen(url)
dataset_raw = np.loadtxt(raw_data, delimiter=",")
print(dataset_raw)
print("over")
```

Running Result:

```python
[[1.000e+00 1.423e+01 1.710e+00 ... 1.040e+00 3.920e+00 1.065e+03]
 [1.000e+00 1.320e+01 1.780e+00 ... 1.050e+00 3.400e+00 1.050e+03]
 [1.000e+00 1.316e+01 2.360e+00 ... 1.030e+00 3.170e+00 1.185e+03]
 ...
 [3.000e+00 1.327e+01 4.280e+00 ... 5.900e-01 1.560e+00 8.350e+02]
 [3.000e+00 1.317e+01 2.590e+00 ... 6.000e-01 1.620e+00 8.400e+02]
 [3.000e+00 1.413e+01 4.100e+00 ... 6.100e-01 1.600e+00 5.600e+02]]
```

```python
#选取最后一列的数据作为y值
data = []
for i in range(0,len(dataset_raw)):
    data.append(dataset_raw[i][-1])
print(data)
```

```python
import matplotlib.pyplot as plt
x = [i for i in range(0,len(data))]
y = data

plt.plot(x, y, ls="-", lw=2, label="white data")
plt.title("white data")
plt.legend()
plt.show()
```

<img src="AppliedTimeSeries_yinaihua_files\2-1.png" style="zoom:67%;" />

```python
#first-diffenence
for i in range(0,len(data)-1):
    data[i] = data[i+1] -  data[i]
x = [i for i in range(0,len(data))]
y = data

plt.plot(x, y, ls="-", lw=2, label="first-diffenence")
plt.title("first-diffenence")
plt.legend()
plt.show()
```

<img src="AppliedTimeSeries_yinaihua_files\2-2.png" style="zoom:67%;" />

```python
#second-diffenence
for i in range(0,len(data)-1):
    data[i] = data[i+1] -  data[i]
x = [i for i in range(0,len(data))]
y = data

plt.plot(x, y, ls="-", lw=2, label="second-diffenence")
plt.title("second-diffenence")
plt.legend()
plt.show()
```

<img src="AppliedTimeSeries_yinaihua_files\2-3.png" style="zoom:67%;" />

##  3. 分解时间序列并平滑转换

+ 数据=平滑+粗糙
+ 最简单的MA:

$$
MA(3) = \frac{1}{3}(x_{t-1}+x_t+x_{t+1})
$$

+ 更复杂的公式（奇数）:
  $$
  WMA_t(2n+1) = \sum_{i=-n}^nw_ix_{t-i}
  $$
  权重 $w_i$:
  $$
  \sum_{i=-n}^nw_i=1
  $$

+ 更复杂的公式（偶数）:
  $$
  MA_{t-1/2}(4) = \frac{1}{4}(x_{t-2}+x{t-1}+x{t}+x{t+1})
  $$

  $$
  MA_{t+1/2}(4) = \frac{1}{4}(x_{t-1}+x{t}+x{t+1}+x{t+2})
  $$

  $$
  MA_{t}(5) = \frac{1}{8}x_{t-2}+\frac{1}{4}x_{t-1}+\frac{1}{4}x_{t}+\frac{1}{4}x_{t+1}+\frac{1}{8}x_{t+2}
  $$

+ 表示为$ X_t $，分解为趋势$ T_t $，季节性$ S_t $和不规则$ I_t $:
    + 加法

    $$
    X_t=T_t+S_t+I_t
    $$

    + 乘法

    $$
    X_t=T_t×S_t×I_t
    $$

    + 经过季节性调整：
      
      
        $$
X_t^{SA,A}=T_t-S_t=T_t+I_t
        $$
        
        $$
        X_t^{SA,M}=\frac{X_t}{S_t}=T_t×I_t
        $$

 

```python
##########例题2.3：周期性可视化，使用的是天然气近三年数据
import pandas as pd
import matplotlib.pylab as plt
data = pd.read_excel("C:/Users/Dell/Desktop/Timeseries/project/Book_dataset/天然气近三年数据.xls")
print(data)
data = data.fillna(method='pad')

plt.figure(figsize=(25, 10))
plt.plot(data['指标'],data['天然气产量当期值(亿立方米)'],'b-')
plt.plot(data['指标'],data['天然气产量累计值(亿立方米)'],'r-')
plt.plot(data['指标'],data['天然气产量同比增长(%)'],'g:')
plt.title("Trend of natural gas in China in recent three years")

plt.legend(["Current value of natural gas production",
            "Cumulative value of natural gas production",
            "Year on year growth of natural gas production(%"])
plt.show()
```

![](AppliedTimeSeries_yinaihua_files\2-4.png)

# 第3章：平稳时间序列的ARMA模型

## 1. 随机过程和平稳性

+ T维概率分布
    + T 均值
      $$
      E(x_1),E(x_2),...,E(x_T)
      $$
    
    + T方差
      $$
      V(x_1),V(x_2),...,V(x_T)
      $$
    
    + 协方差
      $$
      Cov(x_i,x_j),i<j
      $$
    
+ 严格固定
     + 不受时间变化的影响

     + 在任何时间集合$ t_1，t_2，...，t_m $上的联合概率分布必须与在$ t_ {1 + k}，t_ {2 + k}，...，t_处的联合概率分布相同 {m + k} $，其中k是时间的任意偏移
         + m = 1
           $$
            E|X_T^2|<+\infty
           $$
           
           $$
            E(x_1)=E(x_2)=\dots=E(x_T)=\mu
           $$
           
           $$
           V(x_1)=V(x_2)=\dots=V(x_T)=\sigma^2
           $$
           
         + m = 2
         $$
           Cov(x_1,x_{1+k})=Cov(x_2,x_{2+k})=\dots=Cov(x_{T-k},x_{T})=Cov(x_t,x_{t-k})
         $$
         
         + k阶自相关
           
         $$
         \gamma_k= Cov(x_t,x_{t-k})=E((x_t-\mu)(x_{t-k}-\mu))
         $$
         $$
         \gamma_0=E(x_t-\mu)^2=V(x_t)=\sigma_x^2
         $$
         
     + ACF
         $$
         \rho_k=\frac{Cov(x_t,x_{t-k})}{(V(x_t)V(x_{t-k}))^{1/2}}=\frac{\gamma_k}{\gamma_0}=\frac{\gamma_k}{\sigma_x^2}
         $$


## 2. 分解和自相关

+ 线性滤波器表示：
  $$
  x_t-\mu=a_t+\psi_1a_{t-1}+\psi_2a_{t-2}+\dots=\sum_{j=0}^\infty \psi_ja_{t-j}\psi_0=1
  $$

    +  $a_i$:
        $$
        E(a_t)=0
        $$
        
        $$
        V(a_t)=\sigma^2<\infty
        $$
        
        $$
        Cov(a_t,a_{t-k})=0,k \neq 0
        $$
        
    + $x_i$:
        $$
        E(x_t)=\mu
        $$

        $$
        \gamma_0=V(x_t)=\sigma^2\sum_{j=0}^\infty \psi_j^2
        $$

        $$
        \gamma_k=Cov(a_t,a_{t-k})=\sigma^2\sum_{j=0}^\infty \psi_j \psi_{j+k}
        $$

        $$
        \rho_k=\frac{\sum_{j=0}^\infty \psi_j \psi_{j+k}}{\sum_{j=0}^\infty \psi_j^2}
        $$

## 3. 一阶自回归过程

+ AR(1)
    + \mu=0 

    $$
    \psi_j=\phi^j
    $$

    $$
     x_t = \phi x_{t-1}+a_t
    $$

    或者
    $$
    x_t- \phi x_{t-1}=a_t
    $$

## 4. 一阶移动平均过程

+ MA(1)
    + \mu=0
        $$
    \psi_1=-\theta
        $$
        
        $$
        \psi_j=0(j\ge2)
        $$
        
        $$
        x_t=a_t-\theta a_{t-1}
        $$
        
        或者
        $$
        x_t=(1-\theta B)a_t
        $$

## 5. 生成AR和MA过程

+ AR(p):

$$
x_t-\phi_1 x_{t-1}-\phi_2x_{t-2}-\dots-\phi_px_{t-p}=a_t
$$

 AR(p) 过程的描述：

1. ACF的范围是无限的，并且是阻尼指数和阻尼正弦波的组合，并且
2. 2.对于滞后大于$ p $的PACF为零

+ MA(q):

$$
x_t=a_t-\theta_1 a_{t-1}-\dots-\theta_qa_{t-q}
$$

AR和MA流程之间存在重要的对偶关系：

> 虽然AP（p）进程的ACF在范围上是无限的，但PACF在滞后$ p $之后削减了。
>
> MA（q）过程的ACF在滞后$ q $之后被切除，而PACF在范围上是无限的.

## 6. ARMA模型

+ ARMA(1,1):

$$
x_t-\phi x_{t-1} = a_t-\theta a_{t-1}
$$

+ ARMA(p,q):

$$
x_t-\phi_1 x_{t-1}-\dots-\phi_px_{t-p}=a_t-\theta_1 a_{t-1}-\dots-\theta_qa_{t-q}
$$

## 7. ARMA模型的建立和评估

+ 获取参数:  $\mu$,$\sigma_x^2$ 与 $ \rho_k$
    + $\mu$ :样本均值
    + $\sigma_x^2$: 样本方差
+ 程序
    + 鉴定阶段
    + 估计未知模型参数
    + 诊断检查
+ 如何选择合适的模型
     + AIC:
       $$
       AIC(p,q)=log{\widehat{\sigma}}^2+2(p+q)T^{-1}
       $$
     
     + BIC:
       $$
       BIC(p,q)=log{\widehat{\sigma}^2}+(p+q)T^{-1}logT
       $$



```python
##########################例题3.1   ACF&PACF可视化，以600115股票数据为例
# 0)导入包
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pandas.plotting import register_matplotlib_converters
```

```python
# 1）获取数据
data = pd.read_csv('C:/Users/Dell/Desktop/Timeseries/project/Book_dataset/600115.csv', index_col='Date')

print(data.head())
print(data.dtypes)
```

Running Result:

```python
 High   Low  Open  Close      Volume  Adj Close
Date                                                      
2010-01-04  6.19  6.08  6.19   6.09   7638975.0   5.948295
2010-01-05  6.13  6.01  6.09   6.11   6456808.0   5.967830
2010-01-06  6.23  6.08  6.11   6.16  12527866.0   6.016666
2010-01-07  6.34  6.11  6.16   6.21  15031799.0   6.065503
2010-01-08  6.30  5.90  6.30   6.10  21136398.0   5.958062
High         float64
Low          float64
Open         float64
Close        float64
Volume       float64
Adj Close    float64

dtype: object
```

```python
# 2）Date类型转换为时间

data.index = pd.to_datetime(data.index)
print("Select 2010-01:\n", data['2010-01'])
```

```python
# 3）数据预处理
# 首先获取收盘数据，并将其翻转下顺序，因为前面的数据截图可以看到，数据是逆序的，所以需要处理下。
ts = data['Close']
ts = ts[::-1]
print('ts',ts)

# 日收益率
ts_ret = np.diff(ts)
print('日收益率', ts_ret)

# 对数收益率
ts_log = np.log(ts)
ts_diff = ts_log.diff(1)
ts_diff.dropna(inplace=True)
print('对数收益率', ts_diff)

# 4）数据展示
register_matplotlib_converters()

plt.figure()
plt.grid()
plt.plot(ts, color='blue', label='Original')
print('ts',ts)
```

Running Result:

```python
ts Date
2019-05-10    6.30
2019-05-09    6.11
2019-05-08    6.22
2019-05-07    6.34
2019-05-06    6.21
              ... 
2010-01-08    6.10
2010-01-07    6.21
2010-01-06    6.16
2010-01-05    6.11
2010-01-04    6.09
Name: Close, Length: 2268, dtype: float64
            
日收益率 [-0.19000006  0.10999966  0.12000036 ... -0.05000019 -0.04999971
 -0.01999998]

对数收益率 Date
2019-05-09   -0.030623
2019-05-08    0.017843
2019-05-07    0.019109
2019-05-06   -0.020718
2019-04-26    0.060909
                ...   
2010-01-08   -0.016261
2010-01-07    0.017872
2010-01-06   -0.008084
2010-01-05   -0.008150
2010-01-04   -0.003279
Name: Close, Length: 2267, dtype: float64

ts Date
2019-05-10    6.30
2019-05-09    6.11
2019-05-08    6.22
2019-05-07    6.34
2019-05-06    6.21
              ... 
2010-01-08    6.10
2010-01-07    6.21
2010-01-06    6.16
2010-01-05    6.11
2010-01-04    6.09
Name: Close, Length: 2268, dtype: float64

```

```python
# 5）单位根检验

from statsmodels.tsa.stattools import adfuller

def adf_test(ts):
    adftest = adfuller(ts)
    adf_res = pd.Series(adftest[0:4], index=['Test Statistic', 'p-value', 'Lags Used', 'Number of Observations Used'])

    for key, value in adftest[4].items():
        adf_res['Critical Value (%s)' % key] = value
    return adf_res

adftest = adf_test(ts)
print('单位根检验',adftest)
```

Running Result:

```python
单位根检验 Test Statistic                   -1.628951
p-value                           0.468040
Lags Used                        27.000000
Number of Observations Used    2240.000000
Critical Value (1%)              -3.433273
Critical Value (5%)              -2.862831
Critical Value (10%)             -2.567457
dtype: float64
```

```python
# 6）定阶（ACF PACF）
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

def draw_acf_pacf(ts, w):
    plt.clf()
    fig = plt.figure()
    ax1 = fig.add_subplot(211)
    plot_acf(ts, ax=ax1, lags=w)

    ax2 = fig.add_subplot(212)
    plot_pacf(ts, ax=ax2, lags=w)

    plt.show()

draw_acf_pacf(ts, 27)
```

<img src="AppliedTimeSeries_yinaihua_files\3-1.png" style="zoom:80%;" />



```python
###########例题3.2： AR MA和ARMA模型对比，以600115股票数据为例
# 0）导入包
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
```

```python
# 1)获取数据
data = pd.read_csv('C:/Users/Dell/Desktop/Timeseries/project/Book_dataset/600115.csv', index_col='Date')
data.index = pd.to_datetime(data.index)
ts = data['Close']

print(ts.axes)
```

Running Result:

```python
[DatetimeIndex(['2010-01-04', '2010-01-05', '2010-01-06', '2010-01-07',
               '2010-01-08', '2010-01-11', '2010-01-12', '2010-01-13',
               '2010-01-14', '2010-01-15',
               ...
               '2019-04-22', '2019-04-23', '2019-04-24', '2019-04-25',
               '2019-04-26', '2019-05-06', '2019-05-07', '2019-05-08',
               '2019-05-09', '2019-05-10'],
              dtype='datetime64[ns]', name='Date', length=2268, freq=None)]
```

```python
# 2）AR模型
from statsmodels.tsa.arima_model import ARMA, ARIMA
from pandas.plotting import register_matplotlib_converters

register_matplotlib_converters()

def draw_ar(ts, w):
    arma = ARMA(ts, order=(w, 0)).fit(disp=-1)
    ts_predict = arma.predict('2016', '2019', dynamic=True)

    plt.clf()
    plt.plot(ts_predict, 'r:',label="PDT")
    plt.plot(ts, '-',label="ORG")
    plt.legend(loc="best")
    plt.title("AR Test %s" % w)

    plt.show()

draw_ar(ts,4)
```

<img src="AppliedTimeSeries_yinaihua_files\3-2.png" style="zoom:67%;" />

<img src="AppliedTimeSeries_yinaihua_files\3-2-2.png" style="zoom:67%;" />

```python
# 3）MA模型
def draw_ma(ts, w):
    ma = ARMA(ts, order=(0, w)).fit(disp = -1)
    ts_predict_ma = ma.predict()

    plt.clf()
    plt.plot(ts, label="ORG")
    plt.plot(ts_predict_ma, ':',label="PDT")
    plt.legend(loc="best")
    plt.title("MA Test %s" % w)
    plt.show()

    return ts_predict_ma
draw_ma(ts,1)
```

<img src="AppliedTimeSeries_yinaihua_files\3-3.png" style="zoom:67%;" />

<img src="AppliedTimeSeries_yinaihua_files\3-3-2.png" style="zoom:67%;" />

```python
# 4)ARMA模型
#ARMA模型
ts.describe()
```

Running Result:

```python
count    2268.000000
mean        5.668558
std         2.047711
min         2.260000
25%         3.850000
```

```python
# 确定阶数
from statsmodels.tsa.arima_model import ARMA
from itertools import product

# 设置p阶，q阶范围
# product p,q的所有组合
# 设置最好的aic为无穷大
# 对范围内的p,q阶进行模型训练，得到最优模型
ps = range(0, 6)
qs = range(0, 6)
parameters = product(ps, qs)
parameters_list = list(parameters)

best_aic = float('inf')
results = []
for param in parameters_list:
    try:
        model = ARMA(ts, order=(param[0], param[1])).fit()
    except ValueError:
        print("参数错误：", param)
        continue
    aic = model.aic
    if aic < best_aic:
        best_model = model
        best_aic = model.aic
        best_param = param
    results.append([param, model.aic])
results_table = pd.DataFrame(results)
results_table.columns = ['parameters', 'aic']
print("最优模型", best_model.summary())
```

Running Result:

```python
最优模型                               ARMA Model Results                              
==============================================================================
Dep. Variable:                  Close   No. Observations:                 2268
Model:                     ARMA(3, 5)   Log Likelihood                 532.751
Method:                       css-mle   S.D. of innovations              0.191
Date:                Sun, 09 Aug 2020   AIC                          -1045.501
Time:                        23:43:45   BIC                           -988.235
Sample:                             0   HQIC                         -1024.608
                                                                              
===============================================================================
                  coef    std err          z      P>|z|      [0.025      0.975]
-------------------------------------------------------------------------------
const           5.7433      0.728      7.887      0.000       4.316       7.170
ar.L1.Close     1.0883      0.026     41.215      0.000       1.037       1.140
ar.L2.Close    -1.0313      0.036    -28.385      0.000      -1.102      -0.960
ar.L3.Close     0.9326      0.023     39.820      0.000       0.887       0.978
ma.L1.Close    -0.0431      0.033     -1.290      0.197      -0.109       0.022
ma.L2.Close     0.9178      0.031     29.704      0.000       0.857       0.978
ma.L3.Close     0.0752      0.028      2.655      0.008       0.020       0.131
ma.L4.Close     0.0243      0.022      1.115      0.265      -0.018       0.067
ma.L5.Close     0.0494      0.022      2.200      0.028       0.005       0.093
                                    Roots                                    
=============================================================================
                  Real          Imaginary           Modulus         Frequency
-----------------------------------------------------------------------------
AR.1            1.0057           -0.0000j            1.0057           -0.0000
AR.2            0.0501           -1.0314j            1.0326           -0.2423
AR.3            0.0501           +1.0314j            1.0326            0.2423
MA.1            0.0381           -1.0606j            1.0612           -0.2443
MA.2            0.0381           +1.0606j            1.0612            0.2443
MA.3           -2.7638           -0.0000j            2.7638           -0.5000
MA.4            1.0980           -2.3031j            2.5514           -0.1792
MA.5            1.0980           +2.3031j            2.5514            0.1792
-----------------------------------------------------------------------------

```

```python
from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()

plt.plot(ts['2010':'2015'],label = 'ORG')
arma = ARMA(ts, order=(3, 5)).fit(disp = -1)
ts_predict_arma = arma.predict()
plt.plot(ts_predict_arma['2016':'2019'],'r:',label = 'PRE')
plt.title("ARMA(3,5)")
plt.legend()
plt.show()
```

<img src="AppliedTimeSeries_yinaihua_files\3-4.png" style="zoom:67%;" />

<img src="AppliedTimeSeries_yinaihua_files\3-4-2.png" style="zoom:67%;" />



```python
##################例题3.3   AR模型不同参数对比，以600115股票数据为例
import pandas as pd
import matplotlib.pyplot as plt

# 1)获取数据
data = pd.read_csv('C:/Users/Dell/Desktop/Timeseries/project/Book_dataset/600115.csv', index_col='Date')
data.index = pd.to_datetime(data.index)
ts = data['High']

print(ts)

# 2）AR模型选择阶数
from statsmodels.tsa.arima_model import ARMA
from datetime import datetime
from itertools import product

# 设置p阶，q阶范围
# product p,q的所有组合
# 设置最好的aic为无穷大
# 对范围内的p,q阶进行模型训练，得到最优模型

best_aic = float('inf')
results = []
AIC,BIC = list(), list()
for ps in range(1,13):
    try:
        model = ARMA(ts, order=(ps, 0)).fit()
    except ValueError:
        print("参数错误：", ps)
        continue
    aic = model.aic
    bic = model.bic
    AIC.append(aic)
    BIC.append(bic)
    if aic < best_aic:
        best_model = model
        best_aic = model.aic
        best_param = ps
    results.append([ps, model.aic])
results_table = pd.DataFrame(results)
results_table.columns = ['parameters', 'aic']
# print("最优模型", best_model.summary())

ans = pd.DataFrame({'AIC':AIC,'BIC':BIC})
print(ans)

plt.plot(AIC,'b-',label = "AIC")
plt.plot(BIC,'r-',label = 'BIC')
plt.legend()
plt.show()
```

Running Result:

```python
k          AIC        BIC
1   10.593533  10.421733
2   10.915990  10.686924
3   10.973159  10.686826
4   11.009227  10.665628
5   10.989278  10.588412
6   10.978642  10.520510
7   10.959007  10.443608
8   11.007505  10.434840
9   11.058899  10.428967
10  11.051479  10.364281
11  11.119319  10.374855
12  11.099830  10.298098
```

<img src="AppliedTimeSeries_yinaihua_files\3-5.png" style="zoom:67%;" />

# 第4章：非平稳时间序列的ARIMA模型

## 1.非平稳性

+ ARMA模型基于以下假设：基本过程是弱固定的，这限制了均值和方差是恒定的，并且要求自协方差仅取决于“时滞”
+ 非恒定平均水平加上随机误差分量的总和：

$$
x_t = \mu_t + \epsilon+t
$$

​		$\mu _t$ 是非恒定平均水平。

+ 误差分量假定为白噪声序列

$$
x_t = \beta _0+\beta _1 t+ a_t
$$

## 2. ARIMA 过程

+ $x_t$遵循随机过程
  $$
  x_t = x_{t-1} + a_t
  $$

+ $x_t$遵循有漂移的随机过程
  $$
  x_t = x_0+t \theta_0+\sum_{i=0}^t a_{t-i}
  $$
  所以
  $$
  E(x_t) = x_0+t\theta_0
  $$

  $$
  \gamma_{0,t}=V(x_t)=t \sigma^2
  $$

  $$
  \gamma_{k,t}=Cov(x_t,x_{t-k})=(t-k) \sigma^2
  $$

+ x_t与x_(t-1)之间的自相关
  $$
   \rho_{k,t}=\frac{\gamma_{k,t}}{({\gamma_{0,t}\gamma_{0,t-k})}^{\frac{1}{2}}}=({\frac{t-k}{t}})^\frac{1}{2}
  $$

## 3. ARIMA模型

```python
########################例题4.1ARIMA模型-以湖北省GDP为例
# 0) 导入包
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.graphics.tsaplots import acf,pacf,plot_acf,plot_pacf
from statsmodels.tsa.arima_model import ARMA

# 1）数据画图
time_series = pd.Series([151.0, 188.46, 199.38, 219.75, 241.55, 262.58, 328.22, 396.26, 442.04, 517.77, 626.52, 717.08, 824.38, 913.38, 1088.39, 1325.83, 1700.92, 2109.38, 2499.77, 2856.47, 3114.02, 3229.29, 3545.39, 3880.53, 4212.82, 4757.45, 5633.24, 6590.19, 7617.47, 9333.4, 11328.92, 12961.1, 15967.61])
time_series.index = pd.Index(sm.tsa.datetools.dates_from_range('1978','2010'))
time_series.plot(figsize=(6,4))
plt.title("HuBei GDP(1978~2010)")
plt.show()
```

<img src="AppliedTimeSeries_yinaihua_files\4-1.png" style="zoom:80%;" />

The time series is in exponential form with high fluctuation, not stable. You take **the log** of it, and turn it into a linear trend.

```python
# 2）转化
time_series = np.log(time_series)
time_series.plot(figsize=(6,4))
plt.title("HuBei GDP_log")
plt.show()
```

<img src="AppliedTimeSeries_yinaihua_files\4-2.png" style="zoom: 80%;" />

为了确定其稳定性，对数后对数据进行了** ADF单位根检验**。

```python
# 3)单位根检验
t=sm.tsa.stattools.adfuller(time_series, )
output=pd.DataFrame(index=['Test Statistic Value', "p-value", "Lags Used", "Number of Observations Used","Critical Value(1%)","Critical Value(5%)","Critical Value(10%)"],columns=['value'])
output['value']['Test Statistic Value'] = t[0]
output['value']['p-value'] = t[1]
output['value']['Lags Used'] = t[2]
output['value']['Number of Observations Used'] = t[3]
output['value']['Critical Value(1%)'] = t[4]['1%']
output['value']['Critical Value(5%)'] = t[4]['5%']
output['value']['Critical Value(10%)'] = t[4]['10%']
print(output)
```

Running Reslut:

```python
                                value
Test Statistic Value         0.807369
p-value                      0.991754
Lags Used                           1
Number of Observations Used        31
Critical Value(1%)           -3.66143
Critical Value(5%)           -2.96053
Critical Value(10%)          -2.61932
```

根据上述检验，** t统计量大于任何置信度的临界值**，因此可以认为该序列是非平稳**。

因此，对序列进行“区别对待”并进行“ ADF测试”。

```python
# 4）差分并进行ADF检验
time_series = time_series.diff(1)
time_series = time_series.dropna(how=any)
time_series.plot(figsize=(8,6))
plt.title("First-difference")
plt.show()
t=sm.tsa.stattools.adfuller(time_series)
output=pd.DataFrame(index=['Test Statistic Value', "p-value", "Lags Used", "Number of Observations Used","Critical Value(1%)","Critical Value(5%)","Critical Value(10%)"],columns=['value'])
output['value']['Test Statistic Value'] = t[0]
output['value']['p-value'] = t[1]
output['value']['Lags Used'] = t[2]
output['value']['Number of Observations Used'] = t[3]
output['value']['Critical Value(1%)'] = t[4]['1%']
output['value']['Critical Value(5%)'] = t[4]['5%']
output['value']['Critical Value(10%)'] = t[4]['10%']
print(output)
```

Running Result:

<img src="C:figure\4-3.png" style="zoom:67%;" />

```python
                                  value
Test Statistic Value           -3.52276
p-value                      0.00742139
Lags Used                             0
Number of Observations Used          31
Critical Value(1%)             -3.66143
Critical Value(5%)             -2.96053
Critical Value(10%)            -2.61932
```

“差异”之后的序列“基本稳定”并通过ADF测试。

因此，ARIMA：** I = 1 **

然后，根据时间序列的识别规则，结合** ACF图，PAC图，AIC准则和BIC准则**确定ARMA模型的顺序。

**具有最小AIC和BIC值**的组应被选择为理想顺序。.

```python
# 5) p,q
plot_acf(time_series)
plot_pacf(time_series)
plt.show()

r,rac,Q = sm.tsa.acf(time_series, qstat=True)
prac = pacf(time_series,method='ywmle')
table_data = np.c_[range(1,len(r)), r[1:],rac,prac[1:len(rac)+1],Q]
table = pd.DataFrame(table_data, columns=['lag', "AC","Q", "PAC", "Prob(>Q)"])

print(table)
```

Running Result:

<img src="AppliedTimeSeries_yinaihua_files\4-4-1.png" style="zoom:67%;" />

<img src="AppliedTimeSeries_yinaihua_files\4-4-2.png" alt="4-4-2" style="zoom:67%;" />

```python
lag        AC          Q       PAC  Prob(>Q)
0    1.0  0.394807   5.470618  0.394807  0.019339
1    2.0  0.154901   6.340817 -0.001150  0.041986
2    3.0  0.055267   6.455413 -0.006522  0.091438
3    4.0 -0.248371   8.852447 -0.317338  0.064895
4    5.0 -0.260612  11.589317 -0.074026  0.040870
5    6.0 -0.208604  13.410270 -0.056045  0.036964
6    7.0 -0.285771  16.964330 -0.177312  0.017628
7    8.0 -0.174044  18.337541 -0.069038  0.018833
8    9.0 -0.030905  18.382724  0.005425  0.030984
9   10.0  0.029119  18.424658  0.007243  0.048209
10  11.0 -0.066373  18.652901 -0.262074  0.067616
11  12.0 -0.146481  19.820146 -0.259094  0.070566
12  13.0 -0.072985  20.125172 -0.032766  0.092133
13  14.0 -0.127924  21.114325 -0.175995  0.098738
14  15.0  0.014849  21.128436 -0.019744  0.132776
15  16.0  0.162653  22.927440 -0.002973  0.115688
16  17.0  0.106608  23.751802 -0.076931  0.126337
17  18.0  0.114241  24.766054 -0.159748  0.131497
18  19.0  0.138092  26.362025 -0.125241  0.120414
19  20.0  0.094671  27.174631 -0.002794  0.130422
20  21.0  0.021795  27.221614 -0.110668  0.163638
21  22.0  0.044433  27.436416  0.011325  0.195118
22  23.0  0.036667  27.598943  0.022050  0.231329
23  24.0 -0.076350  28.391726 -0.152129  0.243865
24  25.0 -0.035761  28.590497 -0.112621  0.281395
25  26.0  0.013603  28.624051 -0.040208  0.328453
26  27.0 -0.118207  31.664578 -0.105866  0.244735
27  28.0 -0.028374  31.883563  0.008338  0.279275
28  29.0  0.017351  31.992746  0.015010  0.320192
29  30.0 -0.070272  34.679110 -0.043223  0.254504
30  31.0  0.051978  37.618628  0.022608  0.191978
```

```python
#自动取阶p和q 的最大值，即函数里面的max_ar,和max_ma。
#ic 参数表示选用的选取标准，这里设置的为aic,当然也可以用bic。然后函数会算出每个 p和q 组合(这里是(0,0)~(3,3)的AIC的值，取其中最小的。

(p, q) =(sm.tsa.arma_order_select_ic(time_series,max_ar=3,max_ma=3,ic='aic')['aic_min_order'])
print((p,q))
```

Running Result:

```python
(0, 1)
```

```python
# 6) ARIMA(0,1,1)
p,d,q = (0,1,1)
arma_mod = ARMA(time_series,(p,d,q)).fit(disp=-1,method='mle')
summary = (arma_mod.summary2(alpha=.05, float_format="%.8f"))
print(summary)
```

Running result:

```python
 Results: ARMA
====================================================================
Model:              ARMA             BIC:                 -92.9078  
Dependent Variable: y                Log-Likelihood:      51.653    
Date:               2020-08-10 16:36 Scale:               1.0000    
No. Observations:   32               Method:              mle       
Df Model:           2                Sample:              12-31-1979
Df Residuals:       30                                    12-31-2010
Converged:          1.0000           S.D. of innovations: 0.048     
No. Iterations:     16.0000          HQIC:                -95.847   
AIC:                -97.3050                                        
-----------------------------------------------------------------------
            Coef.     Std.Err.       t       P>|t|     [0.025    0.975]
-----------------------------------------------------------------------
const       0.1489      0.0132    11.2852    0.0000    0.1230    0.1748
ma.L1.y     0.5719      0.1713     3.3382    0.0023    0.2361    0.9077
-----------------------------------------------------------------------------
                 Real           Imaginary          Modulus          Frequency
-----------------------------------------------------------------------------
MA.1           -1.7486             0.0000           1.7486             0.5000
====================================================================
```

```python
# 7）白噪声检验
arma_mod = ARMA(time_series,(0,1,2)).fit(disp=-1,method='mle')
resid = arma_mod.resid
t=sm.tsa.stattools.adfuller(resid)
output=pd.DataFrame(index=['Test Statistic Value', "p-value", "Lags Used", "Number of Observations Used","Critical Value(1%)","Critical Value(5%)","Critical Value(10%)"],columns=['value'])
output['value']['Test Statistic Value'] = t[0]
output['value']['p-value'] = t[1]
output['value']['Lags Used'] = t[2]
output['value']['Number of Observations Used'] = t[3]
output['value']['Critical Value(1%)'] = t[4]['1%']
output['value']['Critical Value(5%)'] = t[4]['5%']
output['value']['Critical Value(10%)'] = t[4]['10%']
print(output)

plot_acf(resid)
plot_pacf(resid)
plt.show()
```

Running Reslut:

```python
 value
Test Statistic Value           -3.114
p-value                      0.025534
Lags Used                           1
Number of Observations Used        30
Critical Value(1%)           -3.66992
Critical Value(5%)           -2.96407
Critical Value(10%)          -2.62117
```

<img src="AppliedTimeSeries_yinaihua_files\4-5-1.png" style="zoom:67%;" />

<img src="AppliedTimeSeries_yinaihua_files\4-5-2.png" alt="4-5-2" style="zoom:67%;" />



```python
#################例题4.2 ARIMA模型-自行构建数据集
# 0)导入包
import pandas as pd
import numpy as np
import seaborn as sns #热力图
import itertools
import datetime
import matplotlib.pyplot as plt
import statsmodels.api as sm
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.stattools import adfuller #ADF检验
from statsmodels.stats.diagnostic import acorr_ljungbox #白噪声检验
from statsmodels.graphics.tsaplots import plot_acf,plot_pacf #画图定阶
from statsmodels.tsa.arima_model import ARIMA #模型
from statsmodels.tsa.arima_model import ARMA #模型
from statsmodels.stats.stattools import durbin_watson #DW检验
from statsmodels.graphics.api import qqplot #qq图
```

```python
# 1)数据建立
def generate_data():
    index = pd.date_range(start='2018-1-1',end = '2018-9-1',freq='10T')
    index = list(index)
    data_list = []
    for i in range(len(index)):
        data_list.append(np.random.randn())
    dataframe = pd.DataFrame({'time':index,'values':data_list})
    dataframe.to_csv('C:/Users/Dell/Desktop/Timeseries/project/Book_dataset/4-2-old_data.csv',index=0)
    print('the data is existting')
generate_data()
```



```python
# 2)数据处理
def data_handle():
    data = pd.read_csv('C:/Users/Dell/Desktop/Timeseries/project/Book_dataset/4-2-old_data.csv')
    print(data.describe())  # 查看统计信息,发现最小值有-10000的异常数据
    print((data.isnull()).sum())  # 查看是否存在缺失值
    print((data.duplicated()).sum())  # 重复值

    def change_zero(x):
        if x == -10000:
            return 0
        else:
            return x

    data['values'] = data['values'].apply(lambda x: change_zero(x))

    # 利用均值填充缺失值
    mean = data['values'].mean()

    def change_mean(x):
        if x == 0:
            return mean
        else:
            return x

    data['values'] = data['values'].apply(lambda x: change_mean(x))
    # 保存处理过的数据
    data.to_csv('C:/Users/Dell/Desktop/Timeseries/project/Book_dataset/4-2-new_data.csv', index=0)
    print('new data is existing')
data_handle()
```

Running Result:

```python
             values
count  34993.000000
mean      -1.426390
std      119.532179
min   -10000.000000
25%       -0.676529
50%        0.007056
75%        0.672312
max        4.220282
time      0
values    0
dtype: int64
0
new data is existing
```

```python
# 3）数据重采样
def Resampling():  # 重采样
    df = pd.read_csv('C:/Users/Dell/Desktop/Timeseries/project/Book_dataset/4-2-new_data0.csv')
    # 将默认索引方式转换成时间索引
    df['time'] = pd.to_datetime(df['time'])
    df.set_index("time", inplace=True)

    data = df['2018-1-1':'2018-8-1']  # 取18-1-1到18-8-1做预测
    test = df['2018-8-1':'2018-9-1']
    data_train = data.resample('D').mean()  # 以一天为时间间隔取均值,重采样
    data_test = test.resample('D').mean()

    return data_train, data_test


data_train, data_test = Resampling()
```

+ 接下来，以固定方式处理数据。 有两种方法：

  差分法
  平滑方法

```python
# 4-1）差分法
def stationarity(timeseries):  # 平稳性处理
    # 差分法(不平稳处理),保存成新的列,1阶差分,dropna() 删除缺失值
    diff1 = timeseries.diff(1).dropna()
    diff2 = diff1.diff(1)  # 在一阶差分基础上做二阶差分

    diff1.plot(color='red', title='diff 1', figsize=(10, 4))
    plt.show()
    diff2.plot(color='black', title='diff 2', figsize=(10, 4))
    plt.show()

stationarity(data_train)
```



<img src="AppliedTimeSeries_yinaihua_files\4-8-1.png" style="zoom: 80%;" />

<img src="AppliedTimeSeries_yinaihua_files\4-8-2.png" alt="4-8-2" style="zoom:80%;" />

从以上分析可以得出，如果使用差分方法，我们可以选择一阶差分。

```python
# 4-2）平滑法
timeseries = data_train
# 滚动平均（平滑法不平稳处理）
rolmean = timeseries.rolling(window=4, center=False).mean()
# 滚动标准差
rolstd = timeseries.rolling(window=4, center=False).std()

rolmean.plot(color='green', title='Rolling Mean', figsize=(10, 4))
plt.show()
rolstd.plot(color='blue', title='Rolling Std', figsize=(10, 4))
plt.show()
```

<img src="AppliedTimeSeries_yinaihua_files\4-9-1.png" style="zoom:80%;" />

<img src="AppliedTimeSeries_yinaihua_files\4-9-2.png" alt="4-9-2" style="zoom:80%;" />

可以看出，平滑方法不适用于此数据（通常，此方法更适用于具有“周期性且稳定上升”的数据类型）。

从以上分析中可以得出，我们使用差分法并选择“一阶差分”对数据进行平滑处理。 ARIMA ** I = 1 **

```python
# 5)ADF检验
diff1 = timeseries.diff(1).dropna()

x = np.array(diff1['values'])
adftest = adfuller(x, autolag='AIC')
print(adftest)
```

Running Result:

```python
(-7.252923459405189, 1.7608403147511027e-10, 12, 199, 
{'1%': -3.4636447617687436, '5%': -2.8761761179270766, '10%': -2.57457158581854}, -405.0923252716726)
```

#### 如何确定序列的稳定性？

+ + 原假设和ADF测试结果的统计值在1％，％5％和％10％的不同程度上的比较：
  
    + ADF测试结果**同时小于1％，5％和10％** –这表明对假设的很好拒绝。
      在此数据中，ADF结果为-7.2，小于三个级别的统计值。
  
      如果P值非常“接近0”：
         在此数据中，p值为1.76E-10，接近0。

```python
# 6)白噪声检验
p_value = acorr_ljungbox(timeseries, lags=1)
print (p_value)
```

Running Result:

```python
(array([3.7801974]), array([0.05186254]))
```

> 如果统计的P值**小于显着性水平0.05 **，则可以以95％的置信度拒绝原假设，并将该序列视为**非白噪声序列**
>
> （否则，可以接受零假设，并且该序列被视为纯随机序列）。

```python
# 7)定阶
def determinate_order(timeseries):
    #利用ACF和PACF判断模型阶数
    plot_acf(timeseries,lags=40) #延迟数
    plot_pacf(timeseries,lags=40)
    plt.show()

diff1 = timeseries.diff(1).dropna()
timeseries = diff1
determinate_order(timeseries)
```

<img src="AppliedTimeSeries_yinaihua_files\4-10-1.png" style="zoom:67%;" />

<img src="AppliedTimeSeries_yinaihua_files\4-10-2.png" alt="4-10-2" style="zoom:67%;" />

```python
(p, q) =(sm.tsa.arma_order_select_ic(diff1,max_ar=3,max_ma=3,ic='aic')['aic_min_order'])
print((p,q))
```

Running Result:

```python
(1,1)
```

```python
# 8) 构建模型 ARIMA(1,1,1)
def ARMA_model(train, order):
    arma_model = ARMA(train, order)  # ARMA模型
    result = arma_model.fit()  # 激活模型
    print(result.summary())  # 给出一份模型报告
    ############ in-sample ############
    pred = result.predict()

    pred.plot()
    train.plot()
    print('标准差为{}'.format(mean_squared_error(train, pred)))

    # 残差
    resid = result.resid
    # 利用QQ图检验残差是否满足正态分布
    plt.figure(figsize=(12, 8))
    qqplot(resid, line='q', fit=True)
    plt.show()
    # 利用D-W检验,检验残差的自相关性
    print('D-W检验值为{}'.format(durbin_watson(resid.values)))
    return result

result = ARMA_model(diff1, (1, 1))
```

Running Result:

```python
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =            3     M =           12

At X0         0 variables are exactly at the bounds

At iterate    0    f= -1.05003D+00    |proj g|=  8.38050D+00

At iterate    5    f= -1.05078D+00    |proj g|=  7.26175D-01

At iterate   10    f= -1.05196D+00    |proj g|=  1.03624D+01

At iterate   15    f= -1.05698D+00    |proj g|=  9.37443D-02

At iterate   20    f= -1.05698D+00    |proj g|=  2.65710D-01

At iterate   25    f= -1.05731D+00    |proj g|=  3.62602D+00

At iterate   30    f= -1.06521D+00    |proj g|=  1.05277D+00

At iterate   35    f= -1.06666D+00    |proj g|=  1.30294D+00

At iterate   40    f= -1.06669D+00    |proj g|=  1.13254D-01

At iterate   45    f= -1.06669D+00    |proj g|=  3.62335D-02

At iterate   50    f= -1.06669D+00    |proj g|=  1.24821D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
    3     52     75      1     0     0   2.236D-03  -1.067D+00
  F =  -1.06668929322293     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             

 Cauchy                time 0.000E+00 seconds.
 Subspace minimization time 0.000E+00 seconds.
 Line search           time 0.000E+00 seconds.

 Total User time 0.000E+00 seconds.

                              ARMA Model Results                              
==============================================================================
Dep. Variable:                 values   No. Observations:                  212
Model:                     ARMA(1, 1)   Log Likelihood                 226.138
Method:                       css-mle   S.D. of innovations              0.082
Date:                Mon, 10 Aug 2020   AIC                           -444.276
Time:                        18:18:45   BIC                           -430.850
Sample:                    01-02-2018   HQIC                          -438.850
                         - 08-01-2018                                         
================================================================================
                   coef    std err          z      P>|z|      [0.025      0.975]
--------------------------------------------------------------------------------
const           -0.0001   8.04e-05     -1.784      0.076      -0.000    1.41e-05
ar.L1.values    -0.1406      0.068     -2.070      0.040      -0.274      -0.007
ma.L1.values    -1.0000      0.012    -80.842      0.000      -1.024      -0.976
                                    Roots                                    
=============================================================================
                  Real          Imaginary           Modulus         Frequency
-----------------------------------------------------------------------------
AR.1           -7.1117           +0.0000j            7.1117            0.5000
MA.1            1.0000           +0.0000j            1.0000            0.0000
-----------------------------------------------------------------------------
标准差为0.00703917912290063
D-W检验值为2.0243585702133493
```

<img src="AppliedTimeSeries_yinaihua_files\4-11-1.png" style="zoom:67%;" />

<img src="AppliedTimeSeries_yinaihua_files\4-11-2.png" alt="4-11-2" style="zoom:67%;" />

> 当D-W测试值**接近2 **时，表明模型良好。

# 第5章：单位根，差异和趋势平稳性以及分数差异

## 1.确定时间序列的积分顺序

+ $ x_t $〜$ I（0）$

    + $ x_t $的空缺是有限的，并且不取决于$ t $；
    创新$ a_t $仅对$ x_t $的值产生暂时影响；
    + $ x = 0 $的相交之间的预期时间长度是有限的，因此$ x_t $会在其均值零附近波动；
    自相关$ \ rho_k $在足够大的$ k $的范围内稳步减小，因此它们的和是有限的。
+ $x_t$ ~ $I(1)$   $x_0 = 0$
    + $ x_t $的无穷远变为$ t $的无穷远；
      创新$ a_t $对$ x_t $的价值具有永久性影响，因为$ x_t $是以前所有创新的总和；
    + $ x = 0 $的交叉期望时间是无限的；
    自相关，所有$ k $的$ \ rho_k \ rightarrow 1 $，因为$ t $变为无穷大。

## 2. 测试单位根

+ 在实践中，将$ k $设置为$ [T ^ {0.25}] $应该可以很好地工作。
+ 一个更明智的选择是让AR（1）过程围绕非零均值波动，这样我们就有了以下模型：

$$
x_t=\theta _0+\phi x_{t-1}+a_t,t=1,2,\dots,T
$$

+ ADF测试：如果序列是“平稳”，则没有“单位根”，否则将有单位根。
  + ADF检验的零假设是单位根的存在，因此，如果获得的统计**显着小于** 3个置信度的临界统计值（1％，5％，10％），则表示** 否定原假设。
  + 查看P值是否真的接近**（小数点后4位就足够了）。



```python
###################例题5.1: 单位根检验-以600115股票数据为例
import pandas as pd
import statsmodels.tsa.stattools as ts
# 1)获取数据
data = pd.read_csv('C:/Users/Dell/Desktop/Timeseries/project/Book_dataset/鲁商发展.csv', index_col='Date')

print(data.head())
print(data.dtypes)
```

Running Result:

```python
            High   Low  Open  Close      Volume  Adj Close
Date                                                      
2010-01-04  6.19  6.08  6.19   6.09   7638975.0   5.948295
2010-01-05  6.13  6.01  6.09   6.11   6456808.0   5.967830
2010-01-06  6.23  6.08  6.11   6.16  12527866.0   6.016666
2010-01-07  6.34  6.11  6.16   6.21  15031799.0   6.065503
2010-01-08  6.30  5.90  6.30   6.10  21136398.0   5.958062
High         float64
Low          float64
Open         float64
Close        float64
Volume       float64
Adj Close    float64
dtype: object
```

```python
# 2）单位根检验
data = data['Close']
result = ts.adfuller(data)
print(result)
```

Running Result:

```py
(-1.6030333788903894, 0.48206953506108985, 27, 2240, {'1%': -3.433272682848605, '5%': -2.86283115989218, '10%': -2.567457345543686}, -1043.16845340671)
```



```python
########################例题5.2   单位根存在性-以鲁商发展股票数据为例
import pandas as pd
import statsmodels.tsa.stattools as ts
# 1)获取数据
data = pd.read_csv('C:/Users/Dell/Desktop/Timeseries/project/Book_dataset/鲁商发展.csv', index_col='Date')

print(data.head())
print(data.dtypes)
```

Running Result:

```python
             High    Low   Open  Close      Volume  Adj Close
Date                                                         
2010-01-04  12.48  11.81  12.41  11.81   9247253.0  11.646878
2010-01-05  11.65  11.22  11.65  11.23  10335203.0  11.074889
2010-01-06  11.30  10.83  11.08  11.09   6450348.0  10.936823
2010-01-07  11.25  10.87  11.14  11.11   4448254.0  10.956546
2010-01-08  11.26  11.01  11.12  11.20   3067258.0  11.045303
High         float64
Low          float64
Open         float64
Close        float64
Volume       float64
Adj Close    float64
dtype: object
```

```python
# 2）原始数据单位根检验
data = data['Close']
result = ts.adfuller(data)
print('原始数据',result)

if result[0] < result[4]['1%'] and result[0] < result[4]['5%'] and result[0] < result[4]['10%']:
    print('p值',result[1])
    print('不存在单位根')
else:
    print('存在单位跟')
```

Running Result:

```python
原始数据 (-2.578675432083639, 0.09748520096919844, 27, 2239, {'1%': -3.4332739897016245, '5%': -2.8628317369405174, '10%': -2.5674576527815707}, -1148.4348403295426)
存在单位跟
```

```python
# 3）一阶差分单位根检验
result = ts.adfuller(data,1)
print('一阶差分',result)
if result[0] < result[4]['1%'] and result[0] < result[4]['5%'] and result[0] < result[4]['10%']:
    print('p值',result[1])
    print('不存在单位根')
else:
    print('存在单位跟')

```

Running Result:

```python
一阶差分 (-3.102364205733534, 0.026378720686585026, 1, 2265, {'1%': -3.4332403869849286, '5%': -2.862816899390905, '10%': -2.567449752837351}, -1064.5214274142008)
存在单位跟
```

```python
# 4）二阶差分单位根检验
result = ts.adfuller(data,2)
print('二阶差分',result)
if result[0] < result[4]['1%'] and result[0] < result[4]['5%'] and result[0] < result[4]['10%']:
    print('p值',result[1])
    print('不存在单位根')
else:
    print('存在单位跟')
```

Running Result:

```python
二阶差分 (-3.102364205733534, 0.026378720686585026, 1, 2265, {'1%': -3.4332403869849286, '5%': -2.862816899390905, '10%': -2.567449752837351}, -1063.1166346616865)
存在单位跟
```

## 2. 趋势与差异平稳

+ $ x_t $据说是差值固定（DS）

$$
\bigtriangledown x_t = \epsilon_t
$$

+ 一个可能的替代方法是$ x_t $由隐藏在平稳噪声中的线性趋势生成，现在称为趋势平稳性（TS）：

$$
x_t = \beta_0+\beta _1 t+\epsilon_t
$$

+ ADF 回归:

$$
x_t = \beta_0+\beta_1t+\phi x_{t-1}+\sum_{i=1}^k\delta_x\bigtriangledown x_{t-i}+a_t
$$

+ 统计数据：

$$
\tau_{\tau}=\frac{\hat\phi_T-1}{se(\hat \phi_T)}
$$



```python
#####################例题5.3趋势-以1992-2005年的人口出生率数据为例
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

def Line_Trend_Model( s, ):
    res = {}
    n = len(s)
    m = 2  # 用于计算估计标准误差，线性趋势方程对应的值为 2
    res['t'] = [(i+1) for i in range(n)]  # 对t进行序号化处理
    avg_t = np.mean(res['t'])
    avg_y = np.mean(s)
    ly = sum( map(lambda x,y : x * y, res['t'], s)) - n * avg_t * avg_y
    lt = sum( map(lambda x : x**2, res['t'])) - n * avg_t ** 2
    res['b'] = ly/lt  # 斜率
    res['a'] = avg_y - res['b'] * avg_t  # 截距
    pre_y = res['a'] + res['b'] * np.array(res['t'])  # 趋势线
    res['sigma'] = np.sqrt(sum(map(lambda x,y : (x - y)**2, s, pre_y ))/(n-m))  # 估计的标准误差
    return res

# 引入数据
data = [ 18.24, 18.09, 17.70, 17.12, 16.98, 16.57, 15.64, 14.64, 14.03, 13.38, 12.86, 12.41, 12.29, 12.40,]
dates = pd.date_range('1992-1-1', periods = len(data), freq = 'A')  #'A'参数为每年的最后一天
y = pd.Series( data, index = dates )
# 计算值
param = Line_Trend_Model( y )
pre_y = param['a']+ param['b']* np.array(param['t']) # 趋势值
residual = y - pre_y #残差
db = pd.DataFrame( [param['t'], data, list(pre_y), list(residual),  list(residual**2)],
                    index = [ 't','Y(‰)','Trend','Residual','R sqare'],
                    columns = dates ).T
# 输出结果
print('='*60)
print(db)
print('='*60)
# 计算预测值
t = 16
yt = param['a']+ param['b']* t
print('2007年人口出生率预测值为 {:.2f}‰'.format(yt))
```

Running Result:

```python
============================================================
               t   Y(‰)      Trend  Residual   R sqare
1992-12-31   1.0  18.24  18.650286 -0.410286  0.168334
1993-12-31   2.0  18.09  18.114527 -0.024527  0.000602
1994-12-31   3.0  17.70  17.578769  0.121231  0.014697
1995-12-31   4.0  17.12  17.043011  0.076989  0.005927
1996-12-31   5.0  16.98  16.507253  0.472747  0.223490
1997-12-31   6.0  16.57  15.971495  0.598505  0.358209
1998-12-31   7.0  15.64  15.435736  0.204264  0.041724
1999-12-31   8.0  14.64  14.899978 -0.259978  0.067589
2000-12-31   9.0  14.03  14.364220 -0.334220  0.111703
2001-12-31  10.0  13.38  13.828462 -0.448462  0.201118
2002-12-31  11.0  12.86  13.292703 -0.432703  0.187232
2003-12-31  12.0  12.41  12.756945 -0.346945  0.120371
2004-12-31  13.0  12.29  12.221187  0.068813  0.004735
2005-12-31  14.0  12.40  11.685429  0.714571  0.510612
============================================================
2007年人口出生率预测值为 10.61‰
```

```python
# 画图
fig = plt.figure( figsize = ( 6, 3 ) )
db['Y(‰)'].plot( style = 'bd-',  label = 'Y' )
db['Trend'].plot( style = 'ro-', label = 'Trend')
plt.legend()
plt.grid(axis = 'y')
plt.title('Trend of Birth Rate(1992~2005）')

plt.show()
```

<img src="AppliedTimeSeries_yinaihua_files\6-1.png" style="zoom:80%;" />



```python
###############例题5.4 非平稳时间序列与差分-以“Airpassengers"为例
import pandas as pd
import numpy as np
import matplotlib.pylab as plt
from matplotlib.pylab import rcParams
from statsmodels.tsa.stattools import adfuller
```

```python
# 1）加载数据
data = pd.read_csv('C:/Users/Dell/Desktop/Timeseries/project/Book_dataset/AirPassengers.csv')
print(data.head())
print('\n Data types:')
print(data.dtypes)
```

Running Result:

```python
     Month  #Passengers  Unnamed: 2
0  1949-01          112         NaN
1  1949-02          118         NaN
2  1949-03          132         NaN
3  1949-04          129         NaN
4  1949-05          121         NaN

 Data types:
Month           object
#Passengers      int64
Unnamed: 2     float64
dtype: object
```

```python
# 2）处理数据
dateparse = lambda dates: pd.datetime.strptime(dates, '%Y-%m')
#---其中parse_dates 表明选择数据中的哪个column作为date-time信息，
#---index_col 告诉pandas以哪个column作为 index
#--- date_parser 使用一个function(本文用lambda表达式代替)，使一个string转换为一个datetime变量
data = pd.read_csv('C:/Users/Dell/Desktop/Timeseries/project/Book_dataset/AirPassengers.csv', parse_dates=['Month'], index_col='Month',date_parser=dateparse)
print(data.head())
print(data.index)
```

Running Result:

```python
 #Passengers  Unnamed: 2
Month                              
1949-01-01          112         NaN
1949-02-01          118         NaN
1949-03-01          132         NaN
1949-04-01          129         NaN
1949-05-01          121         NaN
DatetimeIndex(['1949-01-01', '1949-02-01', '1949-03-01', '1949-04-01',
               '1949-05-01', '1949-06-01', '1949-07-01', '1949-08-01',
               '1949-09-01', '1949-10-01',
               ...
               '1960-03-01', '1960-04-01', '1960-05-01', '1960-06-01',
               '1960-07-01', '1960-08-01', '1960-09-01', '1960-10-01',
               '1960-11-01', '1960-12-01'],
              dtype='datetime64[ns]', name='Month', length=144, freq=None)
```

```python
# 3）判断时序稳定性
def test_stationarity(timeseries):
    # 这里以一年为一个窗口，每一个时间t的值由它前面12个月（包括自己）的均值代替，标准差同理。
    rolmean = timeseries.rolling(window=12).mean()
    rolstd = timeseries.rolling(window=12).std()

    # plot rolling statistics:
    fig = plt.figure()
    fig.add_subplot()
    orig = plt.plot(timeseries, color='blue', label='Original')
    mean = plt.plot(rolmean, color='red', label='rolling mean')
    std = plt.plot(rolstd, color='black', label='Rolling standard deviation')

    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)

    # Dickey-Fuller test:

    print('Results of Dickey-Fuller Test:')
    dftest = adfuller(timeseries, autolag='AIC')
    # dftest的输出前一项依次为检测值，p值，滞后数，使用的观测数，各个置信度下的临界值
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])
    for key, value in dftest[4].items():
        dfoutput['Critical value (%s)' % key] = value

    print(dfoutput)

from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()

ts = data['#Passengers']
test_stationarity(ts)
```

Running Result:

```python
Results of Dickey-Fuller Test:
Test Statistic                   0.815369
p-value                          0.991880
#Lags Used                      13.000000
Number of Observations Used    130.000000
Critical value (1%)             -3.481682
Critical value (5%)             -2.884042
Critical value (10%)            -2.578770
dtype: float64
```

<img src="AppliedTimeSeries_yinaihua_files\6-2.png" style="zoom:80%;" />

#### 数据的不平稳性主要有两个原因:

+ 趋势-数据随时间变化。 假设它上升或下降.

+ 季节性-数据在指定时间段内发生变化。 例如，假期或导致数据异常的活动。

```python
# 4）让时序变稳定
ts_log = np.log(ts)
# 移动平均法
moving_avg = ts_log.rolling(window=12).mean()
plt.plot(ts_log ,color = 'blue')
plt.plot(moving_avg, color='red')
```

Running Result:

```python
Results of Dickey-Fuller Test:
Test Statistic                   0.815369
p-value                          0.991880
#Lags Used                      13.000000
Number of Observations Used    130.000000
Critical value (1%)             -3.481682
Critical value (5%)             -2.884042
Critical value (10%)            -2.578770
dtype: float64
```

<img src="AppliedTimeSeries_yinaihua_files\6-3.png" style="zoom:80%;" />

```python
ts_log_moving_avg_diff = ts_log-moving_avg
ts_log_moving_avg_diff.dropna(inplace = True)
test_stationarity(ts_log_moving_avg_diff)
```

Running Result:

```python
Results of Dickey-Fuller Test:
Test Statistic                  -3.162908
p-value                          0.022235
#Lags Used                      13.000000
Number of Observations Used    119.000000
Critical value (1%)             -3.486535
Critical value (5%)             -2.886151
Critical value (10%)            -2.579896
dtype: float64
```

<img src="AppliedTimeSeries_yinaihua_files\6-4.png" style="zoom:80%;" />

```python
# halflife的值决定了衰减因子alpha：  alpha = 1 - exp(log(0.5) / halflife)
expweighted_avg = pd.DataFrame.ewm(ts_log,halflife=12).mean()
ts_log_ewma_diff = ts_log - expweighted_avg
test_stationarity(ts_log_ewma_diff)
```

Running Result:

```python
Results of Dickey-Fuller Test:
Test Statistic                  -3.601262
p-value                          0.005737
#Lags Used                      13.000000
Number of Observations Used    130.000000
Critical value (1%)             -3.481682
Critical value (5%)             -2.884042
Critical value (10%)            -2.578770
dtype: float64
```

<img src="AppliedTimeSeries_yinaihua_files\6-5.png" style="zoom:80%;" />

```python
# 差分
ts_log_diff = ts_log - ts_log.shift()
ts_log_diff.dropna(inplace=True)
test_stationarity(ts_log_diff)
```

Running Result:

```python
Results of Dickey-Fuller Test:
Test Statistic                  -2.717131
p-value                          0.071121
#Lags Used                      14.000000
Number of Observations Used    128.000000
Critical value (1%)             -3.482501
Critical value (5%)             -2.884398
Critical value (10%)            -2.578960
dtype: float64
```

<img src="AppliedTimeSeries_yinaihua_files/6-6.png" style="zoom:80%;" />



```python
##############例题5.5 差分平稳-以湖北省GDP为例
# 0) 导入包
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.graphics.tsaplots import acf,pacf,plot_acf,plot_pacf
from statsmodels.tsa.arima_model import ARMA

# 1）数据画图
time_series = pd.Series([151.0, 188.46, 199.38, 219.75, 241.55, 262.58, 328.22, 396.26, 442.04, 517.77, 626.52, 717.08, 824.38, 913.38, 1088.39, 1325.83, 1700.92, 2109.38, 2499.77, 2856.47, 3114.02, 3229.29, 3545.39, 3880.53, 4212.82, 4757.45, 5633.24, 6590.19, 7617.47, 9333.4, 11328.92, 12961.1, 15967.61])
time_series.index = pd.Index(sm.tsa.datetools.dates_from_range('1978','2010'))
time_series.plot(figsize=(6,4))
plt.title("HuBei GDP(1978~2010)")
plt.show()
```

<img src="AppliedTimeSeries_yinaihua_files\4-1.png" style="zoom:80%;" />

时间序列呈指数形式，波动较大，不稳定。 您取其对数**，并将其变成线性趋势。

```python
# 2）转化
time_series = np.log(time_series)
time_series.plot(figsize=(6,4))
plt.title("HuBei GDP_log")
plt.show()
```

<img src="AppliedTimeSeries_yinaihua_files\4-2.png" style="zoom: 80%;" />

为了确定其稳定性，对数后对数据进行了** ADF单位根检验**。

```python
# 3)单位根检验
t=sm.tsa.stattools.adfuller(time_series, )
output=pd.DataFrame(index=['Test Statistic Value', "p-value", "Lags Used", "Number of Observations Used","Critical Value(1%)","Critical Value(5%)","Critical Value(10%)"],columns=['value'])
output['value']['Test Statistic Value'] = t[0]
output['value']['p-value'] = t[1]
output['value']['Lags Used'] = t[2]
output['value']['Number of Observations Used'] = t[3]
output['value']['Critical Value(1%)'] = t[4]['1%']
output['value']['Critical Value(5%)'] = t[4]['5%']
output['value']['Critical Value(10%)'] = t[4]['10%']
print(output)
```

Running Reslut:

```python
                                value
Test Statistic Value         0.807369
p-value                      0.991754
Lags Used                           1
Number of Observations Used        31
Critical Value(1%)           -3.66143
Critical Value(5%)           -2.96053
Critical Value(10%)          -2.61932
```

根据上述检验，** t统计量大于任何置信度的临界值**，因此可以认为该序列是非平稳**。

因此，对序列进行“区别对待”，并进行“ ADF测试”。

```python
# 4）差分并进行ADF检验
time_series = time_series.diff(1)
time_series = time_series.dropna(how=any)
time_series.plot(figsize=(8,6))
plt.title("First-difference")
plt.show()
t=sm.tsa.stattools.adfuller(time_series)
output=pd.DataFrame(index=['Test Statistic Value', "p-value", "Lags Used", "Number of Observations Used","Critical Value(1%)","Critical Value(5%)","Critical Value(10%)"],columns=['value'])
output['value']['Test Statistic Value'] = t[0]
output['value']['p-value'] = t[1]
output['value']['Lags Used'] = t[2]
output['value']['Number of Observations Used'] = t[3]
output['value']['Critical Value(1%)'] = t[4]['1%']
output['value']['Critical Value(5%)'] = t[4]['5%']
output['value']['Critical Value(10%)'] = t[4]['10%']
print(output)
```

Running Result:

<img src="AppliedTimeSeries_yinaihua_files\4-3.png" style="zoom:67%;" />

```python
                                  value
Test Statistic Value           -3.52276
p-value                      0.00742139
Lags Used                             0
Number of Observations Used          31
Critical Value(1%)             -3.66143
Critical Value(5%)             -2.96053
Critical Value(10%)            -2.61932
```

“差异”之后的序列“基本稳定”并通过ADF测试。

## 6. 多个单位根测试

单位根测试的这种发展是基于以下假设：$ x_t $最多包含一个单位根，因此$ I（0）$或$ I（1）$。

如果不拒绝单位根的零假设，则可能有必要测试该序列是否包含第二个单位根-换句话说，$ x_t $是否为$ I（2）$，因此需要相差两次 引起平稳。



```python
######################例题5.6 差分-以鲁商发展股票数据为例
import pandas as pd
import numpy as np
import statsmodels.tsa.stattools as ts
import matplotlib.pyplot as plt

data = pd.read_csv('C:/Users/Dell/Desktop/Timeseries/project/Book_dataset/鲁商发展.csv', index_col='Date')
print(data.head())
```

Running Result:

```python
High    Low   Open  Close      Volume  Adj Close
Date                                                         
2010-01-04  12.48  11.81  12.41  11.81   9247253.0  11.646878
2010-01-05  11.65  11.22  11.65  11.23  10335203.0  11.074889
2010-01-06  11.30  10.83  11.08  11.09   6450348.0  10.936823
2010-01-07  11.25  10.87  11.14  11.11   4448254.0  10.956546
2010-01-08  11.26  11.01  11.12  11.20   3067258.0  11.045303
```

```python
y = data['Adj Close']
#一阶差分
for i in range(0,len(y)-1):
    y[i] = y[i+1] -  y[i]
print(y)

x = [i for i in range(0,len(y))]
plt.plot(x, y, label="first-diffenence")
plt.title("first-diffenence")
plt.show()

y = np.array(y)
result = ts.adfuller(y)
print(result)
```

Running Result:

```python
Date
2010-01-04   -0.571989
2010-01-05   -0.138066
2010-01-06    0.019723
2010-01-07    0.088758
2010-01-08   -0.552264
                ...   
2019-05-06   -0.009862
2019-05-07    0.029586
2019-05-08   -0.108481
2019-05-09    0.049309
2019-05-10    3.708066
Name: Adj Close, Length: 2267, dtype: float64
(-10.392291779163214, 2.006359229903716e-18, 26, 2240, {'1%': -3.433272682848605, '5%': -2.86283115989218, '10%': -2.567457345543686}, -825.2623219455372)
```

<img src="AppliedTimeSeries_yinaihua_files\6-12.png" style="zoom:80%;" />

```python
#二阶差分
for i in range(0,len(y)-1):
    y[i] = y[i+1] -  y[i]
print(y)

x = [i for i in range(0,len(y))]
plt.plot(x, y, label="second-diffenence")
plt.title("second-diffenence")
plt.show()

y = np.array(y)
result = ts.adfuller(y)
print(result)
```

Running Result:

```python
[0.43392277 0.15778923 0.06903458 ... 0.15779018 3.65875673 3.70806623]
(-11.244958822006996, 1.768980911819217e-20, 20, 2246, {'1%': -3.4332648661965646, '5%': -2.862827708400023, '10%': -2.5674555078663643}, 83.1465217314144)
```

<img src="AppliedTimeSeries_yinaihua_files\6-13.png" style="zoom:80%;" />

## 3. 其他的单位根测试方法

+ PP非参数单位根检验
+ 零假设是单位根的零假设，具有平稳的假设（趋势或水平平稳）



```python
##########例题5.7 单位根检验-以"Airpassengers"为例
#ACF and PACF plots:
from statsmodels.tsa.stattools import acf, pacf
lag_acf = acf(ts_log_diff, nlags=20)
lag_pacf = pacf(ts_log_diff, nlags=20, method='ols')
#Plot ACF:
plt.plot(lag_acf)
plt.axhline(y=0,linestyle='--',color='gray')
plt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')
plt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')
plt.title('Autocorrelation Function')
plt.show()

#Plot PACF:
plt.plot(lag_pacf)
plt.axhline(y=0,linestyle='--',color='gray')
plt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')
plt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')
plt.title('Partial Autocorrelation Function')
plt.tight_layout()
plt.show()
```

<img src="AppliedTimeSeries_yinaihua_files\6-10.png" style="zoom:80%;" />

<img src="AppliedTimeSeries_yinaihua_files\6-10.png" style="zoom:80%;" />

## 4. 检测趋势鲁棒性

+ 正确确定趋势对于单位根和固定测试至关重要。

+ 如果不知道$ \ varepsilon_t $ id是$ I（0）$还是$ I（1）
  + 加权平均$ z_0 $和$ z_1 $
$$
z_{\lambda}=(1-\lambda(U,S))z_0 + \lambda(U,S)z_1
$$
其中U是标准的单位根检验统计量，S是标准的趋势平稳性检验统计量，
$$
\lambda = exp(-\kappa (\frac{U}{S})^2)
$$
在$ H_0 $下将是渐近标准正态。

```python
###############例题5.8  判断-2015/1/1~2015/2/6某餐厅的销售数据
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.pylab import style
from statsmodels.tsa.stattools import adfuller as ADF
from statsmodels.stats.diagnostic import acorr_ljungbox  # 白噪声检验
from statsmodels.tsa.arima_model import ARIMA
import statsmodels.tsa.api as smt
import seaborn as sns
style.use('ggplot')
plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号
```

```python
# 参数初始化
discfile = 'C:/Users/Dell/Desktop/Timeseries/project/Book_dataset/restaurant.xlsx'
forecastnum = 5

data = pd.read_excel(discfile, index_col=u'日期')
print(data)
```

Running Result:

```python
   日期      销量        
2015-01-01  3023
2015-01-02  3039
2015-01-03  3056
2015-01-04  3138
2015-01-05  3188
2015-01-06  3224
2015-01-07  3226
2015-01-08  3029
2015-01-09  2859
2015-01-10  2870
2015-01-11  2910
2015-01-12  3012
2015-01-13  3142
2015-01-14  3252
2015-01-15  3342
2015-01-16  3365
2015-01-17  3339
2015-01-18  3345
2015-01-19  3421
2015-01-20  3443
2015-01-21  3428
2015-01-22  3554
2015-01-23  3615
2015-01-24  3646
2015-01-25  3614
2015-01-26  3574
2015-01-27  3635
2015-01-28  3738
2015-01-29  3707
2015-01-30  3827
2015-01-31  4039
2015-02-01  4210
2015-02-02  4493
2015-02-03  4560
2015-02-04  4637
2015-02-05  4755
2015-02-06  4817
```

```python
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号
data.plot()
plt.show()

from statsmodels.graphics.tsaplots import plot_acf
plot_acf(data).show()

from statsmodels.tsa.stattools import adfuller as ADF
print(u'原始序列的ADF检验结果为：', ADF(data[u'销量']))
```

Running Result:

```python
原始序列的ADF检验结果为： (1.813771015094526, 0.9983759421514264, 10, 26, {'1%': -3.7112123008648155, '5%': -2.981246804733728, '10%': -2.6300945562130176}, 299.4698986602418)
```

<img src="AppliedTimeSeries_yinaihua_files\6-14.png" style="zoom:80%;" />

<img src="AppliedTimeSeries_yinaihua_files\6-15.png" alt="6-15" style="zoom:80%;" />

```python
# 差分
D_data = data.diff().dropna()
D_data.columns = [u'销量差分']
D_data.plot() 
plt.show()
plot_acf(D_data).show()  
from statsmodels.graphics.tsaplots import plot_pacf
plot_pacf(D_data).show()  
print(u'差分序列的ADF检验结果为：', ADF(D_data[u'销量差分']))  

from statsmodels.stats.diagnostic import acorr_ljungbox
print(u'差分序列的白噪声检验结果为：', acorr_ljungbox(D_data, lags=1))  
```

Running Result:

```python
差分序列的ADF检验结果为： (-3.1560562366723537, 0.022673435440048798, 0, 35, {'1%': -3.6327426647230316, '5%': -2.9485102040816327, '10%': -2.6130173469387756}, 287.5909090780334)
差分序列的白噪声检验结果为： (array([11.30402222]), array([0.00077339]))
```

<img src="AppliedTimeSeries_yinaihua_files\6-16.png" style="zoom:80%;" />

<img src="AppliedTimeSeries_yinaihua_files\6-17.png" alt="6-17" style="zoom:80%;" />

<img src="AppliedTimeSeries_yinaihua_files\6-18.png" alt="6-18" style="zoom:80%;" />

## 5. 分数差异和长记忆

+ $ x_t $〜$ I（1）$：ACF线性下降
+ $ x_t $〜$ I（0）$：ACF呈指数下降

## 6.测试分数差异

+ 测试分数差异的一种明显方法是针对d = 1或d = 0的零值构造测试。

## 7.估计分数差异参数

+ FD-F程序的一个缺点是，如果先验地不知道$ d_1 $，就像在标准DF情况下那样，则必须提供一致的估计。

```python
###############例题5.9   分解-以"Airpassengers"为例
from statsmodels.tsa.seasonal import seasonal_decompose

def decompose(timeseries):
    # 返回包含三个部分 trend（趋势部分） ， seasonal（季节性部分） 和residual (残留部分)
    decomposition = seasonal_decompose(timeseries)

    trend = decomposition.trend
    seasonal = decomposition.seasonal
    residual = decomposition.resid

    plt.subplot(411)
    plt.plot(ts_log, label='Original')
    plt.legend(loc='best')
    plt.subplot(412)
    plt.plot(trend, label='Trend')
    plt.legend(loc='best')
    plt.subplot(413)
    plt.plot(seasonal, label='Seasonality')
    plt.legend(loc='best')
    plt.subplot(414)
    plt.plot(residual, label='Residuals')
    plt.legend(loc='best')
    plt.tight_layout()

    return trend, seasonal, residual

#消除了trend 和seasonal之后，只对residual部分作为想要的时序数据进行处理
trend , seasonal, residual = decompose(ts_log)
residual.dropna(inplace=True)
test_stationarity(residual)
```

Running Result:

```python
Results of Dickey-Fuller Test:
Test Statistic                -6.332387e+00
p-value                        2.885059e-08
#Lags Used                     9.000000e+00
Number of Observations Used    1.220000e+02
Critical value (1%)           -3.485122e+00
Critical value (5%)           -2.885538e+00
Critical value (10%)          -2.579569e+00
dtype: float64
```

<img src="AppliedTimeSeries_yinaihua_files\6-19.png" style="zoom:80%;" />

<img src="AppliedTimeSeries_yinaihua_files\6-20.png" style="zoom:80%;" />

# 第6章：突破和非线性趋势

## 1. 突破趋势模型

+ 简单突破趋势模型（水平移动）
  $$
  x_t = \mu_0 + (\mu_1 - \mu_0)DU_t^c + \beta_0t + \varepsilon_t = \mu_0 + \mu DU_t^c + \beta_0t + \varepsilon_t
  $$
  其中$ DU_t ^ c = 0 $如果$ t \ leq T_b ^ c $和$ DU_t ^ c = 1 $如果$ t> T_b ^ c $

+ 另一种可能性是“增长变化”模型（分段趋势）
  $$
  x_t = \mu_0 + \beta+0t+ (\beta_1 - \beta_0)DTt^c + \varepsilon_t = \mu_0 + \beta_0 t+ \beta DT_t^c + \varepsilon_t
  $$

+ 组合模型：
  $$
  x_t = \mu_0 + (\mu_1 - \mu_0)DU_t^c + \beta_0t + (\beta_1 - \beta_0)DTt^c + \varepsilon_t = \mu_0 + \mu DU_t^c + \beta_0t + \beta DT_t^c + \varepsilon_t
  $$

## 2. 突破趋势和单位根测试

+ 单位根变得不可能被拒绝，甚至渐近。
+ 对于给定的测试大小，所有统计量的绝对值自然都比标准DF临界值大。



```python
################例题6.1   趋势移除-以“洗发水数据集“为例
from pandas import read_csv
from pandas import datetime
from matplotlib import pyplot
 # 1)加载数据
def parser(x):
   return datetime.strptime('190'+x, '%Y-%m')

series = read_csv('C:/Users/Dell/Desktop/Timeseries/project/Book_dataset/shampoo.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)
print(series)
series.plot()
pyplot.show()
```

Running Result:

```python
Month
1901-01-01    266.0
1901-02-01    145.9
1901-03-01    183.1
1901-04-01    119.3
1901-05-01    180.3
1901-06-01    168.5
...
1902-12-01    342.3
1903-01-01    339.7
1903-02-01    440.4
1903-03-01    315.9
1903-04-01    439.3
1903-05-01    401.3
1903-06-01    437.4
1903-07-01    575.5
1903-08-01    407.6
1903-09-01    682.0
1903-10-01    475.3
1903-11-01    581.3
1903-12-01    646.9
Name: Sales, dtype: float64
```

<img src="AppliedTimeSeries_yinaihua_files\5-1.png" style="zoom:67%;" />

```python
# 2）手动差分
from pandas import read_csv
from pandas import datetime
from pandas import Series
from matplotlib import pyplot

# create a differenced series
def difference(dataset, interval=1):
   diff = list()
   for i in range(interval, len(dataset)):
      value = dataset[i] - dataset[i - interval]
      diff.append(value)
   return Series(diff)

series = read_csv('C:/Users/Dell/Desktop/Timeseries/project/Book_dataset/shampoo.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)
X = series.values
diff = difference(X)
pyplot.plot(diff)
pyplot.show()
```

<img src="AppliedTimeSeries_yinaihua_files\5-2.png" style="zoom:67%;" />

```python
#3）自动差分
from pandas import read_csv
from pandas import datetime
from matplotlib import pyplot

def parser(x):
   return datetime.strptime('190'+x, '%Y-%m')

series = read_csv('C:/Users/Dell/Desktop/Timeseries/project/Book_dataset/shampoo.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)
diff = series.diff()
pyplot.plot(diff)
pyplot.show()
```

<img src="AppliedTimeSeries_yinaihua_files\5-3.png" style="zoom:67%;" />

## 3. 中断日期未知时的单位根测试

+ 已经考虑了两种用于选择$ \ hat T_b $的数据相关方法，这两种方法均涉及估计适当的去趋势的AO重新定义。
+ 第二种方法涉及选择$ \ hat T_b $作为中断日期，对于该日期，测试中断参数的重要性的某些统计信息将最大化。
+ 即使在执行一些初步修整之后，这等效于最小化所有可能回归中的平方余数。
+ 这两种内生地选择断点的方法的一个重要限制是，必须假设在单位根的零假设下没有发生断点。

```python
##################例题6.2   单位根-以鲁商发展股票数据为例
import pandas as pd
import statsmodels.tsa.stattools as ts
# 1)获取数据
data = pd.read_csv('C:/Users/Dell/Desktop/Timeseries/project/Book_dataset/鲁商发展.csv', index_col='Date')

print(data.head())
print(data.dtypes)
```

Running Result:

```python
             High    Low   Open  Close      Volume  Adj Close
Date                                                         
2010-01-04  12.48  11.81  12.41  11.81   9247253.0  11.646878
2010-01-05  11.65  11.22  11.65  11.23  10335203.0  11.074889
2010-01-06  11.30  10.83  11.08  11.09   6450348.0  10.936823
2010-01-07  11.25  10.87  11.14  11.11   4448254.0  10.956546
2010-01-08  11.26  11.01  11.12  11.20   3067258.0  11.045303
High         float64
Low          float64
Open         float64
Close        float64
Volume       float64
Adj Close    float64
dtype: object
```

```python
data = data['Close']
result = ts.adfuller(data)
print(result)
```

Running Result:

```python
(-2.578675432083639, 0.09748520096919844, 27, 2239, {'1%': -3.4332739897016245, '5%': -2.8628317369405174, '10%': -2.5674576527815707}, -1148.4348403295426)
```

## 4. 稳健的测试以寻找突破趋势

+ 如果已知中断日期为$ T_b ^ c $，且中断分数为$ \ tau _c $，则着眼于分段趋势模型（B），通过关注$ \的自相关校正t检验来扩展HLT方法。 beta = 0 $。

$$
t_\lambda=\lambda(S_0(\tau^c))\times|t_0(\tau^c)|+(1-\lambda(S_0(\tau^c),S_1(\tau^c)))\times|t_1(\tau^c)|
$$

权重函数现在定义为
$$
\lambda(S_0(\tau^c),S_1(\tau^c))=exp(-(-500S_0(\tau^c)S_1(\tau^c))^2)
$$

+ 如果$ \ tau ^ c $是未知的，但假定位于$ 0 <\ tau_ {min}，\ tau_ {max} <1 $之间，则

$$
t_\lambda=\lambda(S_0(\tau^c))\times|t_0(\tau^c)|+m_{\epsilon}(1-\lambda(S_0(\hat\tau),S_1(\hat\tau)))\times|t_1(\hat\tau)|
$$

## 5. 置信区间（ for the break date and multiple breaks）

+ for the break date：限制分布不取决于误差的自相关结构，仅需要估计误差方差$ \ sigma ^ 2 $。
+ 只有一个中断时所有可用的过程都可以扩展到多个中断的情况，但是，实际上，当在未知时间存在多个中断时，当前只有需要专门编程的KP顺序过程可用 。

## 6. 非线性趋势

+ LSTR 函数

$$
S_t(\gamma,m) = (1-exp(-\gamma(t-mT)))^{-1}
$$

$$
S_t(\gamma,m) = 1-exp(-\gamma(t-mT)^2)
$$

+ 然后可以指定三个替代的平滑过渡趋势模型：

$$
x_t=\mu_0+\mu S_t(\gamma,m)+\epsilon_t
$$

$$
x_t=\mu_0+\beta_0t+\mu S_t(\gamma,m)+\epsilon_t
$$

$$
x_t=\mu_0+\beta_0 t+\mu S_t(\gamma.m)+\beta_tS_t(\gamma,m)+\epsilon_t
$$

+ 通过傅立叶级数展开来逼近非线性趋势也变得很流行。

```python
##############例题6.3   平滑法1
import numpy as np
import matplotlib.pyplot as plt
x = np.linspace(0, 2*np.pi, 100)
y = np.sin(x) + np.random.random(100) * 0.2


def smooth(y, box_pts):
    box = np.ones(box_pts) / box_pts
    y_smooth = np.convolve(y, box, mode='same')
    return y_smooth

plt.plot(x, y,'o', label='data')
plt.plot(x, smooth(y, 3), 'r-', lw=2, label='smooth with box size 3')
plt.plot(x, smooth(y, 19), 'g-', lw=2, label='smooth with box size 19')
plt.legend(bbox_to_anchor=(1, 1))
plt.show();
```

<img src="AppliedTimeSeries_yinaihua_files\5-4.png" style="zoom:67%;" />



```python
########例题6.4  平滑法2
from astropy.modeling.models import Lorentz1D
from astropy.convolution import convolve, Gaussian1DKernel, Box1DKernel
lorentz = Lorentz1D(1, 0, 1)
x = np.linspace(-5, 5, 100)

data_1D = lorentz(x) + 0.1 * (np.random.rand(100) - 0.5)
gauss_kernel = Gaussian1DKernel(2)
smoothed_data_gauss = convolve(data_1D, gauss_kernel)

box_kernel = Box1DKernel(5)
smoothed_data_box = convolve(data_1D, box_kernel)

plt.plot(data_1D, label='Original')
plt.plot(smoothed_data_gauss, label='Smoothed with Gaussian1DKernel')
plt.plot(smoothed_data_box, label='Box1DKernel')
plt.legend(bbox_to_anchor=(1, 1))
plt.show();
```

<img src="AppliedTimeSeries_yinaihua_files\5-5.png" style="zoom:80%;" />

```python
smoothed = np.convolve(data_1D, box_kernel.array)
plt.plot(data_1D, label='Original')
plt.plot(smoothed, label='Smoothed with numpy')
plt.legend(bbox_to_anchor=(1, 1))
plt.show();
```

```python
############例题6.5   平滑法3
import numpy as np
from scipy.fftpack import fft,ifft
import matplotlib.pyplot as plt
import seaborn

x=np.linspace(0,1,1400)  

y=7*np.sin(2*np.pi*180*x) + 2.8*np.sin(2*np.pi*390*x)+5.1*np.sin(2*np.pi*600*x)

yy=fft(y)      #快速傅里叶变换
yreal = yy.real    # 获取实数部分
yimag = yy.imag    # 获取虚数部分

yf=abs(fft(y))    # 取绝对值
yf1=abs(fft(y))/len(x)   #归一化处理
yf2 = yf1[range(int(len(x)/2))] #由于对称性，只取一半区间

xf = np.arange(len(y))  # 频率
xf1 = xf
xf2 = xf[range(int(len(x)/2))] #取一半区间


plt.subplot(221)
plt.plot(x[0:50],y[0:50]) 
plt.title('Original wave')

plt.subplot(222)
plt.plot(xf,yf,'r')
plt.title('FFT of Mixed wave(two sides frequency range)',fontsize=7,color='#7A378B') #注意这里的颜色可以查询颜色代码表

plt.subplot(223)
plt.plot(xf1,yf1,'g')
plt.title('FFT of Mixed wave(normalization)',fontsize=9,color='r')

plt.subplot(224)
plt.plot(xf2,yf2,'b')
plt.title('FFT of Mixed wave)',fontsize=10,color='#F08080')


plt.show()
```

<img src="AppliedTimeSeries_yinaihua_files\5-11.png" style="zoom:80%;" />

```python
# -*- coding: utf-8 -*-
import matplotlib.pyplot as plt
import numpy as np
import seaborn

Fs = 150.0;     # sampling rate采样率
Ts = 1.0/Fs;    # sampling interval 采样区间
t = np.arange(0,1,Ts)  # time vector,这里Ts也是步长

ff = 25;     # frequency of the signal
y = np.sin(2*np.pi*ff*t)

n = len(y)     # length of the signal
k = np.arange(n)
T = n/Fs
frq = k/T     # two sides frequency range
frq1 = frq[range(int(n/2))] # one side frequency range

YY = np.fft.fft(y)   # 未归一化
Y = np.fft.fft(y)/n   # fft computing and normalization 归一化
Y1 = Y[range(int(n/2))]

fig, ax = plt.subplots(4, 1)

ax[0].plot(t,y)
ax[0].set_xlabel('Time')
ax[0].set_ylabel('Amplitude')

ax[1].plot(frq,abs(YY),'r') # plotting the spectrum
ax[1].set_xlabel('Freq (Hz)')
ax[1].set_ylabel('|Y(freq)|')

ax[2].plot(frq,abs(Y),'G') # plotting the spectrum
ax[2].set_xlabel('Freq (Hz)')
ax[2].set_ylabel('|Y(freq)|')

ax[3].plot(frq1,abs(Y1),'B') # plotting the spectrum
ax[3].set_xlabel('Freq (Hz)')
ax[3].set_ylabel('|Y(freq)|')

plt.show()
```

<img src="AppliedTimeSeries_yinaihua_files\5-12.png" style="zoom:80%;" />



# 第7章   单变量模型预测简介

### 1：自回归模型AR

自回归模型描述当前值与历史值之间的关系，用变量自身的历史时间数据对自身进行预测。自回归模型必须满足平稳性的要求。

自回归模型首先需要确定一个阶数p，表示用几期的历史值来预测当前值。p阶自回归模型的公式定义为：

![1](AppliedTimeSeries_yinaihua_files/1.jpg)

上式中yt是当前值,u是常数项,p是阶数 ri是自相关系数,et是误差。

自回归模型有很多的限制：
1、自回归模型是用自身的数据进行预测
2、时间序列数据必须具有平稳性
3、自回归只适用于预测与自身前期相关的现象

### 2.移动平均模型MA

移动平均模型关注的是自回归模型中的误差项的累加 ，q阶自回归过程的公式定义如下：



![1](AppliedTimeSeries_yinaihua_files/2.jpg)

移动平均法能有效地消除预测中的随机波动。

### 3 自回归移动平均模型ARMA

自回归模型AR和移动平均模型MA模型相结合，我们就得到了自回归移动平均模型ARMA(p,q)，计算公式如下：

![1](AppliedTimeSeries_yinaihua_files/3.jpg)

### 4.ARIMA

将带漂移的随机游动模型中的白噪声替换成一个ARMA平稳列， 其主要的性质仍能保留。即
$$
Y_t = Y_{t-1} + \mu + X_t
$$
其中\{ X_t \}是零均值平稳可逆ARMA(p,q)平稳列。 这时有
$$
Y_t = Y_0 + \mu t + \sum_{j=1}^t X_j
$$
于是
$$
E(Y_t|Y_0) = Y_0 + \mu t, \quad \text{Var}(Y_t | Y_0) = \text{随}t\text{增大而趋于}\infty
$$
当\mu=0时， \{ Y_t \}的均值固定。 Y_t的条件方差为t的线性函数。 X_{t-j}对Y_t的影响不衰减， 是永久有影响的。 这些表现与带漂移的随机游动基本相同。

称\{ Y_t \}服从ARIMA(p,1,q)模型， 是非平稳的。 \{Y_t\}不能通过减去任何的非随机趋势变成平稳。 但是， 差分运算
$$
\Delta Y_t = (1-B)Y_t = Y_t - Y_{t-1} = \mu + X_t
$$
将ARIMA(p,1,q)序列\{Y_t\}转化成平稳可逆的ARMA(p,q)序列。



设零均值平稳可逆ARMA(p,q)序列\{X_t \}的模型为
$$
P(B) X_t = Q(B) \varepsilon_t
$$
其中\{ \varepsilon_t \}为零均值独立同分布白噪声列， 
$$
P(z)=1 - \phi_1 z - \dots - \phi_p z^p, Q(z)= 1 + \theta_1 z + \dots + \theta_q z^q
$$
。 因为
$$
(1-B)Y_t = \mu + X_t
$$
， 所以
$$
P(B) (1-B)Y_t = P(B)(\mu + X_t) = P(1)\mu + P(B) X_t = P(1)\mu + Q(B)\varepsilon_t记\tilde P(z) = P(z) (1-z), \phi_0 = P(1)\mu
$$
，则Y_t的模型可以写成
$$
\tilde P(B) Y_t = \phi_0 + Q(B) \varepsilon_t
$$
这个模型形式上与一个ARMA(p,q)模型相同， 但是其p+1个特征根中有一个等于1， 其余特征根才在单位圆外。



建模方法：对ARIMA模型建模， 只要计算Y_t的差分， 然后对差分建立ARMA模型即可。

注：有些序列需要二阶差分才能平稳，二阶差分即
$$
\begin{aligned} \Delta^2 Y_t =& (1-B)^2 Y_t = (1-B)(Y_t - Y_{t-1})  = (Y_t - Y_{t-1}) - (Y_{t-1} - Y_{t-2}) \\ =& Y_t - 2 Y_{t-1} + Y_{t-2} \end{aligned}
$$


如果
$$
\xi_t = a + bt
$$
， 则
$$
(1-B) \xi_t = b
$$
即差分还可以消除线性趋势。



如果
$$
\xi_t = a + bt + ct^2
$$
， 则
$$
(1-B)^2 = 2c
$$
即二阶差分可以消除二次多项式趋势。



在python中用`diff(x)`计算一阶差分， 结果从原来的第二个时间点给出； 用`diff(x, lag=2)`计算二阶差分， 结果从原来的第三个时间点给出。

如果Y_t本身已经是弱平稳列， 则不应对Y_t进行差分。 如果Y_t是非随机的线性趋势加平稳列， 虽然差分能将其变成平稳列， 但是也不应该使用差分来做而是应该用回归来做， 用差分来做会在ARMA模型的MA部分引入不必要的单位根。

### 5 模型识别和定阶

模型的识别问题和定阶问题，主要是确定p，d，q三个参数，差分的阶数d一般通过观察图示，1阶或2阶即可。这里我们主要介绍p和q的确定。我们首先介绍两个函数。

#### **自相关函数ACF(autocorrelation function)**

自相关函数ACF描述的是时间序列观测值与其过去的观测值之间的线性相关性。计算公式如下：

![1](AppliedTimeSeries_yinaihua_files/4.jpg)

其中k代表滞后期数，如果k=2，则代表yt和yt-2

#### **偏自相关函数PACF(partial autocorrelation function)**

偏自相关函数PACF描述的是在给定中间观测值的条件下，时间序列观测值预期过去的观测值之间的线性相关性。

举个简单的例子，假设k=3，那么我们描述的是yt和yt-3之间的相关性，但是这个相关性还受到yt-1和yt-2的影响。

PACF剔除了这个影响，而ACF包含这个影响。

#### **拖尾和截尾**

拖尾指序列以指数率单调递减或震荡衰减，而截尾指序列从某个时点变得非常小：

![1](AppliedTimeSeries_yinaihua_files/5.jpg)

出现以下情况，通常视为(偏)自相关系数d阶截尾：
1）在最初的d阶明显大于2倍标准差范围
2）之后几乎95%的(偏)自相关系数都落在2倍标准差范围以内
3）且由非零自相关系数衰减为在零附近小值波动的过程非常突然

![1](AppliedTimeSeries_yinaihua_files/6.jpg)

出现以下情况，通常视为(偏)自相关系数拖尾：
1）如果有超过5%的样本(偏)自相关系数都落入2倍标准差范围之外
2）或者是由显著非0的(偏)自相关系数衰减为小值波动的过程比较缓慢或非常连续。

![1](AppliedTimeSeries_yinaihua_files/7.jpg)

#### **p，q阶数的确定**

根据刚才判定截尾和拖尾的准则，p，q的确定基于如下的规则：

![1](AppliedTimeSeries_yinaihua_files/8.jpg)

根据不同的截尾和拖尾的情况，我们可以选择AR模型，也可以选择MA模型，当然也可以选择ARIMA模型。

#### 参数估计

通过拖尾和截尾对模型进行定阶的方法，往往具有很强的主观性。回想我们之前在参数预估的时候往往是怎么做的，不就是损失和正则项的加权么？我们这里能不能结合最终的预测误差来确定p，q的阶数呢？在相同的预测误差情况下，根据奥斯卡姆剃刀准则，模型越小是越好的。那么，平衡预测误差和参数个数，我们可以根据信息准则函数法，来确定模型的阶数。预测误差通常用平方误差即残差平方和来表示。

常用的信息准则函数法有下面几种：

###### **AIC准则**

AIC准则全称为全称是最小化信息量准则（Akaike Information Criterion），计算公式如下：
AIC = =2 *（模型参数的个数）-2ln（模型的极大似然函数）

###### **BIC准则**

AIC准则存在一定的不足之处。当样本容量很大时，在AIC准则中拟合误差提供的信息就要受到样本容量的放大，而参数个数的惩罚因子却和样本容量没关系（一直是2），因此当样本容量很大时，使用AIC准则选择的模型不收敛与真实模型，它通常比真实模型所含的未知参数个数要多。BIC（Bayesian InformationCriterion）贝叶斯信息准则弥补了AIC的不足，计算公式如下：

BIC = ln(n) * (模型中参数的个数) - 2ln(模型的极大似然函数值)，n是样本容量

注：这两个准则也是对模型进行评估的重要参数

### 6.模型检验

ARIMA模型检验主要有两个：
1）检验参数估计的显著性（t检验）
2）检验残差序列的随机性，即残差之间是独立的:自相关函数法

```python
#################例题7.1
######我选取USB的日收益的1：500个数据进行ARIMA模型和AR模型，并编写python程序，求解这一章中提到的均方误差进行模型评估
######（注：这两个模型只是展现一下如何用python实现模型的评估，所以模型的拟合效果可能不是很好）
```

```
import numpy as np
import pandas as pd
import re
import requests
from matplotlib import pyplot as plt
from scipy.stats import chi2_contingency
from scipy.stats import chi2
```

```
data = pd.read_csv('/home/yinaihua/Desktop/时间序列/期末作业/chapter-07/USB.csv')
```

```
print(data.head())
```

```
    USB.Open   USB.High    USB.Low  USB.Close  USB.Volume  USB.Adjusted
0  35.610001  36.290001  35.610001  36.169998     7697500     25.120195
1  36.169998  36.250000  35.910000  36.200001     5409000     25.141035
2  36.189999  36.189999  35.700001  35.770000     5748100     24.842396
3  35.700001  36.189999  35.490002  35.650002     5752100     24.759054
4  35.730000  35.880001  35.360001  35.630001     6548400     24.745163
```

```
print(data.dtypes)
```

```
Open=data['USB.Open']
Close=data['USB.Close']
#print(Open[0])
```

```
shoupanjia=Close[::-1]
rishouyi = shoupanjia.diff(1)
plt.plot(rishouyi,'r-')
```

```
[<matplotlib.lines.Line2D at 0x7f32d2450ac8>]
```




![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_5_1.png)


```python
##########首先是ARIMA（1，2，3）模型
from statsmodels.tsa.arima_model import ARIMA
model = ARIMA(mydata, (1,2,3)).fit() 
print(model.summary2()) #给出一份模型报告
```

    /home/yinaihua/ENTER/lib/python3.7/site-packages/statsmodels/base/model.py:488: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available
      'available', HessianInversionWarning)
    /home/yinaihua/ENTER/lib/python3.7/site-packages/statsmodels/tsa/arima_model.py:1455: RuntimeWarning: invalid value encountered in sqrt
      return np.sqrt(np.diag(-inv(hess)))
    /home/yinaihua/ENTER/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater
      return (self.a < x) & (x < self.b)
    /home/yinaihua/ENTER/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less
      return (self.a < x) & (x < self.b)
    /home/yinaihua/ENTER/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1821: RuntimeWarning: invalid value encountered in less_equal
      cond2 = cond0 & (x <= self.a)


                              Results: ARIMA
    ===================================================================
    Model:              ARIMA            BIC:                 1445.5962
    Dependent Variable: D2.USB.Close     Log-Likelihood:      -704.17  
    Date:               2020-08-07 17:50 Scale:               1.0000   
    No. Observations:   497              Method:              css-mle  
    Df Model:           5                Sample:              2        
    Df Residuals:       492                                   9        
    Converged:          1.0000           S.D. of innovations: 0.974    
    No. Iterations:     44.0000          HQIC:                1430.256 
    AIC:                1420.3447                                      
    -------------------------------------------------------------------
                        Coef.  Std.Err.    t     P>|t|   [0.025  0.975]
    -------------------------------------------------------------------
    const               0.0000   0.0000   0.2384 0.8117 -0.0000  0.0000
    ar.L1.D2.USB.Close -0.9717   0.0127 -76.4444 0.0000 -0.9966 -0.9467
    ma.L1.D2.USB.Close -1.0000   0.0199 -50.1347 0.0000 -1.0391 -0.9609
    ma.L2.D2.USB.Close -1.0000      nan      nan    nan     nan     nan
    ma.L3.D2.USB.Close  1.0000   0.0200  50.1088 0.0000  0.9609  1.0391
    ----------------------------------------------------------------------------
                    Real           Imaginary          Modulus          Frequency
    ----------------------------------------------------------------------------
    AR.1          -1.0292             0.0000           1.0292             0.5000
    MA.1          -1.0000            -0.0000           1.0000            -0.5000
    MA.2           1.0000            -0.0000           1.0000            -0.0000
    MA.3           1.0000             0.0000           1.0000             0.0000
    ===================================================================




```python
####评估模型预测结果
test=rishouyi[500:505]
predictions = model.predict(start=len(mydata), end=len(mydata)+len(test)-1, 
                            dynamic=False,typ='levels')######有差分，要加typ='levels'
predictions=np.matrix(predictions)
test=np.matrix(test)
# print(predictions[0,1])
# print(test[0,1])
#print(test)
for i in range(5):
    #print(test[i])
    print('predicted=%f, expected=%f' ,predictions[0,i], test[0,i])

from sklearn.metrics import mean_squared_error
error = mean_squared_error(test, predictions)
print('Test MSE: %.3f' % error)

```

    predicted=%f, expected=%f -0.11808473770099881 -0.6400000000000006
    predicted=%f, expected=%f 0.07833345775694289 0.5799979999999962
    predicted=%f, expected=%f -0.11566732637562316 0.0
    predicted=%f, expected=%f 0.06968532146040313 -0.3899990000000031
    predicted=%f, expected=%f -0.11355847693377907 0.4500010000000074
    Test MSE: 0.213



```python
test=np.array(test)
predictions=np.array(predictions)
#print(test[0:1])
plt.plot(test[0,:],'r-')
plt.plot(predictions[0,:],'g-')
```




    [<matplotlib.lines.Line2D at 0x7f32d0652828>]




![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_29_1.png)



```python
############接着，使用AR（2）模型
from statsmodels.tsa.arima_model import ARIMA,ARMA
ar_2= ARMA(rishouyi[1:500], order=(2,0)).fit(disp=-1)
print(ar_2.summary2()) #给出一份模型报告

```

                               Results: ARMA
    ===================================================================
    Model:              ARMA             BIC:                 1407.8527
    Dependent Variable: USB.Close        Log-Likelihood:      -691.50  
    Date:               2020-08-07 17:51 Scale:               1.0000   
    No. Observations:   499              Method:              css-mle  
    Df Model:           3                Sample:              0        
    Df Residuals:       496                                   9        
    Converged:          1.0000           S.D. of innovations: 0.967    
    No. Iterations:     8.0000           HQIC:                1397.615 
    AIC:                1391.0023                                      
    --------------------------------------------------------------------
                      Coef.   Std.Err.     t     P>|t|    [0.025  0.975]
    --------------------------------------------------------------------
    const             0.0300    0.0465   0.6442  0.5197  -0.0612  0.1211
    ar.L1.USB.Close  -0.0638    0.0443  -1.4393  0.1507  -0.1507  0.0231
    ar.L2.USB.Close   0.1329    0.0443   2.9981  0.0029   0.0460  0.2198
    ----------------------------------------------------------------------------
                    Real           Imaginary          Modulus          Frequency
    ----------------------------------------------------------------------------
    AR.1          -2.5136             0.0000           2.5136             0.5000
    AR.2           2.9936             0.0000           2.9936             0.0000
    ===================================================================



    /home/yinaihua/ENTER/lib/python3.7/site-packages/scipy/signal/signaltools.py:1341: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.
      out_full[ind] += zi
    /home/yinaihua/ENTER/lib/python3.7/site-packages/scipy/signal/signaltools.py:1344: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.
      out = out_full[ind]
    /home/yinaihua/ENTER/lib/python3.7/site-packages/scipy/signal/signaltools.py:1350: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.
      zf = out_full[ind]



```python
######对AR（2）模型的预测结果进行评估
test=rishouyi[500:505]
predictions_ar2 = ar_2.predict(start=len(mydata), end=len(mydata)+len(test)-1, 
                               dynamic=False)
predictions_ar2=np.matrix(predictions_ar2)
test=np.matrix(test)
# print(predictions[0,1])
# print(test[0,1])
#print(test)
for i in range(5):
    #print(test[i])
    print('predicted_ar2=%f, expected=%f' ,predictions_ar2[0,i], test[0,i])

from sklearn.metrics import mean_squared_error
error = mean_squared_error(test, predictions_ar2)
print('Test MSE: %.3f' % error)
```

    predicted_ar2=%f, expected=%f -0.09890011862292097 -0.6400000000000006
    predicted_ar2=%f, expected=%f 0.07406466928593247 0.5799979999999962
    predicted_ar2=%f, expected=%f 0.010017302830095685 0.0
    predicted_ar2=%f, expected=%f 0.03708994069699921 -0.3899990000000031
    predicted_ar2=%f, expected=%f 0.026851064293955507 0.4500010000000074
    Test MSE: 0.182



```python
test=np.array(test)
predictions_ar2=np.array(predictions_ar2)
#print(test[0:1])
plt.plot(test[0,:],'r-')
plt.plot(predictions_ar2[0,:],'g-')
```




    [<matplotlib.lines.Line2D at 0x7f32ca2b0f28>]




![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_32_1.png)



```python
###########结论：从MSE可以看出，对于所选取的数据，AR（2）模型要优于ARIMA（1，2，3）
```


```python
##########例题7.2是对一个ARIMA(0,1,3)模型的评估分析，调用的python函数与例题7.1相同
##########例题7.3是对一个错误指定的模型的分析 无python代码
```

# 第8章  不可观察的组件模型，信号提取和滤波器

## 一.UCM

### 1.定义：

Unobserved Components Model（UCM）（Harvey（1989））将时间序列分解为趋势，季节，周期和因预测变量序列引起的回归效应等成分。

### 2.模型来源

在有影响力的文章中，Harvey和Jaeger（1993）描述了使用未观察到的组件模型（也称为“结构时间序列模型”）来得出商业周期的典型事实。特别是，他们提出这样的观点，即通常使用未观察到的组件方法可以更好地实现这些目标，而不是使用流行的Hodrick-Prescott滤波器或Box-Jenkins ARIMA建模技术。

### 3.未观察到的组件

statsmodels中可用的未观察到的组件模型可以写成：
$$
y_t = \underbrace{\mu_{t}}_{\text{trend}} + \underbrace{\gamma_{t}}_{\text{seasonal}} + \underbrace{c_{t}}_{\text{cycle}} + \sum_{j=1}^k \underbrace{\beta_j x_{jt}}_{\text{explanatory}} + \underbrace{\varepsilon_t}_{\text{irregular}}
$$

也就是说：响应时间序列=趋势，季节，周期和回归效应等组成部分的叠加…

模型中的每个组件都体现了系列动力学的一些重要特征。 模型中的组件具有其概率模型。 概率成分模型包括有意义的确定性模式（作为特殊情况）。

### 4.趋势

趋势成分是包含截距和线性时间趋势的回归模型的动态扩展。
$$
\begin{split}\begin{align}
\underbrace{\mu_{t+1}}_{\text{level}} & = \mu_t + \nu_t + \eta_{t+1} \qquad & \eta_{t+1} \sim N(0, \sigma_\eta^2) \\\\
\underbrace{\nu_{t+1}}_{\text{trend}} & = \nu_t + \zeta_{t+1} & \zeta_{t+1} \sim N(0, \sigma_\zeta^2) \\
\end{align}\end{split}
$$
其中级别是可以随着时间动态变化的拦截项的概括，趋势是时间趋势的通用性，因此斜率可以随着时间动态变化。

对于两个元素（水平和趋势），我们可以考虑以下模型：

（1）该元素包括还是不包括（如果包括趋势，则还必须包括一个级别）。

（2）元素是确定性还是随机性（即误差项的方差是否限制为零）

趋势被粗略地定义为在没有任何其他影响变量的情况下，该序列在一段时间内增加或减少或保持恒定的自然趋势。UCM可以通过两种方式对趋势进行建模：第一个是随机游动模型，表示该趋势在序列的时间段内大致保持恒定，第二个是局部线性趋势，具有向上或向下的斜率。

### 5.季节

$$
\gamma_t = - \sum_{j=1}^{s-1} \gamma_{t+1-j} + \omega_t \qquad \omega_t \sim N(0, \sigma_\omega^2)
$$

周期性（季节数）为s，并且定义性为（无误差项），整个一个完整周期中的季节分量总和为零。 包含误差项可以使季节性影响随时间变化。

此模型的变体是：

-周期性

-是否使季节影响随机。（如果季节性影响是随机的，则可以通过MLE（误差项的方差）估算一个附加参数。）

当存在受季节因素（例如，一年的四分之一或一周中的一天等）影响的一致的变化模式时，则存在季节性模式。

### 6.周期

周期性成分旨在在比季节性成分所捕获的时间更长的时间范围内捕获周期性影响。 例如，在经济学中，循环术语通常意在捕捉经济周期，然后预期其周期为“ 1.5至12年”（参见Durbin和Koopman）。

该周期写为：

​					
$$
\begin{split}\begin{align}
c_{t+1} & = c_t \cos \lambda_c + c_t^* \sin \lambda_c + \tilde \omega_t \qquad & \tilde \omega_t \sim N(0, \sigma_{\tilde \omega}^2) \\\\
c_{t+1}^* & = -c_t \sin \lambda_c + c_t^* \cos \lambda_c + \tilde \omega_t^* & \tilde \omega_t^* \sim N(0, \sigma_{\tilde \omega}^2)
\end{align}\end{split}
$$

参数λc（周期的频率）是MLE估计的附加参数。 如果季节性影响是随机的，则可以使用另一个参数来估计（误差项的方差-请注意，这里的两个误差项均具有相同的方差，但假定具有独立的分布）。

### 7.不规则部分

假设不规则分量是白噪声误差项。 它的方差是MLE估计的参数； 即
$$
\varepsilon_t \sim N(0, \sigma_\varepsilon^2)\varepsilon_t \sim N(0, \sigma_\varepsilon^2)
$$

在某些情况下，我们可能希望对不规则分量进行泛化以考虑自回归效应：
$$
\varepsilon_t = \rho(L) \varepsilon_{t-1} + \epsilon_t, \qquad \epsilon_t \sim N(0, \sigma_\epsilon^2)\varepsilon_t = \rho(L) \varepsilon_{t-1} + \epsilon_t, \qquad \epsilon_t \sim N(0, \sigma_\epsilon^2)
$$

在这种情况下，还将通过MLE估计自回归参数。

### 8.回归效应

我们可能想通过包含其他术语来允许解释变量
$$
\sum_{j=1}^k \beta_j x_{jt}$\sum_{j=1}^k \beta_j x_{jt}
$$

或通过包括
$$
\begin{split}\begin{align}
\delta w_t \qquad \text{where} \qquad w_t & = 0, \qquad t < \tau, \\\\
& = 1, \qquad t \ge \tau
\end{align}\end{split}
$$

这些附加参数可以通过MLE估算，也可以通过将其作为状态空间公式的组成部分进行估算。

### 9.拟合UCM模型

由于数据已经过季节性调整，并且没有明显的解释变量，因此考虑的通用模型为：
$$
y_t = \ underbrace {\ mu_ {t}} _ {\ text {trend}} + \ underbrace {c_ {t}} _ {\ text {cycle}} + \ underbrace {\ varepsilon_t} _ {\ text {regularular }}$ y_t = \ underbrace {\ mu_ {t}} _ {\ text {trend}} + \ underbrace {c_ {t}} _ {\ text {cycle}} + \ underbrace {\ varepsilon_t} _ {\ text {regularular }}
$$

不规则的将被认为是白噪声，并且该周期将是随机的和衰减的。 最终的建模选择是用于趋势组件的规范。 Harvey和Jaeger考虑了两种模型：

局部线性趋势（“无限制”模型）

平滑趋势（“强迫”模型，因为我们强迫ση= 0）

下面，我们为每种模型类型构造kwargs字典。 请注意，有两种方法可以指定模型。 一种方法是直接指定组件，如上面的公式所示。 另一种方法是使用映射到各种规范的字符串名称。

注：对于未观察到的组件模型，通常对自己绘制估计的未观察到的组件（例如水平，趋势和周期）本身更具指导性，以查看它们是否提供了有意义的数据描述。

## 滤波：

### 1.基本知识补充：

从时域到频域---傅里叶级数与傅里叶变换：世间万物都随着时间在不停的变动，人的身高、GDP、气温的变换等等。如果我们以时间作为横坐标，观察指标作为纵坐标，画出一个图形，并根据此进行分析。那么这个图形叫做时间序列的二维图形，这种分析叫做在时域上的分析。但是，这些看似随时间不停变动的序列背后，总有一些不随时间变动的因素，即有一些长周期序列。比如，GDP总会上升或者下降，气温总是夏季高于冬季。那么如何将这些长周期序列从整个时间序列里面分离出来，单独去分析这些长周期或者短周期序列呢？傅里叶为我们打开了新世界的大门，傅里叶认为任何连续周期信号都可以由一组适当的正弦曲线组合而成（事实上，后来几乎所有的信号都可以经过合适的变换变为一组正弦曲线的组合，不仅仅限于连续周期信号，周期信号主要靠傅立叶级数，非周期信号主要靠傅立叶变换。），这组不同的正弦波称之为谐波，这种变换称之为傅里叶变换。经过变换之后，一个时间序列就变为由一组不同频率、振幅和相位的波的叠加。如果我们以横轴为频率，纵轴为该频率信号幅度，将变化后的信号画出一个图形出来，这个图形称之为频谱图，在这个频谱图基础上进行分析，这种分析可以叫做在频域的上分析（所谓频域，就是所有频率构成的一个域，即频率域）。

### 2.滤波

滤波我们知道波长等于周期和频率的乘积，对于一个给定的波长，周期和频率成反比。周期越大，频率越小，反之亦然。因此我们只要将信号中不同频率的信号分离出来，其实我们就将不同周期的信号分离出来了。这种将信号中特定波段频率滤除的操作，称之为滤波。一个可以实现这种过程的系统，称之为滤波器。其中，当允许信号中较高频率（短周期序列）的成分通过滤波器时，这种滤波器叫做高通滤波器；当允许信号中较低频率（长周期序列）的成分通过滤波器时，这种滤波器叫做低通滤波器：如果我们设低频段的截止频率为fp1,高频段的截止频率为fp2。频率在fp1与fp2之间的信号能通过其它频率的信号被衰减的滤波器叫做带通滤波器。反之，频率在fp1到fp2的范围之间的被衰减，之外能通过的滤波器叫做带阻滤波器。我们要说的HP滤波系统其实是一个高通滤波器，即滤掉信号中的低频成分（长周期序列）。

### 3.H-P滤波法

来源：H-P滤波 1980年，Hodrick和Prescott首次使用滤波的方法来研究美国战后的经济景气情况。之后，这种方法被广泛地应用于对宏观经济趋势的分析研究中，并被称之为HP滤波。

用法：时间序列的H-P滤波就要分离出频率较高的成分，去掉频率较低的成分，也即去掉长期的趋势项，而对短期的随机波动项进行度量。根据H-P滤波，任何一个经济时间序列可以分解为一个趋势序列和短期波动序列之和。

![104408pidnd020htzedm0l](AppliedTimeSeries_yinaihua_files/104408pidnd020htzedm0l.webp)

### 4.B-P滤波法

定义：**带通滤波器**是指能通过某一频率范围内的频率分量、但将其他范围的频率分量衰减到极低水平的滤波器]，与带阻滤波器这些滤波器也可以用低通滤波器和高通滤波器的组合产生

原理：一个理想的带通滤波器应该有一个完全平坦的通带，在通带内没有放大或者衰减，并且在通带之外所有频率都被完全衰减掉，另外，通带外的转换在极小的频率范围完成。

实际上，并不存在理想的带通滤波器。滤波器并不能够将期望频率范围外的所有频率完全衰减掉，尤其是在所要的通带外还有一个被衰减但是没有被隔离的范围。这通常称为滤波器的滚降现象，并且使用每十倍频波器的性能就与设计更加接近。然而，随着滚降范围越来越小，通带就变得不再平坦，开始出现“波纹”。这种现象在通带的边缘处尤其明显，这种效应称为吉布斯现象。


```python
##########例题8.1是对All Share index的ARIMA（2，1，2）的一个分析，下面，我将使用USB的前500个收盘价数据寻找其合适的ARIMA模型并进行模型预测评估
####首先我们获取数据
exp_8_1_data=Close[1:500]
```


```python
#####然后我们对数据进行直观分析
#####定义数据直观分析函数
def SimAnlysis_ts(ts, w):
    roll_mean = ts.rolling(window = w).mean()
    roll_std = ts.rolling(window = w).std() 
    pd_ewma = ts.ewm(span=w).mean()

    plt.clf()
    plt.figure()
    plt.grid()
    plt.plot(ts, color='blue',label='Original')
    plt.plot(roll_mean, color='red', label='Rolling Mean')
    plt.plot(roll_std, color='black', label = 'Rolling Std')
    plt.plot(pd_ewma, color='yellow', label = 'EWMA')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show()
    #plt.savefig('./图片/'+title+'.pdf', format='pdf')
```


```python
#####然后我们直观分析选用的数据
SimAnlysis_ts(exp_8_1_data, 10)
#####可以看出，这是一个非平稳的时间序列
```




    <Figure size 432x288 with 0 Axes>




![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_37_1.png)



```python
######现在，我想寻找差分阶数
#####定义差分效果函数
def chafen(data,k):
    plt.plot(mydata.diff(k).dropna(),'g-')
    print('k阶差分效果')
    print(adf_test(mydata.diff(k).dropna()))

```


```python
chafen(exp_8_1_data,1)
#######从图中可以看出，一阶差分之后，差分效果有了很明显的提升，同时，p值接近0
```

    k阶差分效果
    Test Statistic                -7.734457e+00
    p-value                        1.103128e-11
    Lags Used                      1.700000e+01
    Number of Observations Used    4.800000e+02
    Critical Value (1%)           -3.444047e+00
    Critical Value (5%)           -2.867580e+00
    Critical Value (10%)          -2.569987e+00
    dtype: float64



![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_39_1.png)



```python
#########下面，我们寻找ARIMA的p,q
#######定义p,q寻找函数,原理是求得取BIC信息量达到最小的模型阶数(默认是1阶差分)
def queding_pq(D_data):
    ##############确定p,q值
    
    from statsmodels.tsa.arima_model import ARIMA
    #定阶
    
    #一般阶数不超过length/10
    
    #pmax = int(len(D_data)/10) 
    pmax = 6
    #一般阶数不超过length/10
    
    #qmax = int(len(D_data)/10) 
    qmax =6
    
    #bic矩阵
    
    bic_matrix = [] 
    for p in range(pmax+1):
      tmp = []
      for q in range(qmax+1):
    #存在部分报错，所以用try来跳过报错。
        try: 
          print(ARIMA(D_data, (p,1,q)).fit().bic)
          tmp.append(ARIMA(D_data, (p,1,q)).fit().bic)
        except:
          tmp.append(100000)
      bic_matrix.append(tmp)
    
    #从中可以找出最小值
    
    bic_matrix = pd.DataFrame(bic_matrix) 
    
    #先用stack展平，然后用idxmin找出最小值位置。
    
    p,q = bic_matrix.stack().idxmin() 
    
    print(u'BIC最小的p值和q值为：%s、%s' %(p,q))
    
    # 取BIC信息量达到最小的模型阶数。
```


```python
queding_pq(exp_8_1_data)
```

    /home/yinaihua/ENTER/lib/python3.7/site-packages/scipy/signal/signaltools.py:1341: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.
      out_full[ind] += zi
    /home/yinaihua/ENTER/lib/python3.7/site-packages/scipy/signal/signaltools.py:1344: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.
      out = out_full[ind]
    /home/yinaihua/ENTER/lib/python3.7/site-packages/scipy/signal/signaltools.py:1350: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.
      zf = out_full[ind]


    1261.0012933556998
    1253.9713171466638
    1253.40497514352
    1259.5975475547405
    1263.6643812598963
    1269.7701689963017
    1274.8962875929765
    1257.1012169559021
    1253.169760364578
    1259.2961182799465
    1255.2608774173384
    1259.3252208119852
    1265.0254022744732
    1261.1840200216523
    1259.96054802585
    1269.3314089650244


    /home/yinaihua/ENTER/lib/python3.7/site-packages/statsmodels/base/model.py:488: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available
      'available', HessianInversionWarning)
    /home/yinaihua/ENTER/lib/python3.7/site-packages/statsmodels/base/model.py:508: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals
      "Check mle_retvals", ConvergenceWarning)


    1273.0341974599824


    /home/yinaihua/ENTER/lib/python3.7/site-packages/statsmodels/base/model.py:488: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available
      'available', HessianInversionWarning)
    /home/yinaihua/ENTER/lib/python3.7/site-packages/statsmodels/base/model.py:508: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals
      "Check mle_retvals", ConvergenceWarning)


    1264.2690989801483
    1270.0246204829305
    1275.5214286861717
    1267.5665370439197
    1270.3564778227897
    1276.235210293071
    1276.078930777221


    /home/yinaihua/ENTER/lib/python3.7/site-packages/statsmodels/tsa/tsatools.py:607: RuntimeWarning: invalid value encountered in true_divide
      (1+np.exp(-params))).copy()
    /home/yinaihua/ENTER/lib/python3.7/site-packages/statsmodels/tsa/tsatools.py:609: RuntimeWarning: invalid value encountered in true_divide
      (1+np.exp(-params))).copy()
    /home/yinaihua/ENTER/lib/python3.7/site-packages/statsmodels/base/model.py:488: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available
      'available', HessianInversionWarning)


    1284.442152740725


    /home/yinaihua/ENTER/lib/python3.7/site-packages/statsmodels/base/model.py:488: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available
      'available', HessianInversionWarning)


    1274.097189017608
    1279.488187693548
    1273.0414832268489
    1279.0241088580385


    /home/yinaihua/ENTER/lib/python3.7/site-packages/statsmodels/base/model.py:488: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available
      'available', HessianInversionWarning)


    nan


    /home/yinaihua/ENTER/lib/python3.7/site-packages/statsmodels/base/model.py:488: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available
      'available', HessianInversionWarning)


    BIC最小的p值和q值为：1、1



```python
##########所以确定是ARIMA（1，1，1）模型
from statsmodels.tsa.arima_model import ARIMA
model_8_1 = ARIMA(exp_8_1_data, (1,1,1)).fit() 
print(model_8_1.summary2()) #给出一份模型报告
```

                              Results: ARIMA
    ===================================================================
    Model:              ARIMA            BIC:                 1253.1698
    Dependent Variable: D.USB.Close      Log-Likelihood:      -614.16  
    Date:               2020-08-07 17:51 Scale:               1.0000   
    No. Observations:   498              Method:              css-mle  
    Df Model:           3                Sample:              1        
    Df Residuals:       495                                   9        
    Converged:          1.0000           S.D. of innovations: 0.830    
    No. Iterations:     17.0000          HQIC:                1242.937 
    AIC:                1236.3274                                      
    -------------------------------------------------------------------
                         Coef.  Std.Err.    t    P>|t|   [0.025  0.975]
    -------------------------------------------------------------------
    const               -0.0241   0.0237 -1.0149 0.3107 -0.0705  0.0224
    ar.L1.D.USB.Close    0.5240   0.1547  3.3866 0.0008  0.2208  0.8273
    ma.L1.D.USB.Close   -0.6975   0.1312 -5.3153 0.0000 -0.9547 -0.4403
    ----------------------------------------------------------------------------
                    Real           Imaginary          Modulus          Frequency
    ----------------------------------------------------------------------------
    AR.1           1.9082             0.0000           1.9082             0.0000
    MA.1           1.4337             0.0000           1.4337             0.0000
    ===================================================================




```python
######对ARIMA（1，1，1）模型的预测结果进行评估
test_81=Close[500:505]
predictions__81 = model_8_1.predict(start=len(exp_8_1_data), 
                                    end=len(exp_8_1_data)+len(test_81)-1, dynamic=False,
                                    typ='levels')
predictions__81=np.matrix(predictions__81)
test_81=np.matrix(test_81)
# print(predictions[0,1])
# print(test[0,1])
#print(test)
for i in range(5):
    #print(test[i])
    print('predicted_81=%f, expected=%f' ,predictions__81[0,i], test_81[0,i])

from sklearn.metrics import mean_squared_error
error = mean_squared_error(test_81, predictions__81)
print('Test_81 MSE: %.3f' % error)
```

    predicted_81=%f, expected=%f 23.99280369670552 24.25
    predicted_81=%f, expected=%f 23.982817672827352 23.709999
    predicted_81=%f, expected=%f 23.96612928396315 24.110001
    predicted_81=%f, expected=%f 23.94592856688879 25.01
    predicted_81=%f, expected=%f 23.923887238406138 25.25
    Test_81 MSE: 0.610



```python
###########例题8.2实现的是用H-P trend filter获取全球气温趋势，下面，我们直接使用datasets模块自带的数据集进行趋势获取
import statsmodels.api as sm
nt=sm.datasets.macrodata.NOTE#datasets模块包含了很多数据集，我们在这里调用macrodata这个数据集，然后查看这个数据集的相关信息
```


```python
######首先载入数据集
df=sm.datasets.macrodata.load_pandas().data
print(df.head())
```

         year  quarter   realgdp  realcons  realinv  realgovt  realdpi    cpi  \
    0  1959.0      1.0  2710.349    1707.4  286.898   470.045   1886.9  28.98   
    1  1959.0      2.0  2778.801    1733.7  310.859   481.301   1919.7  29.15   
    2  1959.0      3.0  2775.488    1751.8  289.226   491.260   1916.4  29.35   
    3  1959.0      4.0  2785.204    1753.7  299.356   484.052   1931.3  29.37   
    4  1960.0      1.0  2847.699    1770.5  331.722   462.199   1955.5  29.54   
    
          m1  tbilrate  unemp      pop  infl  realint  
    0  139.7      2.82    5.8  177.146  0.00     0.00  
    1  141.7      3.08    5.1  177.830  2.34     0.74  
    2  140.5      3.82    5.3  178.657  2.74     1.09  
    3  140.0      4.33    5.6  179.386  0.27     4.06  
    4  139.6      3.50    5.2  180.007  2.31     1.19  



```python
######重新定义日期
index=pd.Index(sm.tsa.datetools.dates_from_range('1959Q1', '2009Q3'))
df.index=index
```


```python
######在这里，我们使用H-P trend filter
gdp_cyclical, gdp_trend=sm.tsa.filters.hpfilter(df['realgdp'])
print(gdp_trend.head())
```

    1959-03-31    2670.837085
    1959-06-30    2698.712468
    1959-09-30    2726.612545
    1959-12-31    2754.612067
    1960-03-31    2782.816333
    Name: realgdp, dtype: float64



```python
########下面，我们将趋势和原数据一起作图
df['gdp_trend']=gdp_trend
df['gdp_cyclical']=gdp_cyclical
df[['realgdp', 'gdp_trend']].plot(figsize=(16, 6), color=['darkgreen', 'red'], linestyle='dashed')
########可以看出，趋势与数据符合的相当的不错！
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7f32c38e0a90>




![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_48_1.png)



```python
######例题8.2使用了H-p filter 获取了英国实际人均对数的趋势，我们使用USB的收盘价1：500数据试一下
######使用USB的收盘价1：500数据试一下
cyclical, trend=sm.tsa.filters.hpfilter(exp_8_1_data)#######获得的是周期项和趋势项
plt.plot(trend,'r-.')
plt.plot(exp_8_1_data,'y-.')
#######可以看出，趋势与我们的数据符合的比较好！
```




    [<matplotlib.lines.Line2D at 0x7f32ca1f9940>]




![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_49_1.png)

# 第9章 季节性和指数平滑

### 1.季节性时间序列

有些时间序列呈现出一定的循环或周期性，这样的时间序列叫**季节性时间序列。**

### 2.季节指数

所谓季节指数就是用简单平均法计算的周期内各时期季节性影响的相对数。

![3a587809453a664e65714e16d4294c7c_720w](AppliedTimeSeries_yinaihua_files/3a587809453a664e65714e16d4294c7c_720w.png)

首先计算周期内各期平均数





![97d336aaef67740c13b95675024f6d5a_720w](AppliedTimeSeries_yinaihua_files/97d336aaef67740c13b95675024f6d5a_720w.png)

然后计算总平均数





![21b361eeb2440291a46167fd066c93b3_720w](AppliedTimeSeries_yinaihua_files/21b361eeb2440291a46167fd066c93b3_720w.png)

再计算季节指数



![857abf4056920d1b424e4ec51bb86a17_720w](AppliedTimeSeries_yinaihua_files/857abf4056920d1b424e4ec51bb86a17_720w.png)

季节指数反映了该季度与总平均值之间的一种比较稳定的关系：

**如果比值大于1，说明该季度的值常常会高于总平均值；**

**如果比值小于1，说明该季度的值常常低于总平均值；**

**如果序列的季节指数都近似为1，就说明该序列没有明显的季节性。**

### 3.**季节性差分**

在有些应用中，季节性是次要的，我们需要把它从数据中消除，这个过程叫**季节调整**，其中**季节性差分化**是一种常见的方法。

我们了解差分（**正规差分化**），其形式为：



![img](https://picb.zhimg.com/80/a68207b3cf26b09c89abfe78d5348fd9_720w.png)

我们将它推广，如果一个序列具有周期性，且周期为s，则**季节性差分化**为：



![img](https://pic2.zhimg.com/80/61aab7ee80c31e773088f8cd52c1a1a9_720w.png)

### 4.纯季节性模型（pure seasonal models）

有些时间序列数据会表现出一定的周期性。有些月度数据，其季节性可以以12个月为一个周期也可能以一个季度为一个周期。
 纯季节性模型的数据只与其季节性滞后期的数据相关（is correlated at the seasonal lags only），季节性自回归阶数用P表示，季节性差分阶数用D表示，季节性移动平均阶数用Q表示，另外用S表示季节性周期的长度，常见的有12、4等。
 对应的季节性模型表示为AR（P）_s,MA(Q)_s, ARMA(P,Q)_s
 它们的ACF PACF图的特点和之前介绍的非季节性的对应模型类似：

|      |      AR(P)_s      |      MA(Q)_s      |      ARMA(P,Q)_s      |
| ---- | :---------------: | :---------------: | :-------------------: |
| ACF  | 在季节性lag处拖尾 |  在lag QS处截尾   | 在季节性lag处拖尾拖尾 |
| PACF |  在lag PS处截尾   | 在季节性lag处拖尾 | 在季节性lag处拖尾拖尾 |

纯季节性模型的数据在非季节性lag处自相关系数/偏自相关系数为0

### 5.**多重季节性模型**

经过了季节性差分和正规差分后，序列成为了平稳时间序列，则我们可以用MA模型对多重差分后的模型建模。则我们有模型：

![8d54a2cf07f6c877dcdd98ce367bf939_720w](AppliedTimeSeries_yinaihua_files/8d54a2cf07f6c877dcdd98ce367bf939_720w.png)

其中，B为**向后推移算子**，代表前一时刻的值。 B的s次方则为前s时刻的值。 {at}为白噪声序列，s为序列的周期。上述模型为**航空模型**。

另外，序列经过了季节性差分和正规差分消除序列相关性（序列平稳），则可以认为是间隔s和间隔1的周期共同影响，因此该模型称为**多重季节性MA模型**

### 6.参数估计

例：

 首先记：



![8660b236ac5553a23c25f4be370b0ab7_720w](AppliedTimeSeries_yinaihua_files/8660b236ac5553a23c25f4be370b0ab7_720w.png)

根据白噪声性质，我们计算序列的方差与协方差如下：

![5164919e46f6f6a2db9a21ec06854a35_720w](AppliedTimeSeries_yinaihua_files/5164919e46f6f6a2db9a21ec06854a35_720w.png)

若s=12,则我们的s为12，根据公式：

![99309b167d63b6f2071a30dfe676da36_720w](AppliedTimeSeries_yinaihua_files/99309b167d63b6f2071a30dfe676da36_720w.png)

可以计算相关系数

解3个方程的方程组。最终可归结为求两个二元一次方程：

![58d07129de15d1f267f936b6df9ff4a1_720w](AppliedTimeSeries_yinaihua_files/58d07129de15d1f267f936b6df9ff4a1_720w.png)

### 7.**综合分析模型（加法模型、乘法模型、混合模型）**——与后面的状态空间模型的分解部分有关

通常时间序列包括3个因素：

**趋势因素T**

**季节性因素S**

**不规则因素I**

有时候也会加入周期要素C,我们这里将它归为季节性因素

常用的综合分析模型有：

**加法模型**



![img](https://picb.zhimg.com/80/eb01c0e8f1b0cbf07573398fb90cf7b4_720w.png)

**乘法模型**





![img](https://picb.zhimg.com/80/b39c9d9ff3b15ba8f3a8f94738285d17_720w.png)

**混合模型**



![img](https://pic4.zhimg.com/80/6df67da2d92aab79d2838fd47ab2434f_720w.png)

```python
#######例题9.1 确定性季节性模型(季节性模式是确定的，可以通过绘图看出季节性模式是天，周月，还是季度)
#######下面使用的是大气的CO2月度数据（1959年1月至1987年12月）
# 该在整个样本中都有明显的趋势和季节性。
co2 = [315.58, 316.39, 316.79, 317.82, 318.39, 318.22, 316.68, 315.01, 314.02, 313.55,
       315.02, 315.75, 316.52, 317.10, 317.79, 319.22, 320.08, 319.70, 318.27, 315.99,
       314.24, 314.05, 315.05, 316.23, 316.92, 317.76, 318.54, 319.49, 320.64, 319.85,
       318.70, 316.96, 315.17, 315.47, 316.19, 317.17, 318.12, 318.72, 319.79, 320.68,
       321.28, 320.89, 319.79, 317.56, 316.46, 315.59, 316.85, 317.87, 318.87, 319.25,
       320.13, 321.49, 322.34, 321.62, 319.85, 317.87, 316.36, 316.24, 317.13, 318.46,
       319.57, 320.23, 320.89, 321.54, 322.20, 321.90, 320.42, 318.60, 316.73, 317.15,
       317.94, 318.91, 319.73, 320.78, 321.23, 322.49, 322.59, 322.35, 321.61, 319.24,
       318.23, 317.76, 319.36, 319.50, 320.35, 321.40, 322.22, 323.45, 323.80, 323.50,
       322.16, 320.09, 318.26, 317.66, 319.47, 320.70, 322.06, 322.23, 322.78, 324.10,
       324.63, 323.79, 322.34, 320.73, 319.00, 318.99, 320.41, 321.68, 322.30, 322.89,
       323.59, 324.65, 325.30, 325.15, 323.88, 321.80, 319.99, 319.86, 320.88, 322.36,
       323.59, 324.23, 325.34, 326.33, 327.03, 326.24, 325.39, 323.16, 321.87, 321.31,
       322.34, 323.74, 324.61, 325.58, 326.55, 327.81, 327.82, 327.53, 326.29, 324.66,
       323.12, 323.09, 324.01, 325.10, 326.12, 326.62, 327.16, 327.94, 329.15, 328.79,
       327.53, 325.65, 323.60, 323.78, 325.13, 326.26, 326.93, 327.84, 327.96, 329.93,
       330.25, 329.24, 328.13, 326.42, 324.97, 325.29, 326.56, 327.73, 328.73, 329.70,
       330.46, 331.70, 332.66, 332.22, 331.02, 329.39, 327.58, 327.27, 328.30, 328.81,
       329.44, 330.89, 331.62, 332.85, 333.29, 332.44, 331.35, 329.58, 327.58, 327.55,
       328.56, 329.73, 330.45, 330.98, 331.63, 332.88, 333.63, 333.53, 331.90, 330.08,
       328.59, 328.31, 329.44, 330.64, 331.62, 332.45, 333.36, 334.46, 334.84, 334.29,
       333.04, 330.88, 329.23, 328.83, 330.18, 331.50, 332.80, 333.22, 334.54, 335.82,
       336.45, 335.97, 334.65, 332.40, 331.28, 330.73, 332.05, 333.54, 334.65, 335.06,
       336.32, 337.39, 337.66, 337.56, 336.24, 334.39, 332.43, 332.22, 333.61, 334.78,
       335.88, 336.43, 337.61, 338.53, 339.06, 338.92, 337.39, 335.72, 333.64, 333.65,
       335.07, 336.53, 337.82, 338.19, 339.89, 340.56, 341.22, 340.92, 339.26, 337.27,
       335.66, 335.54, 336.71, 337.79, 338.79, 340.06, 340.93, 342.02, 342.65, 341.80,
       340.01, 337.94, 336.17, 336.28, 337.76, 339.05, 340.18, 341.04, 342.16, 343.01,
       343.64, 342.91, 341.72, 339.52, 337.75, 337.68, 339.14, 340.37, 341.32, 342.45,
       343.05, 344.91, 345.77, 345.30, 343.98, 342.41, 339.89, 340.03, 341.19, 342.87,
       343.74, 344.55, 345.28, 347.00, 347.37, 346.74, 345.36, 343.19, 340.97, 341.20,
       342.76, 343.96, 344.82, 345.82, 347.24, 348.09, 348.66, 347.90, 346.27, 344.21,
       342.88, 342.58, 343.99, 345.31, 345.98, 346.72, 347.63, 349.24, 349.83, 349.10,
       347.52, 345.43, 344.48, 343.89, 345.29, 346.54, 347.66, 348.07, 349.12, 350.55,
       351.34, 350.80, 349.10, 347.54, 346.20, 346.20, 347.44, 348.67]
#####添加日期
co2 = pd.Series(co2, index=pd.date_range('1-1-1959', periods=len(co2), freq='M'), name = 'CO2')
co2.describe()####看看数据的统计情况
```




    count    348.000000
    mean     330.123879
    std       10.059747
    min      313.550000
    25%      321.302500
    50%      328.820000
    75%      338.002500
    max      351.340000
    Name: CO2, dtype: float64




```python
######我们使用季节性分解命令，画图看一下季节性趋势
from statsmodels.tsa.seasonal import seasonal_decompose
stl = seasonal_decompose(co2)

fig = stl.plot()
######我们可以看出，CO2的季节性趋势非常明显，也就是一个确定性的季节性模型
```


![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_51_0.png)



```python
#######例题9.2实现的是一个季节性ARIMA建模，我将采用
#########portland-oregon-average-monthly.csv数据进行ARIMA建模并进行预测和评价
###########首先导入数据，简单观察一下数据并进行可视化
%matplotlib inline
 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import datetime
from dateutil.relativedelta import relativedelta
import seaborn as sns
import statsmodels.api as sm  
from statsmodels.tsa.stattools import acf  
from statsmodels.tsa.stattools import pacf
from statsmodels.tsa.seasonal import seasonal_decompose
 
df = pd.read_csv('/home/yinaihua/Desktop/时间序列/期末作业/chapter-07/file/portland-oregon-average-monthly.csv',
                 parse_dates=['month'], index_col='month')
print(df.head())
df['riders'].plot(figsize=(12,8), title= 'Monthly Ridership', fontsize=14)
plt.savefig('month_ridership.png', bbox_inches='tight')


```

                riders
    month             
    1973-01-01     648
    1973-02-01     646
    1973-03-01     639
    1973-04-01     654
    1973-05-01     630



![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_52_1.png)



```python
#########然后我们分解数据
decomposition = seasonal_decompose(df['riders'], freq=12)
fig = plt.figure()  
fig = decomposition.plot()  
fig.set_size_inches(12, 6)
```




    <Figure size 432x288 with 0 Axes>




![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_53_1.png)



```python
########编写函数测试一下时间序列是否稳定（这个在前面ARIMA建模中已经写过，稍有不同）
from statsmodels.tsa.stattools import adfuller
def test_stationarity(timeseries):
    
    
    rolmean = timeseries.rolling(window=12).mean()
    rolstd = timeseries.rolling(window=12).std()
 
  
    fig = plt.figure(figsize=(12, 8))
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show()
    
    #Perform Dickey-Fuller test:
    print('Results of Dickey-Fuller Test:')
    dftest = adfuller(timeseries, autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in dftest[4].items():
        dfoutput['Critical Value (%s)'%key] = value
    print(dfoutput)
```


```python
########我们测试一下稳定性
test_stationarity(df['riders'])
########从p值大于0.5和t统计量大于三个Critical Value可以得出，整体的序列并没有到达稳定性要求
```


![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_55_0.png)


    Results of Dickey-Fuller Test:
    Test Statistic                  -1.536597
    p-value                          0.515336
    #Lags Used                      12.000000
    Number of Observations Used    101.000000
    Critical Value (1%)             -3.496818
    Critical Value (5%)             -2.890611
    Critical Value (10%)            -2.582277
    dtype: float64


```python
#########我们先进行一阶差分
df['first_difference'] = df['riders'].diff(1)  
test_stationarity(df['first_difference'].dropna(inplace=False))

```


![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_57_0.png)


    Results of Dickey-Fuller Test:
    Test Statistic                  -1.938696
    p-value                          0.314082
    #Lags Used                      11.000000
    Number of Observations Used    101.000000
    Critical Value (1%)             -3.496818
    Critical Value (5%)             -2.890611
    Critical Value (10%)            -2.582277
    dtype: float64



```python
###############再来看一下12阶查分（即季节查分）
df['seasonal_difference'] = df['riders'].diff(12)  
test_stationarity(df['seasonal_difference'].dropna(inplace=False))
#####从图形上，比一阶差分更不稳定（虽然季节指标已经出来了）

```


![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_58_0.png)


    Results of Dickey-Fuller Test:
    Test Statistic                 -2.469741
    p-value                         0.123011
    #Lags Used                      3.000000
    Number of Observations Used    98.000000
    Critical Value (1%)            -3.498910
    Critical Value (5%)            -2.891516
    Critical Value (10%)           -2.582760
    dtype: float64



```python
#我们再来将一阶查分和季节查分合并起来，再来看下
df['seasonal_first_difference'] = df['first_difference'].diff(12)  
test_stationarity(df['seasonal_first_difference'].dropna(inplace=False))
#####从方差图像可以看出，不稳定
```


![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_59_0.png)


    Results of Dickey-Fuller Test:
    Test Statistic                -9.258520e+00
    p-value                        1.427874e-15
    #Lags Used                     0.000000e+00
    Number of Observations Used    1.000000e+02
    Critical Value (1%)           -3.497501e+00
    Critical Value (5%)           -2.890906e+00
    Critical Value (10%)          -2.582435e+00
    dtype: float64



```python
# 所以再来看看取对数的数据
df['riders_log']= np.log(df['riders'])
test_stationarity(df['riders_log'])
```


![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_60_0.png)


    Results of Dickey-Fuller Test:
    Test Statistic                  -1.677830
    p-value                          0.442570
    #Lags Used                      12.000000
    Number of Observations Used    101.000000
    Critical Value (1%)             -3.496818
    Critical Value (5%)             -2.890611
    Critical Value (10%)            -2.582277
    dtype: float64



```python
# 在此基础上我们再来做下一阶差分：
df['log_first_difference'] = df['riders_log'].diff(1)  
test_stationarity(df['log_first_difference'].dropna(inplace=False))
##我们再来对对数进行季节性差分：
```


![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_61_0.png)


    Results of Dickey-Fuller Test:
    Test Statistic                  -2.047539
    p-value                          0.266126
    #Lags Used                      11.000000
    Number of Observations Used    101.000000
    Critical Value (1%)             -3.496818
    Critical Value (5%)             -2.890611
    Critical Value (10%)            -2.582277
    dtype: float64



```python
df['log_seasonal_difference'] = df['riders_log'].diff(12)  
test_stationarity(df['log_seasonal_difference'].dropna(inplace=False))
```


![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_62_0.png)


    Results of Dickey-Fuller Test:
    Test Statistic                  -1.919681
    p-value                          0.322860
    #Lags Used                       0.000000
    Number of Observations Used    101.000000
    Critical Value (1%)             -3.496818
    Critical Value (5%)             -2.890611
    Critical Value (10%)            -2.582277
    dtype: float64



```python
#####再来试试在对数基础上进行一阶差分+季节差分的效果:
df['log_seasonal_first_difference'] = df['log_first_difference'].diff(12)  
test_stationarity(df['log_seasonal_first_difference'].dropna(inplace=False))
########可以看到稳定了满足需求了
```


![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_63_0.png)


    Results of Dickey-Fuller Test:
    Test Statistic                -8.882112e+00
    p-value                        1.309452e-14
    #Lags Used                     0.000000e+00
    Number of Observations Used    1.000000e+02
    Critical Value (1%)           -3.497501e+00
    Critical Value (5%)           -2.890906e+00
    Critical Value (10%)          -2.582435e+00
    dtype: float64



```python
#######下面我们绘制ACF和PACF图，找出最佳参数
fig = plt.figure(figsize=(12,8))
ax1 = fig.add_subplot(211)
fig = sm.graphics.tsa.plot_acf(df['seasonal_first_difference'].iloc[13:], lags=40, ax=ax1) #从13开始是因为做季节性差分时window是12
ax2 = fig.add_subplot(212)
fig = sm.graphics.tsa.plot_pacf(df['seasonal_first_difference'].iloc[13:], lags=40, ax=ax2)
######我们确定参数为 order=(0,1,0), seasonal_order=(1,1,1,12)
```

    /home/yinaihua/ENTER/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:1283: RuntimeWarning: invalid value encountered in sqrt
      return rho, np.sqrt(sigmasq)



![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_64_1.png)



```python
########创建模型并进行预测
mod = sm.tsa.statespace.SARIMAX(df['riders'], trend='n', order=(0,1,0), seasonal_order=(1,1,1,12))
results = mod.fit()
print(results.summary())
```

    /home/yinaihua/ENTER/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:171: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.
      % freq, ValueWarning)


                                     Statespace Model Results                                 
    ==========================================================================================
    Dep. Variable:                             riders   No. Observations:                  114
    Model:             SARIMAX(0, 1, 0)x(1, 1, 1, 12)   Log Likelihood                -501.340
    Date:                            Fri, 07 Aug 2020   AIC                           1008.680
    Time:                                    17:02:23   BIC                           1016.526
    Sample:                                01-01-1973   HQIC                          1011.856
                                         - 06-01-1982                                         
    Covariance Type:                              opg                                         
    ==============================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
    ------------------------------------------------------------------------------
    ar.S.L12       0.3236      0.186      1.739      0.082      -0.041       0.688
    ma.S.L12      -0.9990     42.039     -0.024      0.981     -83.394      81.397
    sigma2       984.8205   4.13e+04      0.024      0.981   -7.99e+04    8.19e+04
    ===================================================================================
    Ljung-Box (Q):                       36.56   Jarque-Bera (JB):                 4.81
    Prob(Q):                              0.63   Prob(JB):                         0.09
    Heteroskedasticity (H):               1.48   Skew:                             0.38
    Prob(H) (two-sided):                  0.26   Kurtosis:                         3.75
    ===================================================================================
    
    Warnings:
    [1] Covariance matrix calculated using the outer product of gradients (complex-step).



```python
#########预测值与真实值比较：
df['forecast'] = results.predict(start = 102, end= 114, dynamic= True)  
df[['riders', 'forecast']].plot(figsize=(12, 6)) 
plt.savefig('ts_df_predict.png', bbox_inches='tight')
##########可以看出结果比较接近
```


![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_66_0.png)



```python
npredict =df['riders']['1982'].shape[0]
fig, ax = plt.subplots(figsize=(12,6))
npre = 12
ax.set(title='Ridership', xlabel='Date', ylabel='Riders')
ax.plot(df.index[-npredict-npre+1:], df.ix[-npredict-npre+1:, 'riders'], 'o', label='Observed')
ax.plot(df.index[-npredict-npre+1:], df.ix[-npredict-npre+1:, 'forecast'], 'g', label='Dynamic forecast')
legend = ax.legend(loc='lower right')
legend.get_frame().set_facecolor('w')
# plt.savefig('ts_predict_compare.png', bbox_inches='tight')
```

    /home/yinaihua/ENTER/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: 
    .ix is deprecated. Please use
    .loc for label based indexing or
    .iloc for positional indexing
    
    See the documentation here:
    http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated
      """
    /home/yinaihua/ENTER/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: 
    .ix is deprecated. Please use
    .loc for label based indexing or
    .iloc for positional indexing
    
    See the documentation here:
    http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated




![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_67_1.png)



```python
########为了产生针对未来的预测值，我们加入新的时间
start = datetime.datetime.strptime("1982-07-01", "%Y-%m-%d")
date_list = [start + relativedelta(months=x) for x in range(0,12)]
future = pd.DataFrame(index=date_list, columns= df.columns)
df = pd.concat([df, future])
```


```python
########在新加入的时间上来预测未来值并绘图：
df['forecast'] = results.predict(start = 114, end = 125, dynamic= True)  
df[['riders', 'forecast']].ix[-24:].plot(figsize=(12, 8)) 
#plt.savefig('ts_predict_future.png', bbox_inches='tight')

```

    /home/yinaihua/ENTER/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: 
    .ix is deprecated. Please use
    .loc for label based indexing or
    .iloc for positional indexing
    
    See the documentation here:
    http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated
      This is separate from the ipykernel package so we can avoid doing imports until





    <matplotlib.axes._subplots.AxesSubplot at 0x7f5433020710>




![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_69_2.png)



```python
########在例题9.3中使用简单指数平滑预测,并说明简单指数平滑是最优预测方法。同时使用MSE进行预测评价
########我使用的是Hyndman和Athanasopoulos [1]关于指数平滑的优秀论文的第七章中的相关数据
# （比较适合指数平滑模型）
###############首先，我们加载一下数据：
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt
%matplotlib inline

data = [446.6565,  454.4733,  455.663 ,  423.6322,  456.2713,  440.5881, 425.3325,  485.1494,  506.0482,  526.792 ,  514.2689,  494.211 ]
index= pd.date_range(start='1996', end='2008', freq='A')
oildata = pd.Series(data, index)

data = [17.5534,  21.86  ,  23.8866,  26.9293,  26.8885,  28.8314, 30.0751,  30.9535,  30.1857,  31.5797,  32.5776,  33.4774, 39.0216,  41.3864,  41.5966]
index= pd.date_range(start='1990', end='2005', freq='A')
air = pd.Series(data, index)

data = [263.9177,  268.3072,  260.6626,  266.6394,  277.5158,  283.834 , 290.309 ,  292.4742,  300.8307,  309.2867,  318.3311,  329.3724, 338.884 ,  339.2441,  328.6006,  314.2554,  314.4597,  321.4138, 329.7893,  346.3852,  352.2979,  348.3705,  417.5629,  417.1236, 417.7495,  412.2339,  411.9468,  394.6971,  401.4993,  408.2705, 414.2428]
index= pd.date_range(start='1970', end='2001', freq='A')
livestock2 = pd.Series(data, index)

data = [407.9979 ,  403.4608,  413.8249,  428.105 ,  445.3387,  452.9942, 455.7402]
index= pd.date_range(start='2001', end='2008', freq='A')
livestock3 = pd.Series(data, index)

data = [41.7275,  24.0418,  32.3281,  37.3287,  46.2132,  29.3463, 36.4829,  42.9777,  48.9015,  31.1802,  37.7179,  40.4202, 51.2069,  31.8872,  40.9783,  43.7725,  55.5586,  33.8509, 42.0764,  45.6423,  59.7668,  35.1919,  44.3197,  47.9137]
index= pd.date_range(start='2005', end='2010-Q4', freq='QS-OCT')
aust = pd.Series(data, index)
```


```python
###########让我们使用“简单指数平滑”来预测一下的数据
################首先，可视化数据
ax=oildata.plot()
ax.set_xlabel("Year")
ax.set_ylabel("Oil (millions of tonnes)")
print("Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.")
```

    Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.



![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_71_1.png)



```python
###########接下来，我们将实现三种简单的指数平滑方法来实现对油数据的预测
fit1 = SimpleExpSmoothing(oildata).fit(smoothing_level=0.2,optimized=False)
fcast1 = fit1.forecast(3).rename(r'$\alpha=0.2$')
fit2 = SimpleExpSmoothing(oildata).fit(smoothing_level=0.6,optimized=False)
fcast2 = fit2.forecast(3).rename(r'$\alpha=0.6$')
fit3 = SimpleExpSmoothing(oildata).fit()
fcast3 = fit3.forecast(3).rename(r'$\alpha=%s$'%fit3.model.params['smoothing_level'])

plt.figure(figsize=(12, 8))
plt.plot(oildata, marker='o', color='black')
plt.plot(fit1.fittedvalues, marker='o', color='blue')
line1, = plt.plot(fcast1, marker='o', color='blue')
plt.plot(fit2.fittedvalues, marker='o', color='red')
line2, = plt.plot(fcast2, marker='o', color='red')
plt.plot(fit3.fittedvalues, marker='o', color='green')
line3, = plt.plot(fcast3, marker='o', color='green')
plt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])
```




    <matplotlib.legend.Legend at 0x7f5432e825c0>




![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_72_1.png)



```python
########例题9.4是一个Holt-Winters季节性预测模型,这是一个它是一种三次指数平滑预测，
########其背后的理念就是除了水平和趋势外，还将指数平滑应用到季节分量上
############下面我们使用JetRail高铁的乘客数量(这个季节性比较明显的数据）进行Holt-Winters季节性预测模型建模
```


```python
###########创建数据集合并可视化一下
import pandas as pd
import matplotlib.pyplot as plt
 
# Subsetting the dataset
# Index 11856 marks the end of year 2013
df = pd.read_csv('/home/yinaihua/Desktop/时间序列/期末作业/chapter-07/file/train.csv', nrows=11856)
 
# Creating train and test set
# Index 10392 marks the end of October 2013
train = df[0:10392]
test = df[10392:]
 
# 以天为单位生成数据集
df['Timestamp'] = pd.to_datetime(df['Datetime'], format='%d-%m-%Y %H:%M')
df.index = df['Timestamp']
df = df.resample('D').mean()
 
train['Timestamp'] = pd.to_datetime(train['Datetime'], format='%d-%m-%Y %H:%M')
train.index = train['Timestamp']
train = train.resample('D').mean()
 
test['Timestamp'] = pd.to_datetime(test['Datetime'], format='%d-%m-%Y %H:%M')
test.index = test['Timestamp']
test = test.resample('D').mean()
 
#Plotting data
train.Count.plot(figsize=(15,8), title= 'Daily Ridership', fontsize=14)
test.Count.plot(figsize=(15,8), title= 'Daily Ridership', fontsize=14)
plt.show()
```


![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_74_0.png)



```python
from statsmodels.tsa.api import ExponentialSmoothing
 
y_hat_avg = test.copy()
fit1 = ExponentialSmoothing(np.asarray(train['Count']), 
                            seasonal_periods=7, 
                            trend='add', 
                            seasonal='add', ).fit()
                        ####seasonal_periods=7作为每周重复的数据
y_hat_avg['Holt_Winter'] = fit1.forecast(len(test))
plt.figure(figsize=(16, 8))
plt.plot(train['Count'], label='Train')
plt.plot(test['Count'], label='Test')
plt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')
plt.legend(loc='best')
plt.show()
```


![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_75_0.png)



```python
#######测试预测结果（使用均方误差）
from sklearn.metrics import mean_squared_error
from math import sqrt
 
rms = sqrt(mean_squared_error(test['Count'], y_hat_avg['Holt_Winter']))
print(rms)
######我们看出，季节性的预测度较高
```

    23.961492566159794

# 第10章 波动性和广义自回归条件异方差过程

## 1.波动率的特征

波动率(volatility)指的是资产价格的波动强弱程度， 类似于概率论中随机变量标准差的概念。 波动率不能直接观测， 可以从资产收益率中看出波动率的一些特征。

- 存在波动率聚集(volatility clustering)
- 波动率随时间连续变化，一般不出现波动率的跳跃式变动
- 波动率一般在固定的范围内变化，意味着动态的波动率是平稳的
- 在资产价格大幅上扬和大幅下跌两种情形下， 波动率的反映不同， 大幅下跌时波动率一般也更大， 这种现象称为杠杆效应（leverage effect）

这些性质对波动率模型的提出、改进有重要意义， 许多新的波动率模型都是诊断原有模型不能反映上面的某型特征而提出的。 例如， EGARCH模型和TGARCH模型可以反映出波动率在价格上扬和下跌时的不对称性。

不同的波动率计算方法使用不同的数据源。例如， 对IBM股票有如下三种数据源：

- 每个交易日的日收益率；
- 伴随IBM股票的期权数据
- 盘中交易和报价的分笔数据；

分别可以计算如下三种不同的波动率：

- 作为日收益率的条件标准差（或条件方差），建模计算。 本章的模型是针对这种波动率定义。
- 隐含波动率：根据期权的理论公式如BS公式， 从股票价格和期权价格数据反解出模型中的波动率， 这样的得到的波动率称为隐含波动率。 隐含波动率倾向于比用日收益率建模得到的波动率数值要大。 CBOE的VIX指数就是隐含波动率。
- 实际波动率：利用一天内所有的收益率数据， 如每5分钟的收益率，估计一天收益率的条件标准差。

类似于利率， 度量波动率的时间区间一般也取为一年， 波动率一般是年化波动率。 如果有了日收益率（条件标准差）， 可以将其乘以
$$
\sqrt{252}
$$
转换成年化的波动率。

### 2.波动率模型的结构

波动率是收益率的条件标准差。 设r_t是某种资产在时刻t的基于某时间单位（如天、月、年）的对数收益率， 一般认为\{ r_t \}序列是前后不相关的， 或者是低阶相关的， 表现为其ACF基本都在零上下的界限内波动， 或者仅前一两个略超出界限。 但是，\{ r_t \}序列一般也不是前后独立的。

一元波动率模型就是试图刻画收益率这种本身不相关或低阶自相关， 但是不独立的性质。 用F_{t-1}表示截止到t-1时刻的收益率信息， 尤其是包括这些收益率的线性组合， 考察r_t在F_{t-1}条件下的条件均值和条件方差：
$$
\begin{align} \mu_t = E(r_t | F_{t-1}), \quad \sigma_t^2 = \text{Var}(r_t | F_{t-1}) = E[(r_t - \mu_t)^2 | F_{t-1}] \tag{16.1} \end{align}
$$
通过分析实例的经验可知， [(16.1)]中\{r_t \}通常比较简单， 如平稳ARMA(p,q)序列。 例如， 对Intel股票价格对数收益率序列\{r_t \}， 可设
$$
r_t = \mu_t + a_t
$$
, 其中
$$
\mu_t = \mu
$$
为常数， a_t为不相关的白噪声列。

对一般的对数收益率\{r_t \}， 设其服从ARMA(p,q)模型：
$$
r_t = \phi_0 + \sum_{j=1}^p \phi_j r_{t-j} + a_t + \sum_{j=1}^q \theta_j a_{t-j}
$$
其中\{a_t \}为不相关的白噪声列 。 于是
$$
\begin{align} \mu_t = E(r_t | F_{t-1}) = \phi_0 + \sum_{j=1}^p \phi_j r_{t-j} + \sum_{j=1}^q \theta_j a_{t-j} = r_t - a_t \tag{16.2} \end{align}
$$
这里我们对白噪声列仍假定
$$
E(a_t | F_{t-1})=0
$$
， 这个条件比不相关零均值白噪声列的条件要强一些。 r_t可分解为
$$
r_t = \mu_t + a_t
$$


如果可以获得其他的解释变量（外生变量）， 可以建立模型r_t = \mu_t + a_t，其中
$$
\begin{align} \mu_t = \phi_0 + \sum_{i=1}^k \beta_i x_{i, t-1} + \sum_{j=1}^p \phi_j y_{t-j} + \sum_{j=1}^q \theta_j a_{t-j} \tag{16.3} \end{align}
$$
其中x_{i,t-1}是第i个解释变量在t-1时刻的值， y_{t-j}是剔除解释变量影响后的r_{t-j}的值。
$$
\mu_t
$$
服从的ARMA(p,q)的阶与数据的采样频率有关， 股票指数的日频数据往往有较小的前后相关性， 月度数据则可能没有任何显著的前后相关。

模型[(16.3)]中的自变量比较灵活， 例如， 可以取日期星期一哑变量为自变量， 考察所谓的周末效应。 在资产定价模型(Capital Asset Pricing Model, CAPM)中， r_t的的方程可以写成
$$
r_t = \phi_0 + \beta r_{m,t} + a_t
$$
其中r_{m,t}是市场收益率，一般用综合指数收益率代替。



综合[(16.2)和[(16.3)]， 都有
$$
\begin{align} \sigma_t^2 = \text{Var}(r_t | F_{t-1}) = \text{Var}(a_t | F_{t-1}) = E(a_t^2 | F_{t-1}) \end{align}
$$
这里的\sigma_t就是波动率， 是收益率的条件标准差。 如果假设模型中的白噪声\{ a_t \}是独立序列， 则
$$
\sigma_t^2 \equiv \sigma^2
$$
， 波动率就没有建模的可能。 实际上， 假定\{a_t \}是零均值不相关的白噪声， 满足E (a_t | F_{t-1}) = 0， 但不是独立序列。



本章的问题就是对\sigma_t^2建模， 这种模型叫做条件异方差模型。 条件异方差模型分为两类：

- 用确定函数来刻画\sigma_t^2的变化，ARCH和GARCH模型属于这一类；
- 用随机方程描述\sigma_t^2的变化，随机波动率(RV)模型属于这一类。

将收益率r_t分解为
$$
r_t = \mu_t + a_t
$$
后，
$$
a_t = r_t - E(r_t | F_{t-1})
$$
， 称\{ a_t \}为资产收益率在t时刻的**扰动**或**新息**。 和[(16.3)]中的\mu_t 的模型称为r_t的**均值方程**， \sigma_t^2的模型称为r_t的**波动率方程**。 条件异方差模型就是在原来对r_t的均值\mu_t建模的基础上， 再增加一个描述资产收益率的条件方差随时间变化的模型。

### 3.波动率的建立

对资产收益率序列建立波动率模型需要如下四个步骤：

1. 通过检验序列的自相关性建立均值的方程， 必要时还可以引入适当的解释变量；
2. 对均值方程的残差作白噪声检验， 通过后，对残差检验ARCH效应；
3. 如果ARCH效应检验结果显著， 则指定一个波动率模型， 对均值方程和波动率方程进行联合估计；
4. 对得到的模型进行验证， 需要时做改进。

关于均值方程， 资产收益率一般没有自相关（注意，这并不是独立）或者仅有弱的自相关。 如果样本均值显著不等于零， 需要从数据中减去样本均值， 这称为**中心化**。 对某些日收益率或更高频的序列可能需要建立简单的AR模型。 某些情况下可以加入额外的解释变量或者与日期有关的解释变量， 比如反映周末的哑变量， 反映一月份的哑变量，等等。

例如，在Intel股票月对数收益率的均值方程建模时， 其均值方程为一个常数。

### 4.GARCH模型

#### 模型方程

(Bollerslev)提出了ARCH模型的一种重要推广模型， 称为GARCH模型。 对于一个对数收益率序列r_t， 令a_t = r_t - \mu_t = r_t - E(r_t | F_{t-1})为其新息序列， 称\{ a_t \}服从GARCH(m,s)模型， 如果a_t满足
$$
\begin{align}
a_t = \sigma_t \varepsilon_t,
\quad
\sigma_t^2 = \alpha_0 + \sum_{i=1}^m \alpha_i a_{t-i}^2 + \sum_{j=1}^s \beta_j \sigma_{t-j}^2
\tag{18.1}
\end{align}
$$
其中
$$
\{ \varepsilon_t \}
$$
为零均值单位方差的独立同分布白噪声列，
$$
\alpha_0>0, \alpha_i \geq 0, \beta_j \geq 0, 0 < \sum_{i=1}^m \alpha_i + \sum_{j=1}^s \beta_j < 1
$$
， 这最后一个条件用来保证满足模型的a_t的无条件方差有限且不变， 而条件方差
$$
\sigma_t^2
$$
可以随时间t而变。

### 与ARMA模型比较

模型[(18.1)]像是ARMA(p,q)模型， 但是这里的
$$
\sigma_{t-i}^2和a_{t-i}^2
$$
是有关系的，
$$
\sigma_{t-i}^2
$$
是a_{t-i}的条件方差， ARMA模型中的x_{t-i}与
$$
\varepsilon_{t-i}
$$
并没有这样的关系。

为了利用GARCH模型与ARMA模型的相似性， 令
$$
\alpha_i=0
$$
, 当i>m； 令
$$
\beta_j = 0,
$$
 当j>s。 令
$$
\eta_t = a_t^2 - \sigma_t^2
$$
， 下一部分证明了E a_t = 0， 而
$$
\sigma_t^2 = \text{Var}(a_t | F_{t-1}) = E(a_t^2 | F_{t-1})
$$
, 当a_t为严平稳列时\eta_t是鞅差序列，这是比宽白噪声严一些， 比零均值独立同分布白噪声宽一些的条件。 将
$$
\sigma_{t-i}^2 = a_{t-i}^2 - \eta_{t-i}
$$
代入模型[(18.1)]得
$$
\begin{align} a_t^2 = \alpha_0 + \sum_{i=1}^{\max(m,s)} (\alpha_i + \beta_i) a_{t-i}^2 + \eta_t - \sum_{j=1}^s \beta_j \eta_{t-j} \tag{18.2} \end{align}
$$
这就是关于
$$
\{a_t^2\}
$$
的
$$
ARMA(\max(m,s), s)
$$
模型， 由ARMA模型的无条件期望的公式得
$$
E a_t^2 = \frac{\alpha_0}{1 - \sum_{i=1}^{\max(m,s)} (\alpha_i + \beta_i)} = \frac{\alpha_0}{1 - \sum_{i=1}^m \alpha_i - \sum_{j=1}^s \beta_j}
$$
这要求分母为正，即要求
$$
\sum_{i=1}^m \alpha_i + \sum_{j=1}^s \beta_j < 1
$$
。 这时a_t的无条件方差
$$
\text{Var}(a_t)
$$
也等于上式。



###  GARCH模型的性质

下面以最简单的GARCH(1,1)为例研究GARCH模型的性质。 令F_{t-1}表示截止到t-1时刻的a_{t-i}和\sigma_{t-j}所包含的信息。 模型为
$$
\begin{align} a_t =& \sigma_t \varepsilon_t, \quad \varepsilon_t \text{ i.i.d. WN} (0,1)\\ \sigma_t^2 =& \alpha_0 + \alpha_1 a_{t-1}^2 + \beta_1 \sigma_{t-1}^2 \tag{18.3} \end{align}
$$


为了计算无条件均值Ea_t，先计算条件期望
$$
E(a_t | F_{t-1}) = E(\sigma_t \varepsilon_t | F_{t-1}) = \sigma_t E(\varepsilon_t | F_{t-1}) = 0
$$
这里用了
$$
\sigma_t \in F_{t-1}
$$
而
$$
\varepsilon_t
$$
与F_{t-1}独立。 于是
$$
E a_t = E[ E(a_t | F_{t-1})] = 0
$$
即GARCH模型的新息a_t的无条件期望为零。



来计算a_t的无条件方差。 设模型[(18.1)]的\{ a_t \}序列存在严平稳解，则
$$
\begin{aligned} \text{Var}(a_t) =& E(a_t^2) = E[ E(a_t^2 | F_{t-1})] = E[ E(\sigma_t^2 \varepsilon_t^2 | F_{t-1})]\\ =& E[\sigma_t^2 E(\varepsilon_t^2 | F_{t-1})] = E[\sigma_t^2 E(\varepsilon_t^2)] \\ =& E[\sigma_t^2]  = E[\alpha_0 + \alpha_1 a_{t-1}^2 + \beta_1 \sigma_{t-1}^2] \\ =& \alpha_0 + \alpha_1 E(a_{t-1}^2) + \beta_1 E[E(a_{t-1}^2|F_{t-2})]\\ =& \alpha_0 + (\alpha_1 + \beta_1) E(a_{t-1}^2) \end{aligned}
$$
令
$$
E a_t^2 = E a_{t-1}^2
$$
， 解得
$$
\text{Var}(a_t) = E a_t^2 = \frac{\alpha_0}{1 - \alpha_1 - \beta_1} .
$$


GARCH(1,1)模型的性质：

第一，像ARCH模型一样，a_t存在波动率聚集， 一个较大的a_{t-1}或\sigma_{t-1}使得1步以后的条件方差变大， 从而倾向于出现较大的对数收益率。

第二，当\varepsilon_t为标准正态分布时， 在如下条件下a_t有无条件四阶矩：
$$
1 - 2 \alpha_1^2 - (\alpha_1 + \beta_1)^2 > 0
$$
这时超额峰度为
$$
\frac{E a_t^4}{(Ea_t^2)^2} - 3 = \frac{2\left[1 - (\alpha_1 + \beta_1)^2 + \alpha_1^2 \right]} {1 - (\alpha_1 + \beta_1)^2 - 2\alpha_1^2} > 0
$$
即a_t分布厚尾。



第三，GARCH模型给出了一个比较简单的波动率模型。

### 预测

可以用类似ARMA预测的方法预测波动率。 仍以GARCH(1,1)为例， 由模型[(18.3)]， 基于截止到h时刻的观测作超前一步预测：
$$
\sigma_{h+1}^2 = \alpha_0 + \alpha_1 a_{h}^2 + \beta_1 \sigma_{h}^2 \in F_{h}
$$
所以
$$
\begin{align} \sigma_h^2(1) = E(\sigma_{h+1}^2 | F_{h}) = \sigma_{h+1}^2 = \alpha_0 + \alpha_1 a_{h}^2 + \beta_1 \sigma_{h}^2 . \tag{18.4} \end{align}对\sigma_{h+2}^2
$$
，利用
$$
a_t^2 = \sigma_t^2 \varepsilon_t^2，有\begin{aligned} \sigma_{h+2}^2 =& \alpha_0 + \alpha_1 a_{h+1}^2 + \beta_1 \sigma_{h+1}^2  \\ =& \alpha_0 + \alpha_1 \sigma_{h+1}^2 \varepsilon_{h+1}^2 + \beta_1 \sigma_{h+1}^2 \\ =& \alpha_0 + (\alpha_1 \varepsilon_{h+1}^2 + \beta_1) \sigma_{h+1}^2 \end{aligned}
$$
于是
$$
\sigma_h^2(2) = E(\sigma_{h+2}^2 | F_{h}) = \alpha_0 + E(\alpha_1 \varepsilon_{h+1}^2 + \beta_1 | F_h) \sigma_{h+1}^2 = \alpha_0 + (\alpha_1 + \beta_1) \sigma_h^2(1)
$$
类似地，对
$$
\ell \geq 2
$$
有
$$
\sigma_{h+\ell}^2 = \alpha_0 + \alpha_1 \varepsilon_{h+\ell-1}^2 \sigma_{h+\ell-1}^2 + \beta_1 \sigma_{h+\ell-1}^2 = \alpha_0 + (\alpha_1 \varepsilon_{h+\ell-1}^2 + \beta_1) \sigma_{h+\ell-1}^2
$$
于是
$$
\begin{align} \sigma_h^2(\ell) =& E\left\{ \sigma_{h+\ell}^2 | F_h \right\}  = \alpha_0  + E\left\{ (\alpha_1 \varepsilon_{h+\ell-1}^2 + \beta_1) \sigma_{h+\ell-1}^2 | F_h \right\} \\ =& \alpha_0  + E\left\{ E\left[  (\alpha_1 \varepsilon_{h+\ell-1}^2 + \beta_1) \sigma_{h+\ell-1}^2 | F_{h+\ell-2}  \right] | F_h  \right\} \\ =& \alpha_0  + E\left\{ \sigma_{h+\ell-1}^2 E\left[  \alpha_1 \varepsilon_{h+\ell-1}^2 + \beta_1 | F_{h+\ell-2} \right] | F_h \right\}  \quad(\text{注意}\sigma_{h+\ell-1}^2 \in F_{h+\ell-2}) \\ =& \alpha_0  + \left\{ \sigma_{h+\ell-1}^2 (\alpha_1 + \beta_1) | F_h \right\}  \\ =& \alpha_0 + (\alpha_1 + \beta_1) \sigma_h^2(\ell-1) \tag{18.5} \end{align}
$$


预测公式与自回归系数为(\alpha_1 + \beta_1)的ARMA(1,1)的超前预测公式相同。

从
$$
\ell=2
$$
迭代计算得
$$
\sigma_h^2(\ell) = \frac{\alpha_0[1 - (\alpha_1 + \beta_1)^{\ell-1}]}{1 - (\alpha_1 + \beta_1)} + (\alpha_1 + \beta_1)^{(\ell-1)} \sigma_h^2(1)
$$
只要
$$
\alpha_1 + \beta_1 < 1
$$
就有
$$
\sigma_h^2(\ell) \to \frac{\alpha_0}{1 - \alpha_1 - \beta_1}  = \text{Var}(a_t
$$
)即超前多步条件方差预测趋于a_t的无条件方差。



GARCH模型有和ARCH模型类似的弱点： 在高频数据研究发现即使使用t分布， 分布厚尾性也不足；

​																		 对于收益率的正负不对称性无法反映。

### 模型估计

ARCH模型的建模步骤也适用于GARCH模型的建模。 GARCH模型的定阶方法研究不多， 一般用试错法尝试较低阶的GARCH模型， 如GARCH(1,1), GARCH(2,1), GARCH(1,2)等。 许多情况下GARCH(1,1)就能解决问题。

为了估计参数， 可以假定初始的\sigma_t^2已知， 递推计算后续的\sigma_t^2并计算条件似然函数， 求条件似然函数的最大值点得到参数估计。 有时用a_t的样本方差作为初始的\sigma_t的值。

为了检验模型的充分性， 可以计算标准化残差
$$
\tilde a_t = \frac{a_t}{\sigma_t}
$$
通过对
$$
\tilde a_t
$$
和
$$
\tilde a_t^2
$$
的白噪声检验确认模型可以接受。



```python
###########例题10.1 美元汇率的GARCH模型
###############下面，我试着用USB的收盘价涨跌幅数据实现一下GARCH模型
df=Close[1:3000]
data=(df/df.shift(1)-1).dropna()
data.name='USB收盘价涨跌幅'
plt.plot(data,'r-')
```




    [<matplotlib.lines.Line2D at 0x7f32c2eb5748>]




![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_77_1.png)



```python
####注：由于jupyter notebook调用arch包失败，所以，下面的代码直接在python console中运行即可（提前配置好环境）
from arch import arch_model
garch=arch_model(y=data,mean='Constant',lags=0,vol='GARCH',p=1,o=0,q=1,dist='normal')
garchmodel=garch.fit()
garchmodel.params
garchmodel.plot()
#########可以看到GARCH(1,1)模型的参数ω=0.00000298，α=0.1，β=0.88，
# 因此模型表达式为σn2=0.00000298+0.1(un-1)2+0.88(σn-1)2

```

![GARCH_1](AppliedTimeSeries_yinaihua_files/GARCH_1.png)


```python
############下面计算长期波动率，即√VL
vol=np.sqrt(garchmodel.params[1]/(1-garchmodel.params[2]-garchmodel.params[3]))
#vol
#Out[10]: 0.012212037954677738
```


```python
#######例题10.2  使用10.1的模型进行预测
u30=data[-30:]
u30=np.matrix(u30)
vol302=np.zeros(30)
vol302[0]=data[-34:-29].std()#5日MA估计11月19号的波动率
for i in range(29):
    vol302[i+1]=np.sqrt(0.00000298+0.1*u30[0,i]**2+0.88*vol302[i]**2)
####计算日期
plt.plot(vol302,label='GARCH(1,1)')
##########图片见GARCH_2
```

![GARCH_2](AppliedTimeSeries_yinaihua_files/GARCH_2.png)



# 第11章 非线性随机过程

## 1.随机游走

考虑\{ p_t \}的模型


$$
\begin{align} p_t = p_{t-1} + \varepsilon_t, \ t=1,2,\dots \tag{7.1} \end{align}
$$


其中\{ \varepsilon_t \}是零均值独立同分布白噪声列。 称\{ p_t \}是一个**随机游动**(random walk)。

如果p_t是股票的对数价格， p_0是初始上市(initial public offering)的对数价格(即IPO对数价格)。 若\varepsilon_t的分布关于0对称， 则给定p_{t-1}的条件下预测p_t， 上升与下降的概率均为\frac12，无法预测升降。

[(7.1)](表面上看起来像是AR(1)模型， 但是\phi_1 = 1不满足AR(1)的平稳性条件，而且

$$
\begin{align} p_t = p_0 + \sum_{j=0}^{t-1} \varepsilon_{t-j} \tag{7.2} \end{align}
$$


于是
$$
E(p_t | p_0) = p_0, \quad \text{Var}(p_t | p_0) = \sigma^2 t
$$
所以\{ p_t \}非平稳。 从[(7.2)](， 每个\varepsilon_{t-j}对p_t的影响的权重都等于1， 而对于AR(1)模型， \varepsilon_{t-j}对p_t的影响的权重等于|\phi_1|^j， 随距离j的增大按负指数速度衰减。 称[(7.2)]中的\varepsilon_{t-j}对p_t是永久影响的。

随机游动的水平不可预测， 多步预测没有均值反转性质。易见
$$
\hat p_h(1) = E(p_{h} + \varepsilon_{h+1} | \mathscr F_{h}) = p_{h}
$$
其中\mathscr F_t代表截止到时刻t位置的所有观测信息， 定义是包含p_t, p_{t-1}, \dots的\sigma代数。

又
$$
\hat p_h(2) = E(p_h + \varepsilon_{h+1} + \varepsilon_{h+2} | \mathscr F_{h}) = p_h
$$
可见\hat p_h(k)=p_h, k=1,2,\dots。 所以随机游动只能用最后一个观测的水平不经衰减地预测， 没有均值反转。 而AR的预测则是均值与最近一个或几个观测的加权平均， 超前多步预测有均值反转。



\hat p_h(k)的预测均方误差为
$$
E(e_h(k))^2 = E(p_{h+k} - \hat p_h(k))^2 = E(\sum_{j=1}^k \varepsilon_{h+j})^2 = k \sigma^2 \to \infty \ (k\to\infty)
$$
这也说明了此模型不可预测。



从[(7.2)]也有
$$
\text{Var}(p_t | p_0) \to \infty(t\to\infty)
$$
。 这意味着对数价格可以取到接近正负无穷值， 对于个股有合理性， 但是对于综合股指取负无穷则不太合理。

单位根过程的ACF估计是不相合的， 对单位根过程的样本作ACF图， 其衰减速度很慢很慢。

设p_0=0， 单位根过程\{p_t\}有如下特点：

- p_t期望值等于0；
- p_t方差等于\sigma^2 t，随t线性增长，趋于无穷；
- 历史的扰动（新息）的影响不衰减；
- 预测只能用最后一个观测值作为预测， 预测均方误差趋于无穷。
- 样本ACF表现为基本不衰减，近似等于1。

## 2 带漂移的随机游动

上面的随机游动模型的金融意义一般p_t是对数价格， 则\varepsilon_t是零均值的对数收益率。 实际的对数收益率常常是非零的，正数居多。 所以，模型可以推广为


$$
p_t = \mu + p_{t-1} + \varepsilon_t, \ t=1,2,\dots
$$
其中\{ \varepsilon_t \}仍为零均值独立同分布白噪声列。 常数\mu并不代表均值， 而是对数价格p_t的增长速度，称为模型的**漂移**(drift)。 设初始价格为p_0，则
$$
\begin{aligned} p_1 =& p_0 + \mu +  \varepsilon_1 \\ p_2 =& p_0 + 2\mu + \varepsilon_1 + \varepsilon_2 \\ & \cdots\cdots \\ p_t =& p_0 + t\mu + \varepsilon_1 + \dots + \varepsilon_t \end{aligned}于是E(p_t | p_0) = p_0 + \mu t, \quad \text{Var}(p_t | p_0) = \sigma^2 t
$$
所以带漂移的随机游动与不带漂移的随机游动相比， 其条件方差不变， 但是条件均值多了一个随t线性增长（若\mu>0）的\mu t项。



这时，实际的序列的图形将沿着y = p_0 + \mu t这条直线附近变化。 如果\mu=0，则图形也有缓慢的水平变化但是没有这样的固定趋势。

带漂移的随机游动p_t， 可以分解为两部分：
$$
p_t = (p_0 + \mu t) + p_t^*
$$
其中
$$
p_t^* = \sum_{j=1}^t \varepsilon_t
$$
是从0出发的不带漂移的随机游动， p_0 + \mu t是一个非随机的线性趋势。

## 3.非线性模型

## 非线性模型的含义

如果解释变量X的单位变动引起因变量的变化率beta(即斜率)是一个常数。则回归模型是一种(解释)变量线性模型。相反，如果斜率不能保持不变，则回归模型就是一种(解释)变量非线性模型。

非线性模型的一般形式是：

![image-20200812184330100](AppliedTimeSeries_yinaihua_files/image-20200812184330100.png)

式中，Y是被解释变量；

![image-20200812184422747](AppliedTimeSeries_yinaihua_files/image-20200812184422747.png)

是**解释变量**；

![image-20200812184444637](AppliedTimeSeries_yinaihua_files/image-20200812184444637.png)

是**模型参数**；

ui为扰动项；

![image-20200812184531622](AppliedTimeSeries_yinaihua_files/image-20200812184531622.png)

是非线性函数。式中解释变量的个数k与参数个数j不一定相等 。

## 变量非线性模型

Y与X之间不存在[线性关系】，但Y与参数beta之间存在线性关系。例如：

![image-20200812184630127](AppliedTimeSeries_yinaihua_files/image-20200812184630127.png)

## 参数非线性模型

Y与X之间存在线性关系，但是Y和参数beta之间不存在线性关系。例如，下面的模型是一个参数非线性模型，因为beta1以平方的形式出现。

![image-20200812184729113](AppliedTimeSeries_yinaihua_files/image-20200812184729113.png)

对于非线性回归分析，只有参数的线性回归分析才是重要的，因为变量的非线性可通过适当的重新定义来解决 [1] 。

## 非线性模型的几种情况

(1)Y与解释变量不存在线。Y与未知参数也不存在线性关系，但可以通过适当的变换将其化为标准的线性叫归模型。

(2)Y与X不存在线性关系，Y与未知参数也不存在线性关系，而且也不能通过适当的变换将其化为标准的线性回归模型。

## 4.双线性模型

称随机序列{xi}

 服从双线性模型，如果

![image-20200812184818034](AppliedTimeSeries_yinaihua_files/image-20200812184818034.png)

其中ai为i,i,d

随机序列，

![image-20200812184923446](AppliedTimeSeries_yinaihua_files/image-20200812184923446.png)

 。当

![image-20200812184942183](AppliedTimeSeries_yinaihua_files/image-20200812184942183.png)

 ，则(1)式成为ARMA(p,q) 模型，因此双线性模型是线性模型的直接推广。

双线性模型最早来自经济学上的问题。在经济领域有着广阔的背景。

## 5.神经网络

> 在机器学习和认知科学领域，人工神经网络（artificial neural network，缩写ANN），简称神经网络（neural network，缩写NN）或类神经网络，是一种模仿生物神经网络(动物的中枢神经系统，特别是大脑)的结构和功能的数学模型或计算模型，用于对函数进行估计或近似。神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统。现代神经网络是一种非线性统计性数据建模工具。典型的神经网络具有以下三个部分：

1. **结构 （Architecture）** 结构指定了网络中的变量和它们的拓扑关系。例如，神经网络中的变量可以是神经元连接的权重（weights）和神经元的激励值（activities of the neurons）。

2. **激励函数（Activity Rule）** 大部分神经网络模型具有一个短时间尺度的动力学规则，来定义神经元如何根据其他神经元的活动来改变自己的激励值。一般激励函数依赖于网络中的权重（即该网络的参数）。

   例：sigmod函数


​     3.学习规则（Learning Rule）**学习规则指定了网络中的权重如何随着时间推进而调整。这一般被看做是一种长时间尺度的动力学规则。一般情况下，学习规则依赖于神经元的激励值。它也可能依赖于监督者提供的

目标值和当前权重的值。

下面使用数学公式描述每一个神经元工作的方式

（1）输出x
（2）计算z=w*x
（3）输出new_x = f(z)，这里的f是一个函数，可以是sigmoid、tanh、relu等，f就是上文所说到的激励函数。


```python
##########例题11.2   黑子数据建模（黑子数据来源于datetools包）
```


```python
#######首先导入相关包
%matplotlib inline
import numpy as np
from scipy import stats
import pandas as pd
import matplotlib.pyplot as plt

import statsmodels.api as sm
from statsmodels.tsa.arima_model import ARIMA
from statsmodels.graphics.api import qqplot
```


```python
########导入数据
#print(sm.datasets.sunspots.NOTE)
dta = sm.datasets.sunspots.load_pandas().data
dta.index = pd.Index(sm.tsa.datetools.dates_from_range('1700', '2008'))
del dta["YEAR"]
dta.plot(figsize=(12,8))
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7f32da4fbba8>




![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_84_1.png)



```python
fig = plt.figure(figsize=(12,8))
ax1 = fig.add_subplot(211)
fig = sm.graphics.tsa.plot_acf(dta.values.squeeze(), lags=40, ax=ax1)
ax2 = fig.add_subplot(212)
fig = sm.graphics.tsa.plot_pacf(dta, lags=40, ax=ax2)
```


![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_85_0.png)



```python
##########使用AR（2）模型进行模型拟合
arma_mod20 = ARIMA(dta, order=(2, 0, 0)).fit()
print(arma_mod20.params)##########查看参数
```

    const                49.659343
    ar.L1.SUNACTIVITY     1.390656
    ar.L2.SUNACTIVITY    -0.688571
    dtype: float64


    /home/yinaihua/ENTER/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:171: ValueWarning: No frequency information was provided, so inferred frequency A-DEC will be used.
      % freq, ValueWarning)
    /home/yinaihua/ENTER/lib/python3.7/site-packages/scipy/signal/signaltools.py:1341: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.
      out_full[ind] += zi
    /home/yinaihua/ENTER/lib/python3.7/site-packages/scipy/signal/signaltools.py:1344: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.
      out = out_full[ind]
    /home/yinaihua/ENTER/lib/python3.7/site-packages/scipy/signal/signaltools.py:1350: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.
      zf = out_full[ind]



```python
############使用AR（3）进行模型拟合
arma_mod30 = ARIMA(dta, order=(3, 0, 0)).fit()
```

    /home/yinaihua/ENTER/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:171: ValueWarning: No frequency information was provided, so inferred frequency A-DEC will be used.
      % freq, ValueWarning)



```python
##########查看参数
print(arma_mod20.aic, arma_mod20.bic, arma_mod20.hqic)
print(arma_mod30.params)
print(arma_mod30.aic, arma_mod30.bic, arma_mod30.hqic)
```

    2622.6363380639814 2637.569703171572 2628.6067259092274
    const                49.749900
    ar.L1.SUNACTIVITY     1.300810
    ar.L2.SUNACTIVITY    -0.508093
    ar.L3.SUNACTIVITY    -0.129649
    dtype: float64
    2619.4036286966957 2638.0703350811846 2626.866613503253


```python
###########例题11.4  美元汇率的马尔可夫转换模型
##############下面我们以国民生产总值为例，编写python程序，实现马尔可夫转换模型
##############使用的函数：MarkovAutoregression
####首先，导入相关包
%matplotlib inline

import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
import requests
from io import BytesIO
from pandas_datareader.data import DataReader
from datetime import datetime
usrec = DataReader('USREC', 'fred', start=datetime(1947, 1, 1), end=datetime(2013, 4, 1)
```


```python
# 获取RGNP 数据（to replicate Hamilton）
dta = pd.read_stata('https://www.stata-press.com/data/r14/rgnp.dta').iloc[1:]
dta.index = pd.DatetimeIndex(dta.date, freq='QS')
dta_hamilton = dta.rgnp

# 绘图
dta_hamilton.plot(title='Growth rate of Real GNP', figsize=(12,3))

# 拟合模型
mod_hamilton = sm.tsa.MarkovAutoregression(dta_hamilton, k_regimes=2, order=4, switching_ar=False)
res_hamilton = mod_hamilton.fit()
```


![png](AppliedTimeSeries_yinaihua_files/AppliedTimeSeries_yinaihua_91_0.png)



```python
print(res_hamilton.summary())
```

                             Markov Switching Model Results                         
    ================================================================================
    Dep. Variable:                     rgnp   No. Observations:                  131
    Model:             MarkovAutoregression   Log Likelihood                -181.263
    Date:                  Fri, 07 Aug 2020   AIC                            380.527
    Time:                          20:19:50   BIC                            406.404
    Sample:                      04-01-1951   HQIC                           391.042
                               - 10-01-1984                                         
    Covariance Type:                 approx                                         
                                 Regime 0 parameters                              
    ==============================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
    ------------------------------------------------------------------------------
    const         -0.3588      0.265     -1.356      0.175      -0.877       0.160
                                 Regime 1 parameters                              
    ==============================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
    ------------------------------------------------------------------------------
    const          1.1635      0.075     15.614      0.000       1.017       1.310
                               Non-switching parameters                           
    ==============================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
    ------------------------------------------------------------------------------
    sigma2         0.5914      0.103      5.761      0.000       0.390       0.793
    ar.L1          0.0135      0.120      0.112      0.911      -0.222       0.249
    ar.L2         -0.0575      0.138     -0.418      0.676      -0.327       0.212
    ar.L3         -0.2470      0.107     -2.310      0.021      -0.457      -0.037
    ar.L4         -0.2129      0.111     -1.926      0.054      -0.430       0.004
                             Regime transition parameters                         
    ==============================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
    ------------------------------------------------------------------------------
    p[0->0]        0.7547      0.097      7.819      0.000       0.565       0.944
    p[1->0]        0.0959      0.038      2.542      0.011       0.022       0.170
    ==============================================================================
    
    Warnings:
    [1] Covariance matrix calculated using numerical (complex-step) differentiation.

# 第12章 传递函数和自回归分布滞后模型

## 1.单输入传递函数噪声模型

​	此模型中，内生变量（输出）与单个外生变量（输入）相关：
$$
y_t = v(B)x_t + n_t
$$
其中，滞后多项式$v(B) = v_0 + v_1B + v_2B^2 + ……$作为传递函数，允许x通过分布滞后影响y。此模型一关键假设为$x_t$与$n_t$独立，即过去的x会影响y，但y的变化不能反馈到x。

​	由于v(B)的阶数无限，我们需要加一些限制使其变得可行。于是把v(B)写为合理的滞后公式$v(B) = \frac{w(B)B^b}{\delta(B)}$，其中$w(B) = w_0 - w_1B - … - w_sB^s$，$\delta(B) = 1 - \delta_1B - … - \delta_rB^r$。若噪声遵循ARMA(p, q)模型:$n_t = \frac{\theta(B)}{\phi(B)}$，则此模型可写为$y_t = \frac{w(B)}{\delta(B)}x_(t-b) + \frac{\theta(B)}{\phi(B)}a_t$。

​	有多个输入时，模型写为
$$
y_t = \sum_{j = 1}^Mv_j(B)x_{j,t} + n_t = \sum_{j = 1}^M\frac{w_j(B)B^{b_j}}{\delta_j(B)}x_{j,t} + \frac{\theta(B)}{\phi(B)}a_t
$$

## 2.回归分布滞后模型

在上述模型基础上指定了限制模式$\delta_1(B) = … = \delta_M(B) = \phi(B)$，$\theta(B) = 1$则被称为回归分布滞后模型(ARDL)，他能够把噪声成分降低为白噪声，并用最小二乘法估计。

####################例题12.1

查得1952至2017年英国每月的长期利率与短期利率数据如下：

```python
                                          R20        RS
                                        0    4.11  0.994750
                                        1    4.26  1.028250
                                        2    4.33  2.365042
                                        3    4.23  2.317500
                                        4    4.36  2.350833
                                        ..    ...       ...
                                        778  1.95  0.140000
                                        779  2.00  0.050000
                                        780  1.99  0.140000
                                        781  1.91  0.110000
                                        782  1.81  0.020000
```

因题目要求的是长期利率变化与短期利率变化的关系，于是先通过微分求得变化率如下：

```python
                                             DR20       DRS
                                        0    0.15  0.033500
                                        1    0.07  1.336792
                                        2   -0.10 -0.047542
                                        3    0.13  0.033333
                                        4    0.21  0.101000
                                        ..    ...       ...
                                        778  0.05 -0.090000
                                        779 -0.01  0.090000
                                        780 -0.08 -0.030000
                                        781 -0.10 -0.090000
                                        782 -0.12  0.050000

```

得到数据后首先画出时序图，了解数据的大概分布。

```python
import matplotlib.pyplot as plt
import pandas as pd
#引入数据
data=pd.read_excel("data12.xlsx")
R20 = data.R20
RS = data.RS
#画图
plt.plot(R20,label="R20")
plt.plot(RS,label="RS")
plt.legend()
plt.show()
```

以下为长期利率、短期利率的时序图

<img src="https://s1.ax1x.com/2020/08/11/aL7J6H.png" style="zoom:80%;" />

```python
import matplotlib.pyplot as plt
import pandas as pd
#引入数据
data=pd.read_excel("data12.xlsx")
DR20 = data.DR20
DRS = data.DRS
#画图
plt.plot(DR20,label="DR20")
plt.plot(DRS,label="DRS")
plt.legend()
plt.show()
```

以下为长期利率、短期利率变化的时序图：

<img src="https://s1.ax1x.com/2020/08/11/aONB34.png" style="zoom:80%;" />

选择AIC

AIC是衡量统计模型拟合优良性的一种标准，又称赤池信息量准则。它建立在熵的概念基础上，可以权衡所估计模型的复杂度和此模型拟合数据的优良性。AIC越大表明模型拟合越优良，但考虑到避免过度拟合的情况，优先考虑AIC值最小的模型。

```python
from statsmodels.tsa.arima_model import ARMA
#通过AIC判断模型参数
def proper_model(data_ts, maxLag): 
 init_bic = float("inf")
 init_p = 0
 init_q = 0
 init_properModel = None
 for p in np.arange(maxLag):
 for q in np.arange(maxLag):
 model = ARMA(data_ts, order=(p, q))
 try:
 results_ARMA = model.fit(disp=-1, method='css')
 except:
 continue
 bic = results_ARMA.bic
 if bic < init_bic:
 init_p = p
 init_q = q
 init_properModel = results_ARMA
 init_bic = bic
 return init_bic, init_p, init_q, init_properModel
  
proper_model(trainSeting,40)

```

可以得到，我们应该选择ADRL(2,1)模型。

模型实现

根据此模型形式，我们可以假设为
$$
DR20_t = aDR20_{t-1} + bDR20_{t-2} + cDRS_t + dDRS_{t-1} + a_t
$$
此模型本质为多元线性回归

```python
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
#引入数据
data = pd.read_excel("data12.xlsx",usecols=[3,4,5,6])
target = pd.read_excel("data12.xlsx",usecols=[7])
X = data
y = target
#拆分训练集、预测集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=1)
#多元回归模型
lr = LinearRegression()
lr.fit(X_train, y_train)

print(lr.coef_)		#输出系数矩阵
print(lr.intercept_)    #输出常数项
```

结果：

```python
[[ 0.16897312 -0.12392131  0.24663085 -0.05758091]]
[-0.00019778]
```

因此，最终模型为
$$
DR20_t = 0.16DR20_{t-1} - 0.12DR20_{t-2} + 0.24DRS_t - 0.06DR20_{t-1}
$$

# 第13章 向量自回归和格兰杰因果关系

## 1.多元动态回归模型

## 2.向量自回归模型（VAR）

###  VAR模型

以金融价格为例，传统的时间序列模型比如ARIMA,ARIMA-GARCH等，只分析价格自身的变化，模型的形式为：
$$
y_t = \beta_1*y_{t-1} + \beta_2*y_{t-2}+…
$$
其中![y_{t-1}](https://private.codecogs.com/gif.latex?y_%7Bt-1%7D)称为自身的滞后项。

但是VAR模型除了分析自身滞后项的影响外，还分析其他相关因素的滞后项对未来值产生的影响，模型的形式为：
$$
y_t = \beta_1*y_{t-1}+\alpha_1*x_{t-1}+\beta_2*y_{t-2}+\alpha_2*x_{t-2}+…
$$
其中![x_{t-1}](https://private.codecogs.com/gif.latex?x_%7Bt-1%7D)就是其他因子的滞后项。

###  VAR模型的建模步骤

1）画N个因子的序列相关图

2）对N个因子的原始数据进行平稳性检验，也就是ADF检验

3）对应变量（yt）和影响因子（Xt）做协整检验

4）通过AIC,BIC,以及LR定阶。

5）估计参数，看参数的显著性。

6）对参数进行稳定性检验

7）使用乔里斯基正交化残差进行脉冲响应分析

8）使用乔里斯基正交化残差进行方差分解分析

VAR建模的时候以上面的条件为例，其实模型估计参数时会给出三个3个方程(应变量各自不同）：

方程1：
$$
y_t = \beta_1*y_{t-1}+\alpha_1*X1_{t-1}+\Theta_1*X2_{t-1}+\epsilon_t
$$
方程2：$X1_t = \beta_1*X1_{t-1}+\alpha_1*y_{t-1}+\Theta_1*X2_{t-1}+\eta_t$

方程3：$X2_t = \beta_1*X2_{t-1}+\alpha_1*y_{t-1}+\Theta_1*X1_{t-1}+w_t$

方程1的残差序列：$\epsilon_t$

方程2的残差序列：$\eta_t$

方差3的残差序列：$w_t$

**三个方程的乔里斯基正交化的步骤就是：**

正交1：$\frac{\eta_t}{\epsilon_t}$

正交2：$\frac{w_t}{\epsilon_t}$

正交3：$\frac{w_t}{\eta_t}$

正交4：![\frac{\frac{\eta _{t}}{\varepsilon _{t}}}{\frac{\omega _{t}}{\varepsilon _{t}}}](https://private.codecogs.com/gif.latex?%5Cfrac%7B%5Cfrac%7B%5Ceta%20_%7Bt%7D%7D%7B%5Cvarepsilon%20_%7Bt%7D%7D%7D%7B%5Cfrac%7B%5Comega%20_%7Bt%7D%7D%7B%5Cvarepsilon%20_%7Bt%7D%7D%7D)

正交5：![\frac{\frac{\eta _{t}}{\varepsilon _{t}}}{\frac{\omega _{t}}{\eta _{t}}}](https://private.codecogs.com/gif.latex?%5Cfrac%7B%5Cfrac%7B%5Ceta%20_%7Bt%7D%7D%7B%5Cvarepsilon%20_%7Bt%7D%7D%7D%7B%5Cfrac%7B%5Comega%20_%7Bt%7D%7D%7B%5Ceta%20_%7Bt%7D%7D%7D)

最后用正交4/正交5，得到的序列就是乔里斯基正交化残差了。

## 3.格兰杰因果关系检验

​	格兰杰因果检验以自回归模型为基础，能够检验一组时间序列是否为另一组时间序列的原因，但并不是指真正意义上的因果关系而是一个变量对另一个变量的依存性。其基本观念是未来的事件不会对目前与过去产生因果影响，而过去的事件才可能对现在及未来产生影响。

​	格兰杰因果关系检验假设了有关y和x每一变量的预测的信息全部包含在这些变量的时间序列之中。检验要求估计以下的回归：
$$
y_t = \sum_{i-1}^q\alpha_ix_{i-1}+\sum_{j-1}^q\beta_jy_{t-j}+u_{1,t}
$$

$$
x_t = \sum_{i-1}^s\lambda_ix_{t-i}+\sum_{j-1}^s\delta_jy_{t-j}+u_{2,t}
$$

若在包含了变量x、y的过去信息下，对y的预测效果优于单独由y过去信息对y进行的预测效果，就认为x是引致y的格兰杰关系。

​	两个变量间存在四种情况：x是引起y变化的原因、y是引起x变化的原因、x与y互为因果关系、x与y独立。

#################例题13.1 向量自回归模型的应用

*1）导入模块**

```python
# 模型相关包
import statsmodels.api as sm
import statsmodels.stats.diagnostic
# 画图包
import matplotlib.pyplot as plt
# 其他包
import pandas as pd
import numpy as np 
```

*2）画序列相关图**

```python
data = pd.read_excel("data12.xlsx",usecols=[1,2])
R20 = data.R20
RS = data.RS
fig = plt.figure(figsize=(12,8))
plt.plot(R20,'r',label='R20')
plt.plot(RS,'g',label='RS')
plt.title('Correlation: ')
plt.grid(True)
plt.axis('tight')
plt.legend(loc=0)
plt.ylabel('Price')
plt.show()
```

3）ADF单位根**

python里的ADF检验结果就是下面的adfResult，这里用output整理了一下，方便浏览。

```python
adfResult = sm.tsa.stattools.adfuller(data,maxlags)
output = pd.DataFrame(index=['Test Statistic Value', "p-value", "Lags Used", "Number of Observations Used","Critical Value(1%)", "Critical Value(5%)", "Critical Value(10%)"],
					columns=['value'])
output['value']['Test Statistic Value'] = adfResult[0]
output['value']['p-value'] = adfResult[1]
output['value']['Lags Used'] = adfResult[2]
output['value']['Number of Observations Used'] = adfResult[3]
output['value']['Critical Value(1%)'] = adfResult[4]['1%']
output['value']['Critical Value(5%)'] = adfResult[4]['5%']
output['value']['Critical Value(10%)'] = adfResult[4]['10%']
```

**4）协整检验**

python里面的协整检验通过**coint（）这个函数进行的，返回P-value值，越小，说明协整关系越强**。

```python
result = sm.tsa.stattools.coint(data1,data2)
```

5）模型估计+定阶**

**这里插入的数据只能是DATAFRAME格式。**

**数据构造：**

```python
lnDataDict = {'lnSHFEDiff':lnSHFEDiff,'lnXAUDiff':lnXAUDiff}
lnDataDictSeries = pd.DataFrame(lnDataDict,index=lnSHFEDiffIndex)
data = lnDataDictSeries[['lnSHFEDiff','lnXAUDiff']]
```

```python
#建立对象，dataframe就是前面的data，varLagNum就是你自己定的滞后阶数
orgMod = sm.tsa.VARMAX(dataframe,order=(varLagNum,0),trend='nc',exog=None)
#估计：就是模型
fitMod = orgMod.fit(maxiter=1000,disp=False)
# 打印统计结果
print(fitMod.summary())
# 获得模型残差
resid = fitMod.resid
result = {'fitMod':fitMod,'resid':resid}
```

结果：

```python
Statespace Model Results                           
==============================================================================
Dep. Variable:          ['R20', 'RS']   No. Observations:                  783
Model:                         VAR(3)   Log Likelihood                -465.577
                          + intercept   AIC                            965.154
Date:                Wed, 12 Aug 2020   BIC                           1044.428
Time:                        19:55:24   HQIC                           995.638
Sample:                             0                                         
                                - 783                                         
Covariance Type:                  opg                                         
===================================================================================
Ljung-Box (Q):                70.78, 69.44   Jarque-Bera (JB):      418.35, 1533.78
Prob(Q):                        0.00, 0.00   Prob(JB):                   0.00, 0.00
Heteroskedasticity (H):         1.10, 0.27   Skew:                      -0.17, 1.08
Prob(H) (two-sided):            0.45, 0.00   Kurtosis:                   6.57, 9.51
                           Results for equation R20                           
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
intercept      0.0221      0.031      0.705      0.481      -0.039       0.084
L1.R20         1.3087      0.030     43.396      0.000       1.250       1.368
L1.RS         -0.0098      0.022     -0.454      0.650      -0.052       0.033
L2.R20        -0.4517      0.046     -9.716      0.000      -0.543      -0.361
L2.RS          0.0337      0.034      0.984      0.325      -0.033       0.101
L3.R20         0.1360      0.029      4.665      0.000       0.079       0.193
L3.RS         -0.0200      0.023     -0.869      0.385      -0.065       0.025
                           Results for equation RS                            
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
intercept     -0.0145      0.046     -0.313      0.755      -0.106       0.077
L1.R20         0.2979      0.056      5.281      0.000       0.187       0.409
L1.RS          1.1982      0.029     41.048      0.000       1.141       1.255
L2.R20        -0.3507      0.094     -3.750      0.000      -0.534      -0.167
L2.RS         -0.1869      0.053     -3.555      0.000      -0.290      -0.084
L3.R20         0.0772      0.063      1.226      0.220      -0.046       0.200
L3.RS         -0.0390      0.040     -0.977      0.329      -0.117       0.039
                              Error covariance matrix                              
===================================================================================
                      coef    std err          z      P>|z|      [0.025      0.975]
-----------------------------------------------------------------------------------
sqrt.var.R20        0.2783      0.004     63.552      0.000       0.270       0.287
sqrt.cov.R20.RS     0.1949      0.012     16.930      0.000       0.172       0.217
sqrt.var.RS         0.3792      0.006     67.425      0.000       0.368       0.390
===================================================================================

```

6）系数平稳检验：CUSUM检验**

这里的resid就是前面模型的resid

```python
# 原假设：无漂移（平稳），备择假设：有漂移（不平稳）
result = statsmodels.stats.diagnostic.breaks_cusumolsresid(resid)
```

7）脉冲响应图**

**orthogonalized=True代表使用乔里斯基正交。terms代表周期数。**

```python
# orthogonalized=True，代表采用乔里斯基正交 
ax = fitMod.impulse_responses(terms, orthogonalized=True).plot(figsize=(12, 8))
plt.show()
```

8）方差分解图**

这里就用VAR重新估计，然后直接使用**fevd进行方差分解**。

打印summary（）可以看到方差分解的具体结果，plot可以画图

```python
md = sm.tsa.VAR(dataFrame)
re = md.fit(2)
fevd = re.fevd(10)
# 打印出方差分解的结果
print(fevd.summary())
# 画图
fevd.plot(figsize=(12, 16))
plt.show()
```

 ## 4.结构向量自回归模型（SVAR）



​		结构向量自回归模型（SVAR) 可以捕捉模型系统内各个变量之间的即时的结构性关系。而如果仅仅建立一个VAR模型，这样的结构关联性却被转移到了随机扰动向量的方差-协方差矩阵中了。也正是基于这个原因，VAR模型实质上是一个缩减形式，没有明确体现变量间的结构性关系。

​		一个结构向量自回归模型可以写成为：
$$
B_0y_t=c_0+B_1y_1+B_2y_{t-2}+...+B_py_{t-p}+e_t
$$
其中：c0是n×1常数向量，Bi是n×n矩阵，et是n×1误差向量。

​		一个有两个变量的结构VAR(1)可以表示为
$$
\begin{pmatrix}
  1 & B_{0;1,2}\\
  B_{0;2,1} & 1 
 \end{pmatrix}
 \begin{pmatrix}
  y_{1,t} \\
  y_{2,t} 
 \end{pmatrix} = 
 \begin{pmatrix}
  c_{0;1} \\
  c_{0;2} 
 \end{pmatrix} +
\begin{pmatrix}
  B_{1;1,1} & B_{1;1,2}\\
  B_{1;2,1} & B_{1;2,2} 
 \end{pmatrix}+
 \begin{pmatrix}
  e_{1,t} \\
  e_{2,t} 
 \end{pmatrix}
$$
其中：
$$
\sum = E(e_te'_t)=\begin{pmatrix}
  \sigma_1 &0\\
  0 &\sigma_2
 \end{pmatrix}
$$
​		在一定的经济理论基础上的计量经济模型如果已经对各种冲击进行了显性的识别，那么这些模型通常可以变换为VAR或SVAR模型，VAR或SVAR模型是这些模型的简化式。但是有这些模型经过变换得到的VAR模型与一般的VAR模型并不完全相同，变现为两方面：

​		首先，这些模型经过变换得到的VAR模型是一种带有约束的VAR模型,我们可以通过约束检验和似然函数比例方法进行进一步检验来比较这两种模型。

​		其次，这些模型经过变换得到的VAR模型比一般的VAR模型有优越性的地方，但也有不足之处。通常这些模型对冲击进行了显性的识别，因而我们不需要进行冲击识别的过程，而一般的VAR模型所包含的冲击更为广泛，只有施加适当的识别条件，才能得到人们感兴趣的冲击，所以二者通常不能完全相互取代。

​		因此，要使这两种模型都避免Lucas批判(即当经济环境、政策体制、预期等发生变化导致深层次参数发生变化时，可能会导致模型中估计参数的变化及行为方程的不稳定,这将对政策分析和评价造成很大影响)，我们需要对这两种模型进行有关的外生性检验。

# 第14章 误差校正、伪回归和协整

## 1.误差校正模型（ECM）

​	上面讲到的模型都是在稳定时间序列基础上进行的，而对于非稳定时间序列，需要通过差分法先将其化为稳定序列再建立经典的回归模型。由于此方法会引起以下问题，因此需要建立误差修正模型修正偏差。

(1)若x与y是协整的(存在长期稳定的均衡关系):$y_t = \alpha_0 + \alpha_1x_t + \mu_t$且误差项不存在序列相关，则差分项$\Delta y_t = \alpha_1\Delta x_t + v_t$，是序列相关的。

(2)采用差分形式进行估计，只能反映x与y的短期关系而忽略了长期关系，得出的回归方程令人不够满意。

​	若x与y符合ARDL(1,1)模型，即$y_t = \beta_0 + \beta_1x_t + \beta_2x_{t-1} + \mu y_{t-1} + \epsilon_t$，由于变量可能是非平稳的，不能直接用最小二乘法，经过差分并适当变形后得：
$$
\Delta y_t = \beta_1\Delta x_t - \lambda(y_{t-1} - \alpha_0 - \alpha_1 x_{t-1}) + \epsilon_t
$$
其中，$\lambda = 1 - \mu$，$\alpha_0 = \beta_0/(1-\mu)$，$\alpha_1 = (\beta_1 + \beta_2)/(1-\mu)$。此式中，y的变化取决于x的变化及前一时期的非均衡程度，因此y已对前期非均衡程度做出了修正。所以建立误差修正模型要先对变量进行协整分析，并以这种关系构成误差修正项。

## 2.伪回归

​	上述讲到对于非稳定序列我们需要通过差分将其变为稳定序列，其原因就是一组非平稳时间序列之间不存在协整关系时这一组变量构造的回归模型中可能出现一种“假回归”即伪回归。

​	伪回归的特点为其残差序列是非平稳的，即使这样的一种回归有可能拟合优度、显著性水平等指标都很好，但是由于残差序列是一个非平稳序列，说明了这种回归关系不能够真实的反映因变量和解释变量之间存在的均衡关系，而仅仅是一种数字上的巧合而已，利用以上的误差修正模型即是避免伪回归的一种方法。

## 3.协整关系

​	由于很多实际问题中的序列是非稳定的，对于适用于稳定序列的经典回归分析带来了很大的限制，而由差分得到的平稳序列会限制讨论的范围，协整关系为非平稳序列的建模提供了另一种途径。

​	协整关系实际上是一种均衡关系的统计表示，若所考虑的时间序列具有相同的单整阶数，而且他们的线性组合能使组合时间序列的单整阶数降低，则说明他们之间存在显著的协整关系。直观上看，则需要这些变量从长远来看应该具有均衡关系。因此，若时间序列之间存在协整关系，那么他们的线性组合是稳定的，便可以运用到经典的回归分析上。

###############例题14.1 协整关系的判断

下面我们用Johansen Test协整检验法来判断12.1例题中两个时间序列是否存在协整关系。

```python
import pandas as pd
import numpy as np
from statsmodels.tsa.stattools import coint
from statsmodels.tsa.stattools import adfuller

#引入数据
data=pd.read_excel("data12.xlsx")
DR20 = data.DR20
DRS = data.DRS

#判断协整关系
R20_diff = np.diff(DR20)
RS_diff = np.diff(DRS)
print(adfuller(R20_diff))
print(adfuller(RS_diff))
print(coint(DR20,DRS))
```

结果为

```python
(-10.753274729209782, 2.6319427522731977e-19, 21, 760, {'1%': -3.4389835846902788, '5%': -2.8653504478604757, '10%': -2.5687990737534627}, 271.76735194796606)
(-10.043618760896825, 1.4740124216548914e-17, 19, 762, {'1%': -3.4389608473398194, '5%': -2.8653404270188476, '10%': -2.568793735369693}, 944.7295822145363)
(-19.921225905393204, 0.0, array([-3.91049981, -3.34395458, -3.04987798]))
```

可以看到，p值小于0.05，因此可以判断英国长期利率与短期利率间存在协整关系。

#########################例题14.2 最小二乘法回归拟合的应用

利用英国长期利率(R20)、短期利率(RS)进行OLS(最小二乘法)回归拟合：

```python
import pandas as pd
import statsmodels.api as sm

#引入数据
data = pd.read_excel("data12.xlsx")
R20 = data.R20
RS = data.RS

X = sm.add_constant(RS)#给自变量中加入常数项
model1 = sm.OLS(R20,X).fit()
print(model1.summary())#打印拟合结果
print(model1.params)

Y = sm.add_constant(R20)
model2 = sm.OLS(RS,Y).fit()
print(model2.summary())
print(model2.params)
```

结果为

```python
                                        OLS Regression Results                            
            ==============================================================================
            Dep. Variable:                    R20   R-squared:                       0.776
            Model:                            OLS   Adj. R-squared:                  0.776
            Method:                 Least Squares   F-statistic:                     2710.
            Date:                Wed, 12 Aug 2020   Prob (F-statistic):          3.90e-256
            Time:                        20:16:11   Log-Likelihood:                -1498.6
            No. Observations:                 783   AIC:                             3001.
            Df Residuals:                     781   BIC:                             3011.
            Df Model:                           1                                         
            Covariance Type:            nonrobust                                         
            ==============================================================================
                             coef    std err          t      P>|t|      [0.025      0.975]
            ------------------------------------------------------------------------------
            const          2.4485      0.112     21.916      0.000       2.229       2.668
            RS             0.7976      0.015     52.053      0.000       0.768       0.828
            ==============================================================================
            Omnibus:                       53.403   Durbin-Watson:                   0.043
            Prob(Omnibus):                  0.000   Jarque-Bera (JB):               67.212
            Skew:                           0.602   Prob(JB):                     2.54e-15
            Kurtosis:                       3.782   Cond. No.                         14.1
            ==============================================================================

                                        OLS Regression Results                            
            ==============================================================================
            Dep. Variable:                     RS   R-squared:                       0.776
            Model:                            OLS   Adj. R-squared:                  0.776
            Method:                 Least Squares   F-statistic:                     2710.
            Date:                Wed, 12 Aug 2020   Prob (F-statistic):          3.90e-256
            Time:                        20:16:11   Log-Likelihood:                -1576.5
            No. Observations:                 783   AIC:                             3157.
            Df Residuals:                     781   BIC:                             3166.
            Df Model:                           1                                         
            Covariance Type:            nonrobust                                         
            ==============================================================================
                             coef    std err          t      P>|t|      [0.025      0.975]
            ------------------------------------------------------------------------------
            const         -0.9949      0.153     -6.514      0.000      -1.295      -0.695
            R20            0.9732      0.019     52.053      0.000       0.937       1.010
            ==============================================================================
            Omnibus:                       11.555   Durbin-Watson:                   0.050
            Prob(Omnibus):                  0.003   Jarque-Bera (JB):               13.192
            Skew:                           0.216   Prob(JB):                      0.00137
            Kurtosis:                       3.467   Cond. No.                         19.5
            ==============================================================================

```

因此，OLS模型为
$$
R20_t = 2.45 + 0.798RS_t + e_{1,t}
$$

$$
RS_t = -0.99 + 0.97R20_t + e_{2,t}
$$

##########例题14.3   自己构造两个符合协整关系的时间序列

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#构造两个时间序列
np.random.seed(100)
x = np.random.normal(0, 1, 500)
y = np.random.normal(0, 1, 500)
X = pd.Series(np.cumsum(x)) + 100
Y = X + y + 30
for i in range(500):
    X[i] = X[i] - i/10
    Y[i] = Y[i] - i/10
plt.plot(X); plt.plot(Y);
plt.xlabel("Time"); plt.ylabel("Price");
plt.legend(["X", "Y"]);
plt.show()
```

<img src="https://s1.ax1x.com/2020/08/12/ax9We1.png" style="zoom:80%;" />

​	可见我们构建的X、Y序列都是不平稳的，因为他们的均值都随着时间的变化而改变，接下来我们再来看一下这两个时间序列的差值。

```python
#两个时间序列差
plt.plot(Y-X);
plt.axhline((Y-X).mean(), color="red", linestyle="--");
plt.xlabel("Time"); plt.ylabel("Price");
plt.legend(["Y-X", "Mean"]);
plt.show()
```

<img src="https://s1.ax1x.com/2020/08/12/axC9Sg.png" style="zoom:80%;" />

可以看出均值不随时间波动，而且差值也一直在均值上下，因此这两个时间序列的线性组合是平稳的，也就是说这两个时间序列符合协整关系。

# 第15章 向量自回归模型、向量误差修正模型和共同趋势

## 1.向量自回归模型

​	前面我们讨论的仅是两个时间序列之间的关系，接下来我们推广到多元形式。向量自回归模型(VAR)是单变量自回归模型(AR)模型的推广。

​	给定多元时间序列数据$Y\in R^{N*T}$，则对于任意第t个时间间隔，存在如下表达式：
$$
y_t = \sum_{k=1}^dA_ky_{t-k} + \epsilon_t, t = d+1,……,T
$$
其中，$A_k\in R^{N*N}, k=1,2,……,d$ 表示系数矩阵，$\epsilon_t$为噪声。

## 2.向量误差修正模型

​	上述VAR模型中，$A = \sum_{k=1}^TA_k$，$\Pi = A - I_n$。而$\left|\Pi\right| = \left|A_1 + … + A_T - I_n\right| = 0$，即长期矩阵是奇异的，其秩小于n，至少包含一个单位根。设秩为r，则$0<r<n$，说明这n个变量之间存在r个协整关系。定义一个(n*r)的矩阵B，含有r列线性无关的协整向量，因此rank(B) = r。

​	其向量误差修正模型(VECM)为：
$$
\Delta y_t = c + \Phi(B)\Delta y_{t-1} + \beta e_{t-1} + u_t
$$
直接做差分结果为$\Delta y_t = c + \Phi(B)\Delta y_{t-1} + u_t$，可以看出修正模型中增加了修正项$\beta e_{t-1}$，从长期来看均衡状态精确地存在，从短期来看对于每个时刻t，误差项促使其朝着长期均值移动，减少了偏差。

##################例题15.1  向量自回归模型的代数关系

​	题目给出了VAR(2)的形式：
$$
y_{1,t} = a_{11,1}y_{1,t-1} + a_{12,1}y_{2,t-1} + a_{11,2}y_{1,t-2} + a_{12,2}y_{2,t-2} + u_{1,t}
$$

$$
y_{2,t} = a_{21,1}y_{1,t-1} + a_{22,1}y_{2,t-1} + a_{21,2}y_{1,t-2} + a_{22,2}y_{2,t-2} + u_{2,t}
$$

以及系数矩阵
$$
A_1 = \begin{bmatrix}a_{11,1}&a_{12,1}\\a_{21,1}&a_{22,1}\end{bmatrix}$，$A_2 = \begin{bmatrix}a_{11,2}&a_{12,2}\\a_{21,2}&a_{22,2}\end{bmatrix}$，$A = A_1 + A_2 = \begin{bmatrix}a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix}
$$
。

​	根据上述模型，$\Pi = A - I_n$，因此在本题中
$$
\Pi=A-I_2=\begin{bmatrix}\pi_{11}&\pi_{12}\\\pi_{21}&\pi_{22}\end{bmatrix}=\begin{bmatrix}a_{11}-1&a_{12}\\a_{21}&a_{22}-1\end{bmatrix}
$$
又由
$$
\left|\Pi\right| = \pi_{11}\pi_{22}-\pi_{12}\pi_{21} = 0
$$
，根据$\alpha'、\beta$与$\Pi$的关系，把$\pi_{21}、\pi_{22}$分别表示成
$$
(\pi_{22}/\pi_{12})\pi_{11}、(\pi_{22}/\pi_{12})\pi_{12}
$$
，则
$$
\Pi = \begin{bmatrix}\pi_{11}&\pi_{12}\\(\pi_{22}/\pi_{12})\pi_{11}&(\pi_{22}/\pi_{12})\pi_{12}\end{bmatrix}，\beta=\begin{bmatrix}\pi_{11}\\(\pi_{22}/\pi_{12})\end{bmatrix}，\alpha' = \begin{bmatrix}1&\pi_{12}/\pi_{11}\end{bmatrix}
$$
。因为p=2，所以$\Phi(B) = -1+A_1 = -A_2$。

​	根据向量误差修正模型
$$
\Delta y_t=\Phi(B)\Delta y_{t-1}+\beta\alpha'y_{t-1}+u_t
$$
，把系数带入之后得到
$$
\begin{bmatrix}\Delta y_{1,t}\\\Delta y_{2,t}\end{bmatrix}=-\begin{bmatrix}a_{11,2}&a_{12,2}\\a_{21,2}&a_{22,2}\end{bmatrix}\begin{bmatrix}\Delta y_{1,t-1}\\\Delta y_{2,t-1}\end{bmatrix}+\begin{bmatrix}\pi_{11}\\(\pi_{22}/\pi_{12})\end{bmatrix}\begin{bmatrix}1&\pi_{12}/\pi_{11}\end{bmatrix}\begin{bmatrix}y_{1,t-1}\\y_{2,t-1}\end{bmatrix}+\begin{bmatrix}u_{1,t}\\u_{2,t}\end{bmatrix}
$$

########例题15.2 格兰杰因果关系检验

​	利用前面例题中的英国长期、短期利率变化时间序列进行格兰杰因果关系检验

```python
from statsmodels.tsa.stattools import grangercausalitytests
import pandas as pd
df = pd.read_excel('data12.xlsx', usecols=[1,2])
grangercausalitytests(df, maxlag=3)
```

结果为

```python
Granger Causality
number of lags (no zero) 1
ssr based F test:         F=0.2887  , p=0.5912  , df_denom=779, df_num=1
ssr based chi2 test:   chi2=0.2898  , p=0.5903  , df=1
likelihood ratio test: chi2=0.2898  , p=0.5904  , df=1
parameter F test:         F=0.2887  , p=0.5912  , df_denom=779, df_num=1

Granger Causality
number of lags (no zero) 2
ssr based F test:         F=0.7080  , p=0.4930  , df_denom=776, df_num=2
ssr based chi2 test:   chi2=1.4251  , p=0.4904  , df=2
likelihood ratio test: chi2=1.4238  , p=0.4907  , df=2
parameter F test:         F=0.7080  , p=0.4930  , df_denom=776, df_num=2

Granger Causality
number of lags (no zero) 3
ssr based F test:         F=0.4407  , p=0.7240  , df_denom=773, df_num=3
ssr based chi2 test:   chi2=1.3340  , p=0.7211  , df=3
likelihood ratio test: chi2=1.3328  , p=0.7214  , df=3
parameter F test:         F=0.4407  , p=0.7240  , df_denom=773, df_num=3
```

由于p值大于0.05，因此英国长期利率、短期利率间不符合格兰杰因果关系。

## 3.脉冲响应

​	脉冲响应函数(IRF)可以反映一个内生变量对误差项所带来的冲击的反应，即在随机误差项上施加一个标准差大小的冲击后，观察其对内生变量当期值和未来值的影响程度。对于平稳的VAR来说，冲激响应应当显示为持续且渐进正态而不会消失。

####################例题15.3  脉冲响应的图像分析

<img src="https://s1.ax1x.com/2020/08/11/aOxyQ0.png" style="zoom: 80%;" />

​	对于VECM模型，第一幅图片中，给RS一个冲击后，R20迅速上升达到峰值后又迅速下降并收敛于一非零常数，但依旧比起始值高，之后一直保持平稳。可以看出，短期利率的上升能够促进长期利率上升，且能保持较长时间。第二幅图片中，给R20一个冲击后，RS迅速上升到达峰值后持续缓慢下降，逐步接近于0。可见长期利率的升高短期内可以促进短期利率的升高，但随着时间的推移短期利率逐渐回落且低于起始值。

<img src="https://s1.ax1x.com/2020/08/11/aX9178.png" style="zoom:80%;" />

​	对于VAR模型，与上述分析方法相同。但对比两幅图片可以看出，VECM模型中冲激响应最终都收敛于一非零常数，但在此模型中，冲激响应逐渐减小且没有收敛到一具体值。因此，经过误差修正后的VECM模型更加稳定。

############例题15.4 判断两序列是否存在相同周期

​	由结构向量误差修正模型我们可以得到以下式子：
$$
d(R20)=c(1)*(R20(-1)-1*RS(-1)-1.19)+c(2)*d(R20(-1))+c(3)*d(rs(-1))+c(4)*d(R20(-2))+c(5)*d(RS(-2))
$$

$$
d(RS)=c(6)*(R20(-1)-1*RS(-1)-1.19)+c(7)*d(R20(-1))+c(8)*d(RS(-1))+c(9)*d(R20(-2))+c(10)*d(RS(-2))+c(11)*d(R20)
$$

可以解得$c(6)=c(7)=c(8)=c(9)=c(10)=0$，因此R20与RS不具有相同的周期，也就是说长期利率与短期利率间一个变化另一个无法立刻同步做出变化，也是符合常理的。

# 第16章 组合和计数时间序列

## 1.时间序列预测模型

​	时间序列预测分析就是利用过去一段时间内某事件时间的特征来预测未来一段时间内该事件的特征。和回归分析模型的预测不同，时间序列模型是依赖于事件发生的先后顺序的，同样大小的值改变顺序后输入模型产生的结果是不同的。

​	前面所讲的AR、MA、ARMA模型等都是传统的预测模型，首先需要对观测值序列进行平稳性检测，如果不平稳，则对其进行差分运算直到差分后的数据平稳；在数据平稳后则对其进行白噪声检验，白噪声是指零均值常方差的随机平稳序列；如果是平稳非白噪声序列就计算ACF（自相关系数）、PACF（偏自相关系数），进行ARMA等模型识别，对已识别好的模型，确定模型参数，最后应用预测并进行误差分析。

## 2.计数时间序列

​	很多时候时间序列是由整数值组成的，不适合将数据视为连续的，我们可以将模型离散化。如IN-AR(1)过程：
$$
x_t = a*x_{t-1}+w_t
$$

$$
a*x_{t-1}=\sum_{i=1}^{x_{t-1}}y_{i,t-1}
$$

IN-MA(1)过程：
$$
x_t = w_t + b*w_{t-1}
$$

$$
b*x_{t-1}=\sum_{i=1}^{w_{i-1}}y_{i,t-1}
$$

其中$x_t$，t=1,2，……都是非负整数。

####################例题16.1  不同体重人口比例的预测模型

查得$x_{1,t}$(不超重百分比)，$x_{2,t}$(超重百分比)，$x_{3,t}$(肥胖百分比)数据及图像如下：

![](https://s1.ax1x.com/2020/08/12/ajEoBd.png)

<img src="https://s1.ax1x.com/2020/08/12/ajK4i9.png" style="zoom:80%;" />

下面，根据题意求得$y_{1,t},y_{2,t},t$的值

```python
import pandas as pd
import math
#引入数据
data = pd.read_excel("data16.xlsx")
x1 = data.x1
x2 = data.x2
x3 = data.x3
y1 = []
y2 = []
t = []
#计算y1、y2、t
for i in range(len(x1)):
    y1.append(math.log(x1[i]/x3[i]))
    y2.append(math.log(x2[i]/x3[i]))
    t.append(i+1)
```

其中，t值没有取真实的年分值，而是把1993年的t值设置为1，以此类推。

接下来，分别对$y_{1,t},y_{2,t}$与t进行三次非线性拟合：

```python
import numpy as np
#非线性拟合模型
f1 = np.polyfit(t, y1, 3)
print('f1 is :\n',f1)

f2 = np.polyfit(t, y2, 3)
print('f2 is :\n',f2)
```

结果如下

```python
f1 is :
 [-1.23262544e-04  6.51604400e-03 -1.25996607e-01  1.31429570e+00]
f2 is :
 [-6.28485921e-05  3.41758938e-03 -7.54244888e-02  1.02373348e+00]
```

因此
$$
y_{1,t} = -0.000123t^3 + 0.006516t^2 - 0.126t + 1.3143
$$

$$
y_{2,t} = -0.00006285t^3 + 0.00342t^2 - 0.0754t + 1.024
$$

下面我们根据拟合的式子对2015~2020年的$x_1,x_2,x_3$数据进行预测并画出图像

```python
import pandas as pd
import math
import numpy as np
import matplotlib.pyplot as plt
data = pd.read_excel("data16.xlsx")
x1 = data.x1
x2 = data.x2
x3 = data.x3
y1 = []
y2 = []
t = []

for i in range(len(x1)):
    y1.append(math.log(x1[i]/x3[i]))
    y2.append(math.log(x2[i]/x3[i]))
    t.append(i+1)

plt.plot(t,x1,label="BMI <25")
plt.plot(t,x2,label="BMI 25~30")
plt.plot(t,x3,label="BMI >30")

f1 = np.polyfit(t, y1, 3)
print('f1 is :\n',f1)

f2 = np.polyfit(t, y2, 3)
print('f2 is :\n',f2)

predict_t = [23,24,25,26,27,28,29]
predict_y1 = []
predict_y2 = []
for i in predict_t:
    predict_y1.append(np.polyval(f1,i))
    predict_y2.append(np.polyval(f2,i))   #求出y1、y2的预测值
x1_x3 = []
x2_x3 = []
pre_x3 = []
pre_x1 = []
pre_x2 = []
for i in range(7):
    x1_x3.append(math.e**predict_y1[i])   #x1/x3=e^y1
    x2_x3.append(math.e**predict_y2[i])	  #x2/x3=e^y2
    pre_x3.append(1/(1+x1_x3[i]+x2_x3[i])*100)	#x1/x3+x2/x3=(1-x3)/x3
    pre_x1.append(pre_x3[i]*x1_x3[i])	#x1=(x1/x3)*x3
    pre_x2.append(pre_x3[i]*x2_x3[i])	#x2=(x2/x3)*x3

print(pre_x1)
plt.plot(predict_t,pre_x1,'--',label="BMI <25")
plt.plot(predict_t,pre_x2,'--',label="BMI 25~30")
plt.plot(predict_t,pre_x3,'--',label="BMI >30")
plt.legend()
plt.show()
```

<img src="https://s1.ax1x.com/2020/08/12/ajQgN4.png" style="zoom:80%;" />

根据预测，到2020年“肥胖”的成年人比例会增加2.9%，此数据可以帮助这一领域的公共政策提供信息。

#######例题16.2 英国支出份额的回归模型及分析

得到英国1955至2017年每个季度消费(c)、投资(i)、政府(g)、其他(x)的份额数据：

```python
                                     cons       inv       gov     other
                            0    0.546848  0.120680  0.291704  0.040768
                            1    0.565355  0.127042  0.297245  0.010357
                            2    0.544228  0.124751  0.281175  0.049846
                            3    0.544298  0.128362  0.279502  0.047838
                            4    0.543463  0.129519  0.277558  0.049460
                            ..        ...       ...       ...       ...
                            245  0.490477  0.128165  0.147759  0.233599
                            246  0.489015  0.128105  0.146271  0.236608
                            247  0.490050  0.128351  0.145954  0.235646
                            248  0.489447  0.128430  0.146299  0.235824
                            249  0.489192  0.128823  0.146842  0.235143		
```

下面根据题意求得$y_1,y_2,y_3,t$的值并画出图像：

```python
import pandas as pd
import math
import matplotlib.pyplot as plt
#引入数据
data = pd.read_excel("data16_2.xlsx")
c = data.cons
i = data.inv
g = data.gov
x = data.other
y1 = []
y2 = []
y3 = []
t = []
#计算y1、y2、y3
for l in range(len(c)):
    t.append(l)
    y1.append(math.log(c[l]/x[l]))
    y2.append(math.log(i[l]/x[l]))
    y3.append(math.log(g[l]/x[l]))

plt.plot(y1,label="y1")
plt.plot(y2,label="y2")
plt.plot(y3,label="y3")
plt.legend()
plt.show()
```

<img src="https://s1.ax1x.com/2020/08/12/ajJWCD.png" style="zoom:80%;" />

下面求得$y_1,y_2,y_3$的变化率$dy_1,dy_2,dy_3$后建立VAR模型

```python
                                          dy1       dy2       dy3
                                0    1.403472  1.421568  1.389006
                                1   -1.609317 -1.589432 -1.626812
                                2    0.041250  0.069652  0.035154
                                3   -0.034885 -0.024372 -0.040330
                                4    0.119957  0.141903  0.123797
                                ..        ...       ...       ...
                                245 -0.015784 -0.013267 -0.022921
                                246  0.006189  0.005991  0.001904
                                247 -0.001983 -0.000134  0.001605
                                248  0.002370  0.005950  0.006597
                                249  0.000000  0.000000  0.000000

                                [250 rows x 3 columns]
```



```python
import statsmodels.api as sm

data2 = pd.read_excel("data16_2.xlsx",usecols=[5,6,7])
orgMod = sm.tsa.VARMAX(data2,order=(3,0),exog=None)
#估计：就是模型
fitMod = orgMod.fit()
# 打印统计结果
print(fitMod.summary())
# 获得模型残差
resid = fitMod.resid
result = {'fitMod':fitMod,'resid':resid}
```

得到结果为
$$
dy1_t = -0.155dy1_{t-3}+u_{1,t}
$$

$$
dy2_t = -0.16dy1_{t-3}+u_{2,t}
$$

$$
dy3_t = -0.16dy1_{t-2}-0.34dy1_{t-3}+0.157dy3_{t-2}+0.183dy3_{t-3}+u_{3,t}
$$

可见，y1即消费/其他比率的增长率是纯自回归过程，因此该比率是外生的，会推动其他两个比率的变化。

# 第17章 状态空间模型

### 1.引入原因：

现实生活中，大多数的时间序列都是非平稳的。如果要对于该时间序列进行建模预测，就必须对时间序列进行分解。

分解的原则：将一个时间序列分解为确定性成分和随机性成分

其中，确定性成分分为：长期趋势Trend（Tt）[直观来说就是时间序列增加和下降的趋势]，季节成分Season(St)[就是时间序列受季节的影响部分]，序列相关性Co...（Ct）[即自相关性，即序列的某一项和它前面一项或者几项的关系，这是金融时间序列分析最重要的分解因素，直观来看就是时间序列在某一个阶段出现正相关或者负相关。一个典型的例子就是波动聚类，直观来讲就是高波动的阶段伴随着高波动的阶段出现，低波动的阶段伴随着低波动的阶段出现]

随机性成分是：残差项It[即随机噪声，这是时间序列的不确定性所在，随机噪声总是夹杂在非平稳性时间序列中，致使时间序列表现出某种震荡式的无规律运动]。

### 2.关于成分分解：

比较容易识别的是时间序列中的趋势和季节成分。比较复杂的是对于自相关性的建模（这也是金融时间序列分析的核心）。

在估计和抽取了确定性成分之后，使得随即项是一个平稳过程。然后求得关于随机项的合适的概率模型，分析它的性质。

### 3.作用：

状态空间模型起到的就是这样一个对时间序列进行分解的作用，将这四个成分从时间序列中分解出来。

### 4.时间序列分解的具体形式：

一般来说，一个序列{yt}可以直接或者经过函数变换后分解为如下的加法模型或者是乘法模型

#### ![时间序列分解](AppliedTimeSeries_yinaihua_files/加法或乘法模型.png)

（对于趋势明显为指数增长，且季节波动幅度也随着时间增加的序列，一般采用乘法模型）

补充：从系统理论的角度来理解，对于一个（非时变）的动态系统，要对其系统的动态特征做完整而充分的描述，需要将状态向量与量测向量结合起来。

量测向量：即通过某些物理手段可以直接观测到的向量（这里是指直接观测到的时间序列{yt}，在金融时间序列中，量测向量一般是1维的，即在每一个时刻只对应一个值）

状态向量：即用来描述系统动态特征的向量，一般可以通过经验确定都有哪几个状态向量，但是无法通过物理手段直接观测，一般记为{xt}。{xt}在时间序列中，一般是4维的，即每一个时刻都会对应3个分解量（Tt,St,Ct,）,随机噪声不属于状态向量。

所以，状态空间模型本质上就是通过某些神奇的操作，将量测向量与状态向量进行了结合。

下面，就来看一看状态空间模型是通过哪些具体的操作，运用哪些具体的知识，将可以观测的量与不可观测的量进行结合的呢？

### 5.状态空间模型的具体介绍

核心是两个方程，即状态方程和量测方程。

![时间序列分解](AppliedTimeSeries_yinaihua_files/状态方程和量测方程.png)

#### 方程含义：

状态方程：描述的是状态空间从前一时刻到当前时刻的变化规律，从方程可以看出，当前时刻的状态xt是由上一时刻的状态x(t-1)、状态转移矩阵Φ和状态噪声wt决定（一般可以设为正态（离散）白噪声）的。

量测噪声：描述的是观测值与系统的状态之间的关系。从量测方程我们可以看出，当前时刻的观测值是由上一时刻的状态值x(t-1)、量测矩阵A与状态噪声vt（同样可以设为正态（离散）白噪声，这大大减小了噪声成分估计的难度）决定。

补充：

关于正态白噪声。首先，（离散）白噪声是指一个具有如下特征的序列：均值为0，方差为![equation](D:/尹爱华/大二下/网上授课/时间序列/期末作业/chapter-17（待删除微信程序部分）/图片/equation.svg)，且

对于任意k>=1,自相关系数![equation](D:/尹爱华/大二下/网上授课/时间序列/期末作业/chapter-17（待删除微信程序部分）/图片/equation (1).svg)均为0。正态（离散）白噪声即高斯白噪声，需要补充一个特征：该序列来自于正态分布。所以，如果设随机噪声为正态（离散）白噪声，则对于每一个噪声序列，需要确定的量仅仅是![equation](D:/尹爱华/大二下/网上授课/时间序列/期末作业/chapter-17（待删除微信程序部分）/图片/equation.svg)。这大大的减少了对噪声成分量测的难度。

#### 关于状态空间（从系统的角度，这是一个很宏观的角度）:

关于两个重要的矩阵：状态转移矩阵Φ与量测矩阵A都应该是常数矩阵（所有的元素都是常数，需要通过极大似然估计法或者是EM算法进行估计）

状态空间模型的反映：系统内部状态（通过状态方程），系统的内部状态与外部的输入（x(t-1)）+输出（yt）变量的联系（通过量测方程）

向量与矩阵思想的应用：状态空间模型将多个时间变量序列处理为向量序列，同时，采用状态转移矩阵和量测矩阵。这样极大地简化了多输入输出变量的建模问题。

（不太理解，是不是因为方程是一次形式）状态空间模型能够用现在和过去的最小信息形式描述系统的状态。所以，不需要大量的历史数据做支撑。

#### 将空间状态模型应用到时间序列上，即将变量和参数矩阵进一步具体化：



![空间状态模型应用到时间序列](AppliedTimeSeries_yinaihua_files/空间状态模型应用到时间序列.png)

##### 注：

上述这些展开的所有参数都还是矩阵，不是一个具体的参数（这个在下面的具体例子中可以看出来）

下标j=1,2,3分别对应于趋势项、循环项、季节项。即：

Φ1,*A*1,*X*1*t*,*w*1t均对应于趋势项。

Φ2,*A*2,*X*2*t*,*w*2t均对应于循环项。

Φ3,*A*3,*X*3*t*,*w*3t均对应于季节项。

剩下的工作是要得到状态向量序列{Xt}。如果估计出{Xt}，则时间序列{yt}的分解就完成了。这可以通过Kalman滤波等方法对非平稳时间序列进行外推、内插及平滑，计算出每个时刻的状态向量。


```python
###########例题17.1  实现的是The Muth Model及卡尔曼滤波器的相关公式推导，无python代码
```


```python
###########例17.2  实现的是Muth模型中的预测和平滑相关公式推导，无python代码
```


```python
###########例题17.3实现的是状态空间模型建模
###########下面，我们对每月航空公司数据进行state space 建模
########导入相关包
%matplotlib inline
import numpy as np
import pandas as pd
from scipy.stats import norm
import statsmodels.api as sm
import matplotlib.pyplot as plt
```


```python
# 获取数据
from datetime import datetime
import requests
air2 = requests.get('https://www.stata-press.com/data/r12/air2.dta').content
data = pd.read_stata(BytesIO(air2))
data.index = pd.date_range(start=datetime(data.time[0], 1, 1),periods=len(data), freq='MS')
data['lnair'] = np.log(data['air'])

# 拟合模型
mod = sm.tsa.statespace.SARIMAX(data['lnair'], order=(2,1,0), seasonal_order=(1,1,0,12), simple_differencing=True)
res = mod.fit(disp=False)
print(res.summary())

```

                                     Statespace Model Results                                 
    ==========================================================================================
    Dep. Variable:                       D.DS12.lnair   No. Observations:                  131
    Model:             SARIMAX(2, 0, 0)x(1, 0, 0, 12)   Log Likelihood                 240.821
    Date:                            Fri, 07 Aug 2020   AIC                           -473.643
    Time:                                    21:21:18   BIC                           -462.142
    Sample:                                02-01-1950   HQIC                          -468.970
                                         - 12-01-1960                                         
    Covariance Type:                              opg                                         
    ==============================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
    ------------------------------------------------------------------------------
    ar.L1         -0.4057      0.080     -5.045      0.000      -0.563      -0.248
    ar.L2         -0.0799      0.099     -0.809      0.419      -0.274       0.114
    ar.S.L12      -0.4723      0.072     -6.592      0.000      -0.613      -0.332
    sigma2         0.0014      0.000      8.403      0.000       0.001       0.002
    ===================================================================================
    Ljung-Box (Q):                       49.89   Jarque-Bera (JB):                 0.72
    Prob(Q):                              0.14   Prob(JB):                         0.70
    Heteroskedasticity (H):               0.54   Skew:                             0.14
    Prob(H) (two-sided):                  0.04   Kurtosis:                         3.23
    ===================================================================================
    
    Warnings:
    [1] Covariance matrix calculated using the outer product of gradients (complex-step).

